<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="A multiclass CNN categorizer." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Sign Language Exercise | In Too Deep</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/keras/sign-language-exercise/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/keras/rock-paper-scissors/" rel="prev" title="Rock-Paper-Scissors" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="Sign Language Exercise" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/keras/sign-language-exercise/" property="og:url">
<meta content="A multiclass CNN categorizer." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-08-25T13:59:38-07:00" property="article:published_time">
<meta content="cnn" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/keras/sign-language-exercise/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/keras/sign-language-exercise/">Sign Language Exercise</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/sign-language-exercise/" rel="bookmark"><time class="published dt-published" datetime="2019-08-25T13:59:38-07:00" itemprop="datePublished" title="2019-08-25 13:59">2019-08-25 13:59</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/keras/sign-language-exercise/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org4418fc3">Beginning</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#orgc8f4ae7">Imports</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org24e2f58">Python</a></li>
<li><a href="/posts/keras/sign-language-exercise/#orgad4d279">PyPi</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org4ff1d96">Graeae</a></li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org82bd68a">Set Up</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#orgfe5e2cd">Plotting</a></li>
<li><a href="/posts/keras/sign-language-exercise/#orgc3395f2">Timer</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org9ab4919">The Environment</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org1c51b59">Middle</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#orgc2c288b">The Datasets</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org34b5f34">Data Generators</a></li>
<li><a href="/posts/keras/sign-language-exercise/#org0e2a5a5">The Model</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#org6a5c56c">Train It</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/sign-language-exercise/#org979d4e7">End</a>
<ul>
<li><a href="/posts/keras/sign-language-exercise/#orgfb3c149">Source</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org4418fc3">
<h2 id="org4418fc3">Beginning</h2>
<div class="outline-text-2" id="text-org4418fc3">
<p>This data I'm using is the <a href="https://www.kaggle.com/datamunge/sign-language-mnist/home">Sign-Language MNIST</a> set (hosted on Kaggle). It's a drop-in replacement for the MNIST dataset that contains images of hands showing letters in American Sign Language that was created by taking 1,704 photos of hands showing letters in the alphabet and then using ImageMagick to alter the photos to create a training set with 27,455 images and a test set with 7,172 images.</p>
</div>
<div class="outline-3" id="outline-container-orgc8f4ae7">
<h3 id="orgc8f4ae7">Imports</h3>
<div class="outline-text-3" id="text-orgc8f4ae7"></div>
<div class="outline-4" id="outline-container-org24e2f58">
<h4 id="org24e2f58">Python</h4>
<div class="outline-text-4" id="text-org24e2f58">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgad4d279">
<h4 id="orgad4d279">PyPi</h4>
<div class="outline-text-4" id="text-orgad4d279">
<div class="highlight">
<pre><span></span>from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image as keras_image
from tensorflow.keras.utils import to_categorical
import hvplot.pandas
import matplotlib.pyplot as pyplot
import matplotlib.image as matplotlib_image

import numpy
import pandas
import seaborn
import tensorflow
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4ff1d96">
<h4 id="org4ff1d96">Graeae</h4>
<div class="outline-text-4" id="text-org4ff1d96">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, SubPathLoader, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org82bd68a">
<h3 id="org82bd68a">Set Up</h3>
<div class="outline-text-3" id="text-org82bd68a"></div>
<div class="outline-4" id="outline-container-orgfe5e2cd">
<h4 id="orgfe5e2cd">Plotting</h4>
<div class="outline-text-4" id="text-orgfe5e2cd">
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'retina'")
seaborn.set(style="whitegrid",
            rc={"axes.grid": False,
                "font.family": ["sans-serif"],
                "font.sans-serif": ["Open Sans", "Latin Modern Sans", "Lato"],
                "figure.figsize": (8, 6)},
            font_scale=1)
FIGURE_SIZE = (12, 10)

Embed = partial(
    EmbedHoloviews,
    folder_path="../../files/posts/keras/sign-language-exercise/")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc3395f2">
<h4 id="orgc3395f2">Timer</h4>
<div class="outline-text-4" id="text-orgc3395f2">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9ab4919">
<h4 id="org9ab4919">The Environment</h4>
<div class="outline-text-4" id="text-org9ab4919">
<div class="highlight">
<pre><span></span>ENVIRONMENT = SubPathLoader("DATASETS")
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1c51b59">
<h2 id="org1c51b59">Middle</h2>
<div class="outline-text-2" id="text-org1c51b59"></div>
<div class="outline-3" id="outline-container-orgc2c288b">
<h3 id="orgc2c288b">The Datasets</h3>
<div class="outline-text-3" id="text-orgc2c288b">
<div class="highlight">
<pre><span></span>root_path = Path(ENVIRONMENT["SIGN_LANGUAGE_MNIST"]).expanduser()
</pre></div>
<div class="highlight">
<pre><span></span>def get_data(test_or_train: str) -&gt; tuple:
    """Gets the MNIST data

    The pixels are reshaped so that they are 28x28
    Also, an extra dimension is added to make the shape:
     (&lt;rows&gt;, 28, 28, 1)

    Also converts the labels to a categorical (so there are 25 columns)

    Args:
     test_or_train: which data set to load

    Returns: 
     images, labels: numpy arrays with the data
    """
    path = root_path/f"sign_mnist_{test_or_train}.csv"
    data = pandas.read_csv(path) 
    labels = data.label
    labels = to_categorical(labels)
    pixels = data[[column for column in data.columns if column.startswith("pixel")]]
    pixels = pixels.values.reshape(((len(pixels), 28, 28, 1)))
    print(labels.shape)
    print(pixels.shape)
    return pixels, labels
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org0ff5bde"></a>Training<br>
<div class="outline-text-5" id="text-org0ff5bde">
<p>The data is a CSV with the first column being the labels and the rest of the columns holding the pixel values. To make it work with our networks we need to re-shape the data so that we have a shape of (&lt;rows&gt;, 28, 28). The 28 comes from the fact that there are 784 pixel columns(28 x 28 = 784).</p>
<div class="highlight">
<pre><span></span>train_images, train_labels = get_data("train")
assert train_images.shape == (27455, 28, 28, 1)
assert train_labels.shape == (27455, 25)
</pre></div>
<pre class="example">
(27455, 25)
(27455, 28, 28, 1)

</pre>
<p>As you can see, there's a lot of columns in the original set. The first one is the "label" and the rest are the "pixel" columns.</p>
<div class="highlight">
<pre><span></span>test_images, test_labels = get_data("test")
assert test_images.shape == (7172, 28, 28, 1)
assert test_labels.shape == (7172, 25)
</pre></div>
<pre class="example">
(7172, 25)
(7172, 28, 28, 1)

</pre>
<p><b>Note:</b> The original exercise calls for doing this with the python <code>csv</code> module. But why?</p>
</div>
</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org34b5f34">
<h3 id="org34b5f34">Data Generators</h3>
<div class="outline-text-3" id="text-org34b5f34">
<div class="highlight">
<pre><span></span>training_data_generator = ImageDataGenerator(
    rescale = 1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

validation_data_generator = ImageDataGenerator(rescale = 1./255)

train_generator = training_data_generator.flow(
        train_images, train_labels,
)

validation_generator = validation_data_generator.flow(
        test_images, test_labels,
)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0e2a5a5">
<h3 id="org0e2a5a5">The Model</h3>
<div class="outline-text-3" id="text-org0e2a5a5">
<p>Part of the exercise requires that we only use two convolutional layers.</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.models.Sequential([
    # Input Layer/convolution
    tensorflow.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),
    tensorflow.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tensorflow.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tensorflow.keras.layers.MaxPooling2D(2,2),
    # Flatten
    tensorflow.keras.layers.Flatten(),
    tensorflow.keras.layers.Dropout(0.5),
    # Fully-connected and output layers
    tensorflow.keras.layers.Dense(512, activation='relu'),
    tensorflow.keras.layers.Dense(25, activation='softmax'),
])
</pre></div>
<div class="highlight">
<pre><span></span>model.summary()
</pre></div>
<pre class="example">
Model: "sequential_6"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_12 (Conv2D)           (None, 26, 26, 64)        640       
_________________________________________________________________
max_pooling2d_12 (MaxPooling (None, 13, 13, 64)        0         
_________________________________________________________________
conv2d_13 (Conv2D)           (None, 11, 11, 128)       73856     
_________________________________________________________________
max_pooling2d_13 (MaxPooling (None, 5, 5, 128)         0         
_________________________________________________________________
flatten_6 (Flatten)          (None, 3200)              0         
_________________________________________________________________
dropout_6 (Dropout)          (None, 3200)              0         
_________________________________________________________________
dense_12 (Dense)             (None, 512)               1638912   
_________________________________________________________________
dense_13 (Dense)             (None, 25)                12825     
=================================================================
Total params: 1,726,233
Trainable params: 1,726,233
Non-trainable params: 0
_________________________________________________________________
</pre></div>
<div class="outline-4" id="outline-container-org6a5c56c">
<h4 id="org6a5c56c">Train It</h4>
<div class="outline-text-4" id="text-org6a5c56c">
<div class="highlight">
<pre><span></span>model.compile(loss="categorical_crossentropy", optimizer="rmsprop", metrics=["accuracy"])
MODELS = Path("~/models/sign-language-mnist/").expanduser()
assert MODELS.is_dir()
best_model = MODELS/"two-cnn-layers.hdf5"
checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(
    str(best_model), monitor="val_accuracy", verbose=1, 
    save_best_only=True)

with TIMER:
    model.fit_generator(generator=train_generator,
                        epochs=25,
                        callbacks=[checkpoint],
                        validation_data = validation_generator,
                        verbose=2)
</pre></div>
<pre class="example">
2019-08-25 16:25:13,710 graeae.timers.timer start: Started: 2019-08-25 16:25:13.710604
I0825 16:25:13.710640 140637170140992 timer.py:70] Started: 2019-08-25 16:25:13.710604
Epoch 1/25

Epoch 00001: val_accuracy improved from -inf to 0.45427, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 8s - loss: 2.6016 - accuracy: 0.2048 - val_loss: 1.5503 - val_accuracy: 0.4543
Epoch 2/25

Epoch 00002: val_accuracy improved from 0.45427 to 0.71403, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.8267 - accuracy: 0.4160 - val_loss: 0.8762 - val_accuracy: 0.7140
Epoch 3/25

Epoch 00003: val_accuracy improved from 0.71403 to 0.74888, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.4297 - accuracy: 0.5323 - val_loss: 0.7413 - val_accuracy: 0.7489
Epoch 4/25

Epoch 00004: val_accuracy improved from 0.74888 to 0.76157, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.1984 - accuracy: 0.6100 - val_loss: 0.6402 - val_accuracy: 0.7616
Epoch 5/25

Epoch 00005: val_accuracy improved from 0.76157 to 0.84816, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 1.0498 - accuracy: 0.6570 - val_loss: 0.4581 - val_accuracy: 0.8482
Epoch 6/25

Epoch 00006: val_accuracy improved from 0.84816 to 0.85778, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.9340 - accuracy: 0.6944 - val_loss: 0.4195 - val_accuracy: 0.8578
Epoch 7/25

Epoch 00007: val_accuracy improved from 0.85778 to 0.90240, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.8522 - accuracy: 0.7189 - val_loss: 0.3270 - val_accuracy: 0.9024
Epoch 8/25

Epoch 00008: val_accuracy did not improve from 0.90240
858/858 - 7s - loss: 0.7963 - accuracy: 0.7410 - val_loss: 0.3144 - val_accuracy: 0.8887
Epoch 9/25

Epoch 00009: val_accuracy did not improve from 0.90240
858/858 - 7s - loss: 0.7388 - accuracy: 0.7560 - val_loss: 0.3184 - val_accuracy: 0.8984
Epoch 10/25

Epoch 00010: val_accuracy improved from 0.90240 to 0.92777, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.7127 - accuracy: 0.7692 - val_loss: 0.2045 - val_accuracy: 0.9278
Epoch 11/25

Epoch 00011: val_accuracy improved from 0.92777 to 0.93572, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 9s - loss: 0.6798 - accuracy: 0.7792 - val_loss: 0.1813 - val_accuracy: 0.9357
Epoch 12/25

Epoch 00012: val_accuracy improved from 0.93572 to 0.94046, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6506 - accuracy: 0.7875 - val_loss: 0.1857 - val_accuracy: 0.9405
Epoch 13/25

Epoch 00013: val_accuracy improved from 0.94046 to 0.94074, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6365 - accuracy: 0.7941 - val_loss: 0.1691 - val_accuracy: 0.9407
Epoch 14/25

Epoch 00014: val_accuracy improved from 0.94074 to 0.95706, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.6127 - accuracy: 0.8028 - val_loss: 0.1426 - val_accuracy: 0.9571
Epoch 15/25

Epoch 00015: val_accuracy did not improve from 0.95706
858/858 - 7s - loss: 0.6009 - accuracy: 0.8076 - val_loss: 0.1925 - val_accuracy: 0.9265
Epoch 16/25

Epoch 00016: val_accuracy improved from 0.95706 to 0.96207, saving model to /home/athena/models/sign-language-mnist/two-cnn-layers.hdf5
858/858 - 7s - loss: 0.5883 - accuracy: 0.8121 - val_loss: 0.1393 - val_accuracy: 0.9621
Epoch 17/25

Epoch 00017: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5785 - accuracy: 0.8127 - val_loss: 0.2188 - val_accuracy: 0.9250
Epoch 18/25

Epoch 00018: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5728 - accuracy: 0.8158 - val_loss: 0.2003 - val_accuracy: 0.9350
Epoch 19/25

Epoch 00019: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5633 - accuracy: 0.8225 - val_loss: 0.1452 - val_accuracy: 0.9578
Epoch 20/25

Epoch 00020: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5536 - accuracy: 0.8223 - val_loss: 0.1341 - val_accuracy: 0.9605
Epoch 21/25

Epoch 00021: val_accuracy did not improve from 0.96207
858/858 - 8s - loss: 0.5477 - accuracy: 0.8252 - val_loss: 0.1500 - val_accuracy: 0.9442
Epoch 22/25

Epoch 00022: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5367 - accuracy: 0.8291 - val_loss: 0.1435 - val_accuracy: 0.9568
Epoch 23/25

Epoch 00023: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5425 - accuracy: 0.8336 - val_loss: 0.1598 - val_accuracy: 0.9615
Epoch 24/25

Epoch 00024: val_accuracy did not improve from 0.96207
858/858 - 8s - loss: 0.5243 - accuracy: 0.8330 - val_loss: 0.1749 - val_accuracy: 0.9483
Epoch 25/25

Epoch 00025: val_accuracy did not improve from 0.96207
858/858 - 7s - loss: 0.5163 - accuracy: 0.8379 - val_loss: 0.1353 - val_accuracy: 0.9587
2019-08-25 16:28:20,707 graeae.timers.timer end: Ended: 2019-08-25 16:28:20.707567
I0825 16:28:20.707660 140637170140992 timer.py:77] Ended: 2019-08-25 16:28:20.707567
2019-08-25 16:28:20,712 graeae.timers.timer end: Elapsed: 0:03:06.996963
I0825 16:28:20.712478 140637170140992 timer.py:78] Elapsed: 0:03:06.996963
</pre>
<div class="highlight">
<pre><span></span>predictor = load_model(best_model)
</pre></div>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(model.history.history)
plot = data.hvplot().opts(title="Sign Language MNIST Training and Validation",
                          fontsize={"title": 16},
                          width=1000, height=800)
Embed(plot=plot, file_name="training")()
</pre></div>
<object data="/posts/keras/sign-language-exercise/training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>I'm not sure why these small networks do so well, bit this one seems to be doing fairly well.</p>
<div class="highlight">
<pre><span></span>loss, accuracy=predictor.evaluate(test_images, test_labels, verbose=0)
print(f"Loss: {loss:.2f}, Accuracy: {accuracy:.2f}")
</pre></div>
<pre class="example">
Loss: 4.36, Accuracy: 0.72

</pre>
<p>So, actually, the performance drops quite a bit outside of the training, even though I'm using the same data-set.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org979d4e7">
<h2 id="org979d4e7">End</h2>
<div class="outline-text-2" id="text-org979d4e7"></div>
<div class="outline-3" id="outline-container-orgfb3c149">
<h3 id="orgfb3c149">Source</h3>
<div class="outline-text-3" id="text-orgfb3c149">
<ul class="org-ul">
<li>The exercise comes from <a href="https://github.com/lmoroney/dlaicourse/tree/master/Exercises/Exercise%208%20-%20Multiclass%20with%20Signs">DLAIcourse Exercise 8</a> - Multiclass With Signs</li>
<li>The Data Set comes from <a href="https://www.kaggle.com/datamunge/sign-language-mnist/home">Kaggle</a></li>
</ul>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/cnn/" rel="tag">cnn</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/keras/rock-paper-scissors/" rel="prev" title="Rock-Paper-Scissors">Previous post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents Â© 2019 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
