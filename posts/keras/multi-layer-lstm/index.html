<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Using a multi-layer LSTM model to classify the IMDB reviews." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Multi-Layer LSTM | In Too Deep</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML" type="text/javascript"></script>
<script async src="../../../assets/javascript/bokeh-1.3.4.min.js" type="text/javascript"></script>
<meta content="Cloistered Monkey" name="author">
<link href="/posts/keras/imdb-reviews-tensorflow-dataset/" rel="prev" title="IMDB Reviews Tensorflow Dataset" type="text/html">
<link href="/posts/keras/he-used-sarcasm/" rel="next" title="He Used Sarcasm" type="text/html">
<meta content="In Too Deep" property="og:site_name">
<meta content="Multi-Layer LSTM" property="og:title">
<meta content="https://necromuralist.github.io/In-Too-Deep/posts/keras/multi-layer-lstm/" property="og:url">
<meta content="Using a multi-layer LSTM model to classify the IMDB reviews." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-09-19T16:07:27-07:00" property="article:published_time">
<meta content="lstm" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/In-Too-Deep/"><span id="blog-title">In Too Deep</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="/archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="/categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="/rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/In-Too-Deep/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="/posts/keras/multi-layer-lstm/index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="/posts/keras/multi-layer-lstm/">Multi-Layer LSTM</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="/posts/keras/multi-layer-lstm/" rel="bookmark"><time class="published dt-published" datetime="2019-09-19T16:07:27-07:00" itemprop="datePublished" title="2019-09-19 16:07">2019-09-19 16:07</time></a></p>
<p class="sourceline"><a class="sourcelink" href="/posts/keras/multi-layer-lstm/index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orgd994ec2">Beginning</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orgf3ec838">Imports</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org3b50025">Python</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org0d45145">PyPi</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgd79ef7f">Others</a></li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org37e70b6">Set Up</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#orgade859f">The Timer</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgb3f6621">Plotting</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org792f5d6">The Dataset</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org7a2bc37">Middle</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org0ec5523">Set Up the Datasets</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org0fe8824">The Model</a>
<ul>
<li><a href="/posts/keras/multi-layer-lstm/#org144495a">Embedding</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgf9646ef">Bidirectional</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgbf69ab5">LSTM</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#org2987b9b">Compile It</a></li>
</ul>
</li>
<li><a href="/posts/keras/multi-layer-lstm/#org552337b">Train the Model</a></li>
<li><a href="/posts/keras/multi-layer-lstm/#orgc6932fb">Looking at the Performance</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgd994ec2">
<h2 id="orgd994ec2">Beginning</h2>
<div class="outline-text-2" id="text-orgd994ec2"></div>
<div class="outline-3" id="outline-container-orgf3ec838">
<h3 id="orgf3ec838">Imports</h3>
<div class="outline-text-3" id="text-orgf3ec838"></div>
<div class="outline-4" id="outline-container-org3b50025">
<h4 id="org3b50025">Python</h4>
<div class="outline-text-4" id="text-org3b50025">
<div class="highlight">
<pre><span></span>from functools import partial
from pathlib import Path
import pickle
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0d45145">
<h4 id="org0d45145">PyPi</h4>
<div class="outline-text-4" id="text-org0d45145">
<div class="highlight">
<pre><span></span>import holoviews
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd79ef7f">
<h4 id="orgd79ef7f">Others</h4>
<div class="outline-text-4" id="text-orgd79ef7f">
<div class="highlight">
<pre><span></span>from graeae import Timer, EmbedHoloviews
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org37e70b6">
<h3 id="org37e70b6">Set Up</h3>
<div class="outline-text-3" id="text-org37e70b6"></div>
<div class="outline-4" id="outline-container-orgade859f">
<h4 id="orgade859f">The Timer</h4>
<div class="outline-text-4" id="text-orgade859f">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb3f6621">
<h4 id="orgb3f6621">Plotting</h4>
<div class="outline-text-4" id="text-orgb3f6621">
<div class="highlight">
<pre><span></span>Embed = partial(EmbedHoloviews,
                folder_path="../../files/posts/keras/multi-layer-lstm/")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org792f5d6">
<h4 id="org792f5d6">The Dataset</h4>
<div class="outline-text-4" id="text-org792f5d6">
<p>This once again uses the <a href="https://www.tensorflow.org/datasets/catalog/imdb_reviews">IMDB dataset</a> with 50,000 reviews. It has already been converted from strings to integers - each word is encoded as its own integer. Adding <code>with_info=True</code> returns an object that contains the dictionary with the word to integer mapping. Passing in <code>imdb_reviews/subwords8k</code> limits the vocabulary to 8,000 words.</p>
<p><b>Note:</b> The first time you run this it will download a fairly large dataset so it might appear to hang, but after the first time it is fairly quick.</p>
<div class="highlight">
<pre><span></span>dataset, info = tensorflow_datasets.load("imdb_reviews/subwords8k",
                                         with_info=True,
                                         as_supervised=True)
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7a2bc37">
<h2 id="org7a2bc37">Middle</h2>
<div class="outline-text-2" id="text-org7a2bc37"></div>
<div class="outline-3" id="outline-container-org0ec5523">
<h3 id="org0ec5523">Set Up the Datasets</h3>
<div class="outline-text-3" id="text-org0ec5523">
<div class="highlight">
<pre><span></span>train_dataset, test_dataset = dataset["train"], dataset["test"]
tokenizer = info.features['text'].encoder
</pre></div>
<p>Now we're going to shuffle and padd the data. The <code>BUFFER_SIZE</code> argument sets the size of the data to sample from. In this case 10,000 entries in the training set will be selected to be put in the buffer and then the "shuffle" is created by randomly selecting items from the buffer, replacing each item as it's selected until all the data has been through the buffer. The <code>padded_batch</code> method creates batches of consecutive data and pads them so that they are all the same shape.</p>
<p>The BATCH_SIZE needs to be tuned a little. If it's too big the amount of memory needed might keep the GPU from being able to use it (and it might not generalize), and if it's too small, you will take a long time to train, so you have to do a little tuning. If you train it and the GPU process percentage stays at 0, try reducing the Batch Size.</p>
<p>Also note that if you change the batch-size you have to go back to the previous step and re-define <code>train_dataset</code> and <code>test_dataset</code> because we alter them in the next step and re-altering them makes the shape wrong somehow.</p>
<div class="highlight">
<pre><span></span>BUFFER_SIZE = 10000
# if the batch size is too big it will run out of memory on the GPU 
# so you might have to experiment with this
BATCH_SIZE = 32

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)
test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_dataset.output_shapes)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0fe8824">
<h3 id="org0fe8824">The Model</h3>
<div class="outline-text-3" id="text-org0fe8824">
<p>The previous model had one Bidirectional layer, this will add a second one.</p>
</div>
<div class="outline-4" id="outline-container-org144495a">
<h4 id="org144495a">Embedding</h4>
<div class="outline-text-4" id="text-org144495a">
<p>The <a href="https://www.tensorflow.org/guide/embedding">Embedding layer</a> converts our inputs of integers and converts them to vectors of real-numbers, which is a better input for a neural network.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgf9646ef">
<h4 id="orgf9646ef">Bidirectional</h4>
<div class="outline-text-4" id="text-orgf9646ef">
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional">Bidirectional layer</a> is a wrapper for Recurrent Neural Networks.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgbf69ab5">
<h4 id="orgbf69ab5">LSTM</h4>
<div class="outline-text-4" id="text-orgbf69ab5">
<p>The <a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/LSTM">LSTM layer</a> implements Long-Short-Term Memory. The first argument is the size of the outputs. This is similar to the model that we ran previously on the same data, but it has an extra layer (so it uses more memory).</p>
<div class="highlight">
<pre><span></span>model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tensorflow.keras.layers.Bidirectional(
        tensorflow.keras.layers.LSTM(64, return_sequences=True)),
    tensorflow.keras.layers.Bidirectional(
        tensorflow.keras.layers.LSTM(32)),
    tensorflow.keras.layers.Dense(64, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         66048     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                41216     
_________________________________________________________________
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 635,329
Trainable params: 635,329
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-4" id="outline-container-org2987b9b">
<h4 id="org2987b9b">Compile It</h4>
<div class="outline-text-4" id="text-org2987b9b">
<div class="highlight">
<pre><span></span>model.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org552337b">
<h3 id="org552337b">Train the Model</h3>
<div class="outline-text-3" id="text-org552337b">
<div class="highlight">
<pre><span></span>ONCE_PER_EPOCH = 2
NUM_EPOCHS = 10
with TIMER:
    history = model.fit(train_dataset,
                        epochs=NUM_EPOCHS,
                        validation_data=test_dataset,
                        verbose=ONCE_PER_EPOCH)
</pre></div>
<pre class="example">
2019-09-21 17:26:50,395 graeae.timers.timer start: Started: 2019-09-21 17:26:50.394797
I0921 17:26:50.395130 140275698915136 timer.py:70] Started: 2019-09-21 17:26:50.394797
Epoch 1/10
W0921 17:26:51.400280 140275698915136 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
782/782 - 224s - loss: 0.6486 - accuracy: 0.6039 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
782/782 - 214s - loss: 0.4941 - accuracy: 0.7661 - val_loss: 0.6706 - val_accuracy: 0.6744
Epoch 3/10
782/782 - 216s - loss: 0.4087 - accuracy: 0.8266 - val_loss: 0.4024 - val_accuracy: 0.8222
Epoch 4/10
782/782 - 217s - loss: 0.2855 - accuracy: 0.8865 - val_loss: 0.3343 - val_accuracy: 0.8645
Epoch 5/10
782/782 - 216s - loss: 0.2097 - accuracy: 0.9217 - val_loss: 0.2936 - val_accuracy: 0.8837
Epoch 6/10
782/782 - 217s - loss: 0.1526 - accuracy: 0.9467 - val_loss: 0.3188 - val_accuracy: 0.8771
Epoch 7/10
782/782 - 215s - loss: 0.1048 - accuracy: 0.9657 - val_loss: 0.3750 - val_accuracy: 0.8710
Epoch 8/10
782/782 - 216s - loss: 0.0764 - accuracy: 0.9757 - val_loss: 0.3821 - val_accuracy: 0.8762
Epoch 9/10
782/782 - 216s - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.4747 - val_accuracy: 0.8683
Epoch 10/10
782/782 - 216s - loss: 0.0438 - accuracy: 0.9883 - val_loss: 0.4441 - val_accuracy: 0.8704
2019-09-21 18:02:56,353 graeae.timers.timer end: Ended: 2019-09-21 18:02:56.353722
I0921 18:02:56.353781 140275698915136 timer.py:77] Ended: 2019-09-21 18:02:56.353722
2019-09-21 18:02:56,356 graeae.timers.timer end: Elapsed: 0:36:05.958925
I0921 18:02:56.356238 140275698915136 timer.py:78] Elapsed: 0:36:05.958925
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgc6932fb">
<h3 id="orgc6932fb">Looking at the Performance</h3>
<div class="outline-text-3" id="text-orgc6932fb">
<p>To get the history I had to pickle it and then copy it over to the machine with this org-notebook, so you can't just run this notebook and make it work unless everything is run on the same machine (which it wasn't).</p>
<div class="highlight">
<pre><span></span>path = Path("~/history.pkl").expanduser()
with path.open("wb") as writer:
    pickle.dump(history.history, writer)
</pre></div>
<div class="highlight">
<pre><span></span>path = Path("~/history.pkl").expanduser()
with path.open("rb") as reader:
    history = pickle.load(reader)
</pre></div>
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history)
best = data.val_loss.idxmin()
best_line = holoviews.VLine(best)
plot = (data.hvplot() * best_line).opts(
    title="Two-Layer LSTM Model",
    width=1000,
    height=800)
Embed(plot=plot, file_name="lstm_training")()
</pre></div>
<object data="/posts/keras/multi-layer-lstm/lstm_training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It looks like the best epoch was the fifth one, with a validation loss of 0.29 and a validation accuracy of 0.88, after that it looks like it overfits. It seems that text might be a harder problem than images.</p>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="/categories/lstm/" rel="tag">lstm</a></li>
<li><a class="tag p-category" href="/categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="/posts/keras/imdb-reviews-tensorflow-dataset/" rel="prev" title="IMDB Reviews Tensorflow Dataset">Previous post</a></li>
<li class="next"><a href="/posts/keras/he-used-sarcasm/" rel="next" title="He Used Sarcasm">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Contents Â© 2020 <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="/assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script> 
</body>
</html>
