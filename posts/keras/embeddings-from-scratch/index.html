<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Walking through the tensorflow word embeddbings tutorial." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Embeddings from Scratch | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/embeddings-from-scratch/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../imdb-lstm-with-tokenization/" rel="prev" title="IMDB GRU With Tokenization" type="text/html">
<link href="../nlp-classification-exercise/" rel="next" title="NLP Classification Exercise" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Embeddings from Scratch" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/keras/embeddings-from-scratch/" property="og:url">
<meta content="Walking through the tensorflow word embeddbings tutorial." property="og:description">
<meta content="article" property="og:type">
<meta content="2019-09-25T13:30:12-07:00" property="article:published_time">
<meta content="embeddings" property="article:tag">
<meta content="keras" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Embeddings from Scratch</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2019-09-25T13:30:12-07:00" itemprop="datePublished" title="2019-09-25 13:30">2019-09-25 13:30</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgf8fc987">Beginning</a>
<ul>
<li><a href="#orgf3422a0">Imports</a>
<ul>
<li><a href="#orgd858691">Python</a></li>
<li><a href="#org1cabd2d">PyPi</a></li>
<li><a href="#org5daed17">Others</a></li>
</ul>
</li>
<li><a href="#orga138d4c">Set Up</a>
<ul>
<li><a href="#orgb6ec616">Plotting</a></li>
<li><a href="#org3d4dbe4">The Timer</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3d3130a">Middle</a>
<ul>
<li><a href="#org9c58da9">Some Constants</a></li>
<li><a href="#orge5593e0">The Embeddings Layer</a></li>
<li><a href="#orgc6f3569">The Dataset</a>
<ul>
<li><a href="#org15a8798">Add Padding</a></li>
<li><a href="#orge25b7c7">Checkout a Sample</a></li>
</ul>
</li>
<li><a href="#org9b00cc7">Build a Model</a></li>
<li><a href="#orgb894f99">Compile and Train</a></li>
</ul>
</li>
<li><a href="#org53703c7">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgf8fc987">
<h2 id="orgf8fc987">Beginning</h2>
<div class="outline-text-2" id="text-orgf8fc987">
<p>This is a walk-through of the tensorflow <a href="https://www.tensorflow.org/beta/tutorials/text/word_embeddings">Word Embeddings</a> tutorial, just to make sure I can do it.</p>
</div>
<div class="outline-3" id="outline-container-orgf3422a0">
<h3 id="orgf3422a0">Imports</h3>
<div class="outline-text-3" id="text-orgf3422a0"></div>
<div class="outline-4" id="outline-container-orgd858691">
<h4 id="orgd858691">Python</h4>
<div class="outline-text-4" id="text-orgd858691">
<div class="highlight">
<pre><span></span>from argparse import Namespace
from functools import partial
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1cabd2d">
<h4 id="org1cabd2d">PyPi</h4>
<div class="outline-text-4" id="text-org1cabd2d">
<div class="highlight">
<pre><span></span>from tensorflow import keras
from tensorflow.keras import layers
import hvplot.pandas
import pandas
import tensorflow
import tensorflow_datasets
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5daed17">
<h4 id="org5daed17">Others</h4>
<div class="outline-text-4" id="text-org5daed17">
<div class="highlight">
<pre><span></span>from graeae import EmbedHoloviews, Timer
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga138d4c">
<h3 id="orga138d4c">Set Up</h3>
<div class="outline-text-3" id="text-orga138d4c"></div>
<div class="outline-4" id="outline-container-orgb6ec616">
<h4 id="orgb6ec616">Plotting</h4>
<div class="outline-text-4" id="text-orgb6ec616">
<div class="highlight">
<pre><span></span>prefix = "../../files/posts/keras/"
slug = "embeddings-from-scratch"

Embed = partial(EmbedHoloviews, folder_path=f"{prefix}{slug}")
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3d4dbe4">
<h4 id="org3d4dbe4">The Timer</h4>
<div class="outline-text-4" id="text-org3d4dbe4">
<div class="highlight">
<pre><span></span>TIMER = Timer()
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3d3130a">
<h2 id="org3d3130a">Middle</h2>
<div class="outline-text-2" id="text-org3d3130a"></div>
<div class="outline-3" id="outline-container-org9c58da9">
<h3 id="org9c58da9">Some Constants</h3>
<div class="outline-text-3" id="text-org9c58da9">
<div class="highlight">
<pre><span></span>Text = Namespace(
    vocabulary_size=1000,
    embeddings_size=16,
    max_length=500,
    padding="post",
)

Tokens = Namespace(
    padding = "&lt;PAD&gt;",
    start = "&lt;START&gt;",
    unknown = "&lt;UNKNOWN&gt;",
    unused = "&lt;UNUSED&gt;",
)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge5593e0">
<h3 id="orge5593e0">The Embeddings Layer</h3>
<div class="outline-text-3" id="text-orge5593e0">
<div class="highlight">
<pre><span></span>print(layers.Embedding.__doc__)
</pre></div>
<pre class="example" id="orga8366ac">
Turns positive integers (indexes) into dense vectors of fixed size.

  e.g. `[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]`

  This layer can only be used as the first layer in a model.

  Example:

  ```python
  model = Sequential()
  model.add(Embedding(1000, 64, input_length=10))
  # the model will take as input an integer matrix of size (batch,
  # input_length).
  # the largest integer (i.e. word index) in the input should be no larger
  # than 999 (vocabulary size).
  # now model.output_shape == (None, 10, 64), where None is the batch
  # dimension.

  input_array = np.random.randint(1000, size=(32, 10))

  model.compile('rmsprop', 'mse')
  output_array = model.predict(input_array)
  assert output_array.shape == (32, 10, 64)
  ```

  Arguments:
    input_dim: int &gt; 0. Size of the vocabulary,
      i.e. maximum integer index + 1.
    output_dim: int &gt;= 0. Dimension of the dense embedding.
    embeddings_initializer: Initializer for the `embeddings` matrix.
    embeddings_regularizer: Regularizer function applied to
      the `embeddings` matrix.
    embeddings_constraint: Constraint function applied to
      the `embeddings` matrix.
    mask_zero: Whether or not the input value 0 is a special "padding"
      value that should be masked out.
      This is useful when using recurrent layers
      which may take variable length input.
      If this is `True` then all subsequent layers
      in the model need to support masking or an exception will be raised.
      If mask_zero is set to True, as a consequence, index 0 cannot be
      used in the vocabulary (input_dim should equal size of
      vocabulary + 1).
    input_length: Length of input sequences, when it is constant.
      This argument is required if you are going to connect
      `Flatten` then `Dense` layers upstream
      (without it, the shape of the dense outputs cannot be computed).

  Input shape:
    2D tensor with shape: `(batch_size, input_length)`.

  Output shape:
    3D tensor with shape: `(batch_size, input_length, output_dim)`.
  
</pre>
<div class="highlight">
<pre><span></span>embedding_layer = layers.Embedding(Text.vocabulary_size, Text.embeddings_size)
</pre></div>
<p>The first argument is the number of possible words in the vocabulary and the second is the number of dimensions. The Emebdding is a sort of lookup table that maps an integer that represents a word to a vector. In this case we're going to build a vocabulary of 1,000 words represented by vectors with a length of 32. The weights in the vectors are learned when we train the model and will encode the distance between words.</p>
<p>The input to the embeddings layer is a 2D tensor of integers with the shape (<code>number of samples</code>, <code>sequence_length</code>). The sequences are integer-encoded sentences of the same length - so you have to pad the shorter sentences to match the longest one (the <code>sequence_length</code>).</p>
<p>The ouput of the embeddings layer is a 3D tensor with the shape (<code>number of samples</code>, <code>sequence_length</code>, <code>embedding_dimensionality</code>).</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgc6f3569">
<h3 id="orgc6f3569">The Dataset</h3>
<div class="outline-text-3" id="text-orgc6f3569">
<div class="highlight">
<pre><span></span>(train_data, test_data), info = tensorflow_datasets.load(
    "imdb_reviews/subwords8k",
    split=(tensorflow_datasets.Split.TRAIN,
           tensorflow_datasets.Split.TEST),
    with_info=True, as_supervised=True)
</pre></div>
<div class="highlight">
<pre><span></span>encoder = info.features["text"].encoder
print(encoder.subwords[:10])
</pre></div>
<pre class="example">
['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']
</pre></div>
<div class="outline-4" id="outline-container-org15a8798">
<h4 id="org15a8798">Add Padding</h4>
<div class="outline-text-4" id="text-org15a8798">
<div class="highlight">
<pre><span></span>padded_shapes = ([None], ())
train_batches = train_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes)
test_batches = test_data.shuffle(Text.vocabulary_size).padded_batch(
    10, padded_shapes=padded_shapes
)
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge25b7c7">
<h4 id="orge25b7c7">Checkout a Sample</h4>
<div class="outline-text-4" id="text-orge25b7c7">
<div class="highlight">
<pre><span></span>batch, labels = next(iter(train_batches))
print(batch.numpy())
</pre></div>
<pre class="example">
[[  62    9    4 ...    0    0    0]
 [  19 2428    6 ...    0    0    0]
 [ 691    2  594 ... 7961 1457 7975]
 ...
 [6072 5644 8043 ...    0    0    0]
 [ 977   15   57 ...    0    0    0]
 [5646    2    1 ...    0    0    0]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org9b00cc7">
<h3 id="org9b00cc7">Build a Model</h3>
<div class="outline-text-3" id="text-org9b00cc7">
<div class="highlight">
<pre><span></span>model = keras.Sequential([
    layers.Embedding(encoder.vocab_size, Text.embeddings_size),
    layers.GlobalAveragePooling1D(),
    layers.Dense(1, activation="sigmoid")
])
</pre></div>
<div class="highlight">
<pre><span></span>print(model.summary())
</pre></div>
<pre class="example" id="org54e8ba0">
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, None, 16)          130960    
_________________________________________________________________
global_average_pooling1d (Gl (None, 16)                0         
_________________________________________________________________
dense (Dense)                (None, 1)                 17        
=================================================================
Total params: 130,977
Trainable params: 130,977
Non-trainable params: 0
_________________________________________________________________
None
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb894f99">
<h3 id="orgb894f99">Compile and Train</h3>
<div class="outline-text-3" id="text-orgb894f99">
<div class="highlight">
<pre><span></span>model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
ONCE_PER_EPOCH = 2
with TIMER:
    history = model.fit(train_batches, epochs=10,
                        validation_data=test_batches,
                        verbose=ONCE_PER_EPOCH,
                        validation_steps=20)
</pre></div>
<pre class="example" id="orgced6564">
2019-09-28 17:14:52,764 graeae.timers.timer start: Started: 2019-09-28 17:14:52.764725
I0928 17:14:52.764965 140515023214400 timer.py:70] Started: 2019-09-28 17:14:52.764725
W0928 17:14:52.806057 140515023214400 deprecation.py:323] From /home/hades/.virtualenvs/In-Too-Deep/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
Epoch 1/10
 val_loss: 0.3015 - val_accuracy: 0.8900
2019-09-28 17:17:36,036 graeae.timers.timer end: Ended: 2019-09-28 17:17:36.036090
I0928 17:17:36.036139 140515023214400 timer.py:77] Ended: 2019-09-28 17:17:36.036090
2019-09-28 17:17:36,037 graeae.timers.timer end: Elapsed: 0:02:43.271365
I0928 17:17:36.037808 140515023214400 timer.py:78] Elapsed: 0:02:43.271365
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org53703c7">
<h2 id="org53703c7">End</h2>
<div class="outline-text-2" id="text-org53703c7">
<div class="highlight">
<pre><span></span>data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="Training/Validation Performance",
                          width=1000,
                          height=800)
Embed(plot=plot, file_name="training")()
</pre></div>
<object data="training.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Amazingly, even with such a simple model, it managed a 92 % validation accuracy.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/embeddings/" rel="tag">embeddings</a></li>
<li><a class="tag p-category" href="../../../categories/keras/" rel="tag">keras</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../imdb-lstm-with-tokenization/" rel="prev" title="IMDB GRU With Tokenization">Previous post</a></li>
<li class="next"><a href="../nlp-classification-exercise/" rel="next" title="NLP Classification Exercise">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
