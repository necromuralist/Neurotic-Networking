<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Building and traning the CBOW Model." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Word Embeddings: Training the Model | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings-training-the-model/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../word-embeddings-shakespeare-data/" rel="prev" title="Word Embeddings: Shakespeare Data" type="text/html">
<link href="../word-embeddings-visualizing-the-embeddings/" rel="next" title="Word Embeddings: Visualizing the Embeddings" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Word Embeddings: Training the Model" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings-training-the-model/" property="og:url">
<meta content="Building and traning the CBOW Model." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-13T14:42:07-08:00" property="article:published_time">
<meta content="cbow" property="article:tag">
<meta content="nlp" property="article:tag">
<meta content="word embeddings" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Word Embeddings: Training the Model</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-13T14:42:07-08:00" itemprop="datePublished" title="2020-12-13 14:42">2020-12-13 14:42</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org9ce1dc9">Building and Training the Model</a>
<ul>
<li><a href="#orgbb5027a">Imports</a></li>
<li><a href="#org4a50639">Set Up</a></li>
</ul>
</li>
<li><a href="#org843b312">Middle</a>
<ul>
<li><a href="#org2131362">Initializing the model</a></li>
<li><a href="#org1014537">Softmax</a>
<ul>
<li><a href="#org8fa7496">The Implementation</a></li>
</ul>
</li>
<li><a href="#orgcc3428e">Forward propagation</a>
<ul>
<li><a href="#orga33557d">Test the function</a></li>
</ul>
</li>
<li><a href="#org0364e34">Pack Index with Frequency</a></li>
<li><a href="#org1d4c2fd">Vector Generator</a></li>
<li><a href="#orged69945">Batch Generator</a></li>
<li><a href="#orgb46ad60">Cost function</a>
<ul>
<li><a href="#orgb38eab9">Test the function</a></li>
</ul>
</li>
<li><a href="#org389067c">Training the Model - Backpropagation</a>
<ul>
<li><a href="#orga912871">Test the function</a></li>
</ul>
</li>
<li><a href="#orgaeda02e">Gradient Descent</a>
<ul>
<li><a href="#org15aa68a">Test Your Function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org78da4ff">End</a>
<ul>
<li><a href="#org0a0f5d1">Bundling It Up</a>
<ul>
<li><a href="#org0086ddc">Imports</a></li>
<li><a href="#org6f8e176">Enum Setup</a></li>
<li><a href="#orgeff3af3">Named Tuples</a></li>
<li><a href="#org23b175f">The CBOW Model</a></li>
<li><a href="#orgb8a68e3">Batch Generator</a></li>
<li><a href="#org794da07">The Trainer</a></li>
</ul>
</li>
<li><a href="#org50eadc0">Testing It</a>
<ul>
<li><a href="#org22d7914">Forward Propagation</a></li>
<li><a href="#orgca20e6a">Cross Entropy Loss</a></li>
<li><a href="#org508d870">Back Propagation</a></li>
<li><a href="#org4be8afb">Putting Some Stuff Together</a></li>
<li><a href="#org9e60158">The Batches</a></li>
<li><a href="#orgb9986ea">Gradient Descent</a></li>
<li><a href="#org5e7d89b">Gradient Re-do</a></li>
<li><a href="#org04a335e">Troubleshooting the Batches</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org9ce1dc9">
<h2 id="org9ce1dc9">Building and Training the Model</h2>
<div class="outline-text-2" id="text-org9ce1dc9">
<p>In the <a href="../word-embeddings-shakespeare-data/">previous post</a> we did some preliminary set up and data pre-processing. Now we're going to build and train a Continuous Bag of Words (CBOW) model.</p>
</div>
<div class="outline-3" id="outline-container-orgbb5027a">
<h3 id="orgbb5027a">Imports</h3>
<div class="outline-text-3" id="text-orgbb5027a">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">contain_exactly</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">DataCleaner</span><span class="p">,</span> <span class="n">MetaData</span>

<span class="c1"># my other stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4a50639">
<h3 id="org4a50639">Set Up</h3>
<div class="outline-text-3" id="text-org4a50639">
<p>Code from the previous post.</p>
<div class="highlight">
<pre><span></span><span class="n">cleaner</span> <span class="o">=</span> <span class="n">DataCleaner</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">MetaData</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">speak</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="s2">"files/posts/nlp/word-embeddings-training-the-model"</span><span class="p">)</span>
<span class="n">Plot</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<p>Something to help remember what the numpy <code>axis</code> argument is.</p>
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org843b312">
<h2 id="org843b312">Middle</h2>
<div class="outline-text-2" id="text-org843b312"></div>
<div class="outline-3" id="outline-container-org2131362">
<h3 id="org2131362">Initializing the model</h3>
<div class="outline-text-3" id="text-org2131362">
<p>You will now initialize two matrices and two vectors.</p>
<ul class="org-ul">
<li>The first matrix (\(W_1\)) is of dimension \(N \times V\), where <i>V</i> is the number of words in your vocabulary and <i>N</i> is the dimension of your word vector.</li>
<li>The second matrix (\(W_2\)) is of dimension \(V \times N\).</li>
<li>Vector \(b_1\) has dimensions \(N\times 1\)</li>
<li>Vector \(b_2\) has dimensions \(V\times 1\).</li>
<li>\(b_1\) and \(b_2\) are the bias vectors of the linear layers from matrices \(W_1\) and \(W_2\).</li>
</ul>
<p>At this stage we are just initializing the parameters.</p>
<p>Please use <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html">numpy.random.rand</a> to generate matrices that are initialized with random values from a uniform distribution, ranging between 0 and 1.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: initialize_model</span>
<span class="k">def</span> <span class="nf">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span><span class="n">V</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Initialize the matrices with random values</span>

<span class="sd">    Args: </span>
<span class="sd">       N:  dimension of hidden vector </span>
<span class="sd">       V:  dimension of vocabulary</span>
<span class="sd">       random_seed: random seed for consistent results in the unit tests</span>
<span class="sd">     Returns: </span>
<span class="sd">       W1, W2, b1, b2: initialized weights and biases</span>
<span class="sd">    """</span>

    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>
    <span class="c1"># W1 has shape (N,V)</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
    <span class="c1"># W2 has shape (V,N)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
    <span class="c1"># b1 has shape (N,1)</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># b2 has shape (V,1)</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
<p>Test your function example.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span><span class="n">tmp_N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">tmp_V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape: </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape: </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape: </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape: </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
tmp_W1.shape: (4, 10)
tmp_W2.shape: (10, 4)
tmp_b1.shape: (4, 1)
tmp_b2.shape: (10, 1)
</pre></div>
</div>
<div class="outline-3" id="outline-container-org1014537">
<h3 id="org1014537">Softmax</h3>
<div class="outline-text-3" id="text-org1014537">
<p>Before we can start training the model, we need to implement the softmax function as defined in equation 5:</p>
<p>\[ \text{softmax}(z_i) = \frac{e^{z_i} }{\sum_{i=0}^{V-1} e^{z_i} } \tag{5} \]</p>
<ul class="org-ul">
<li>Array indexing in code starts at 0.</li>
<li><i>V</i> is the number of words in the vocabulary (which is also the number of rows of <i>z</i>).</li>
<li><i>i</i> goes from 0 to |V| - 1.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org8fa7496">
<h4 id="org8fa7496">The Implementation</h4>
<div class="outline-text-4" id="text-org8fa7496">
<ul class="org-ul">
<li>Assume that the input <i>z</i> to <code>softmax</code> is a 2D array</li>
<li>Each training example is represented by a column of shape (V, 1) in this 2D array.</li>
<li>There may be more than one column, in the 2D array, because you can put in a batch of examples to increase efficiency. Let's call the batch size lowercase <i>m</i>, so the <i>z</i> array has shape (V, m)</li>
<li>When taking the sum from \(i=1 \cdots V-1\), take the sum for each column (each example) separately.</li>
</ul>
<p>Please use</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy.exp</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html">numpy.sum</a> (set the axis so that you take the sum of each column in z)</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: softmax</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       z: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate yhat (softmax)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">yhat</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># Test the function</span>
<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[[0.5        0.73105858 0.88079708]
 [0.5        0.26894142 0.11920292]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgcc3428e">
<h3 id="orgcc3428e">Forward propagation</h3>
<div class="outline-text-3" id="text-orgcc3428e">
<p>We're going to implement the forward propagation <i>z</i> according to equations (1) to (3).</p>
\begin{align} h &amp;= W_1 \ X + b_1 \tag{1} \\ a &amp;= ReLU(h) \tag{2} \\ z &amp;= W_2 \ a + b_2 \tag{3} \\ \end{align}
<p>For that, you will use as activation the Rectified Linear Unit (ReLU) given by:</p>
<p>\[ f(h)=\max (0,h) \tag{6} \]</p>
<p><b>Hints:</b></p>
<ul class="org-ul">
<li>You can use <a href="https://numpy.org/doc/stable/reference/generated/numpy.maximum.html">numpy.maximum(x1,x2)</a> to get the maximum of two values</li>
<li>Use <a href="https://numpy.org/doc/stable/reference/generated/numpy.dot.html">numpy.dot(A,B)</a> to matrix multiply A and B</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: forward_prop</span>
<span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Pass the data through the network</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases to be learned</span>
<span class="sd">    Returns: </span>
<span class="sd">       z:  output score vector</span>
<span class="sd">    """</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>

    <span class="c1"># Calculate h</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

    <span class="c1"># Apply the relu on h (store result in h)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Calculate z</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">h</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga33557d">
<h4 id="orga33557d">Test the function</h4>
<div class="outline-text-4" id="text-orga33557d">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"x has shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"N is </span><span class="si">{</span><span class="n">tmp_N</span><span class="si">}</span><span class="s2"> and vocabulary size V is </span><span class="si">{</span><span class="n">tmp_V</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"call forward_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"z has shape </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"z has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"h has shape </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"h has values:"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org8ac0560">
x has shape (3, 1)
N is 2 and vocabulary size V is 3
call forward_prop

z has shape (3, 1)
z has values:
[[0.55379268]
 [1.58960774]
 [1.50722933]]

h has shape (2, 1)
h has values:
[[0.92477674]
 [1.02487333]]
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org0364e34">
<h3 id="org0364e34">Pack Index with Frequency</h3>
<div class="outline-text-3" id="text-org0364e34">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span>
                              <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""combines indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>
<span class="sd">     word_to_index: mapping of word to index</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequency_dict</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="n">packed</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">)):</span>
        <span class="n">word_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">frequency</span> <span class="o">=</span> <span class="n">frequency_dict</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]]</span>
        <span class="n">packed</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">packed</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1d4c2fd">
<h3 id="org1d4c2fd">Vector Generator</h3>
<div class="outline-text-3" id="text-org1d4c2fd">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Generates vectors of fraction of context words each word represents</span>

<span class="sd">    Args:</span>
<span class="sd">     data: source of the vectors</span>
<span class="sd">     word_to_index: mapping of word to index in the vocabulary</span>
<span class="sd">     half_window: number of tokens on either side of the word to keep</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="n">half_window</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
                         <span class="o">+</span> <span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context_words</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orged69945">
<h3 id="orged69945">Batch Generator</h3>
<div class="outline-text-3" id="text-orged69945">
<p>This uses a not so common form of the <a href="https://docs.python.org/3/reference/compound_stmts.html#while">while</a> loop. Whenever you run a loop and it reaches the end (so you didn't break it) then it will run the <code>else</code> clause.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">original</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Generate batches of vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the training data</span>
<span class="sd">     word_to_index: map of word to vocabulary index</span>
<span class="sd">     half_window: number of tokens to take from either side of word</span>
<span class="sd">     batch_size: Number of vectors to put in each training batch</span>
<span class="sd">     original: run the original buggy code</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of X, Y batches</span>
<span class="sd">    """</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">vectors</span><span class="p">(</span><span class="n">data</span><span class="p">,</span>
                        <span class="n">word_to_index</span><span class="p">,</span>
                        <span class="n">half_window</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">original</span><span class="p">:</span>
            <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
                <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
                <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span>
</pre></div>
<p>So every time <code>batch_x</code> reaches the <code>batch_size</code> it yields the tuple and then creates a new batch before continuing the outer for-loop.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb46ad60">
<h3 id="orgb46ad60">Cost function</h3>
<div class="outline-text-3" id="text-orgb46ad60">
<p>The cross-entropy loss function.</p>
<ul class="org-ul">
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html">numpy.squeeze</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.multiply.html">numpy.multiply</a></li>
<li><a href="https://numpy.org/doc/stable/reference/generated/numpy.log.html">numpy.log</a></li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     y: array with the actual words labeled</span>
<span class="sd">     y_hat: our model's guesses for the words</span>
<span class="sd">     batch_size: the number of examples per training run</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgb38eab9">
<h4 id="orgb38eab9">Test the function</h4>
<div class="outline-text-4" id="text-orgb38eab9">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call compute_cost"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_cost </span><span class="si">{</span><span class="n">tmp_cost</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org6537aaa">
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)
tmp_yhat.shape: (5778, 4)
call compute_cost
tmp_cost 9.9560
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org389067c">
<h3 id="org389067c">Training the Model - Backpropagation</h3>
<div class="outline-text-3" id="text-org389067c">
<p>Now that you have understood how the CBOW model works, you will train it. You created a function for the forward propagation. Now you will implement a function that computes the gradients to backpropagate the errors.</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: back_prop</span>
<span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">h</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">W2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">b2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculates the gradients</span>

<span class="sd">    Args: </span>
<span class="sd">       x:  average one hot vector for the context </span>
<span class="sd">       yhat: prediction (estimate of y)</span>
<span class="sd">       y:  target vector</span>
<span class="sd">       h:  hidden vector (see eq. 1)</span>
<span class="sd">       W1, W2, b1, b2:  matrices and biases  </span>
<span class="sd">       batch_size: batch size </span>

<span class="sd">     Returns: </span>
<span class="sd">       grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   </span>
<span class="sd">    """</span>
    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Compute l1 as W2^T (Yhat - Y)</span>
    <span class="c1"># Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Apply relu to l1</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Compute the gradient of W1</span>
    <span class="n">grad_W1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of W2</span>
    <span class="n">grad_W2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b1</span>
    <span class="n">grad_b1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1"># Compute the gradient of b2</span>
    <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yhat</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga912871">
<h4 id="orga912871">Test the function</h4>
<div class="outline-text-4" id="text-orga912871">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_Ind2word</span> <span class="o">=</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">tmp_word2Ind</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">"get a batch of data"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_x.shape </span><span class="si">{</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_y.shape </span><span class="si">{</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Initialize weights and biases"</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W1.shape </span><span class="si">{</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_W2.shape </span><span class="si">{</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b1.shape </span><span class="si">{</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_b2.shape </span><span class="si">{</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Forwad prop to get z and h"</span><span class="p">)</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_z.shape: </span><span class="si">{</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_h.shape: </span><span class="si">{</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Get yhat by calling softmax"</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">tmp_m</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">tmp_C</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"call back_prop"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W1.shape </span><span class="si">{</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_W2.shape </span><span class="si">{</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b1.shape </span><span class="si">{</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_grad_b2.shape </span><span class="si">{</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
<pre class="example" id="org5e178a3">
get a batch of data
tmp_x.shape (5778, 4)
tmp_y.shape (5778, 4)

Initialize weights and biases
tmp_W1.shape (50, 5778)
tmp_W2.shape (5778, 50)
tmp_b1.shape (50, 1)
tmp_b2.shape (5778, 1)

Forwad prop to get z and h
tmp_z.shape: (5778, 4)
tmp_h.shape: (50, 4)

Get yhat by calling softmax
tmp_yhat.shape: (5778, 4)

call back_prop
tmp_grad_W1.shape (50, 5778)
tmp_grad_W2.shape (5778, 50)
tmp_grad_b1.shape (50, 1)
tmp_grad_b2.shape (5778, 1)
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgaeda02e">
<h3 id="orgaeda02e">Gradient Descent</h3>
<div class="outline-text-3" id="text-orgaeda02e">
<p>Now that you have implemented a function to compute the gradients, you will implement batch gradient descent over your training set.</p>
<p><b>Hint:</b> For that, you will use <code>initialize_model</code> and the <code>back_prop</code> functions which you just created (and the <code>compute_cost</code> function). You can also use the provided <code>get_batches</code> helper function:</p>
<p>Also: print the cost after each batch is processed (use batch size = 128).</p>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: gradient_descent</span>
<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">V</span><span class="p">:</span> <span class="nb">int</span> <span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>    
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">282</span><span class="p">)</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">word2Ind</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1">### START CODE HERE (Replace instances of 'None' with your own code) ###</span>
        <span class="c1"># Get z and h</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span> <span class="p">(</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="c1"># Get gradients</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                       <span class="n">yhat</span><span class="p">,</span>
                                                       <span class="n">y</span><span class="p">,</span>
                                                       <span class="n">h</span><span class="p">,</span>
                                                       <span class="n">W1</span><span class="p">,</span>
                                                       <span class="n">W2</span><span class="p">,</span>
                                                       <span class="n">b1</span><span class="p">,</span>
                                                       <span class="n">b2</span><span class="p">,</span>
                                                       <span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org15aa68a">
<h4 id="org15aa68a">Test Your Function</h4>
<div class="outline-text-4" id="text-org15aa68a">
<div class="highlight">
<pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">num_iters</span> <span class="o">=</span> <span class="mi">150</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Call gradient_descent"</span><span class="p">)</span>
<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">num_iters</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org8cfa3d7">
Call gradient_descent
iters: 10 cost: 0.789141
iters: 20 cost: 0.105543
iters: 30 cost: 0.056008
iters: 40 cost: 0.038101
iters: 50 cost: 0.028868
iters: 60 cost: 0.023237
iters: 70 cost: 0.019444
iters: 80 cost: 0.016716
iters: 90 cost: 0.014660
iters: 100 cost: 0.013054
iters: 110 cost: 0.012133
iters: 120 cost: 0.011370
iters: 130 cost: 0.010698
iters: 140 cost: 0.010100
iters: 150 cost: 0.009566
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org78da4ff">
<h2 id="org78da4ff">End</h2>
<div class="outline-text-2" id="text-org78da4ff">
<p>The <a href="../word-embeddings-visualizing-the-embeddings/">next post</a> is one on extracting and visualizing the embeddings using Principal Component Analysis.</p>
</div>
<div class="outline-3" id="outline-container-org0a0f5d1">
<h3 id="org0a0f5d1">Bundling It Up</h3>
<div class="outline-text-3" id="text-org0a0f5d1"></div>
<div class="outline-4" id="outline-container-org0086ddc">
<h4 id="org0086ddc">Imports</h4>
<div class="outline-text-4" id="text-org0086ddc">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span><span class="p">,</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span><span class="p">,</span> <span class="n">unique</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org6f8e176">
<h4 id="org6f8e176">Enum Setup</h4>
<div class="outline-text-4" id="text-org6f8e176">
<div class="highlight">
<pre><span></span><span class="nd">@unique</span>
<span class="k">class</span> <span class="nc">Axis</span><span class="p">(</span><span class="n">Enum</span><span class="p">):</span>
    <span class="n">ROWS</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">COLUMNS</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgeff3af3">
<h4 id="orgeff3af3">Named Tuples</h4>
<div class="outline-text-4" id="text-orgeff3af3">
<div class="highlight">
<pre><span></span><span class="n">Gradients</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Gradients"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>

<span class="n">Weights</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Weights"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"input_weights"</span><span class="p">,</span> <span class="s2">"hidden_weights"</span><span class="p">,</span> <span class="s2">"input_bias"</span><span class="p">,</span> <span class="s2">"hidden_bias"</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org23b175f">
<h4 id="org23b175f">The CBOW Model</h4>
<div class="outline-text-4" id="text-org23b175f">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">CBOW</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""A continuous bag of words model builder</span>

<span class="sd">    Args:</span>
<span class="sd">     hidden: number of rows in the hidden layer</span>
<span class="sd">     vocabulary_size: number of tokens in the vocabulary</span>
<span class="sd">     learning_rate: learning rate for back-propagation updates</span>
<span class="sd">     random_seed: int</span>
<span class="sd">    """</span>
    <span class="n">hidden</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.03</span>
    <span class="n">random_seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span>    
    <span class="n">_random_generator</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># layer one</span>
    <span class="n">_input_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_input_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>

    <span class="c1"># hidden layer</span>
    <span class="n">_hidden_weights</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_hidden_bias</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org29db65e"></a>The Random Generator<br>
<div class="outline-text-5" id="text-org29db65e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">random_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PCG64</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""The random number generator"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_seed</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_random_generator</span>
</pre></div>
</div>
</li>
<li><a id="orge3ed843"></a>First Layer Weights<br>
<div class="outline-text-5" id="text-orge3ed843">
<p>These are initialized using numpy's new generator. I originally using their standard-normal version by mistake and the model did horrible. Using the <a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.random.html#numpy.random.Generator.random">Generator.random</a> gives you a uniform distribution which seems to be what you're supposed to use.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Weights for the first layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span>
</pre></div>
</div>
</li>
<li><a id="orgcde46dc"></a>First Layer Bias<br>
<div class="outline-text-5" id="text-orgcde46dc">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">input_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Bias for the input layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span>
</pre></div>
</div>
</li>
<li><a id="orgf34237e"></a>Hidden Layer Weights<br>
<div class="outline-text-5" id="text-orgf34237e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""The weights for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span>
</pre></div>
</div>
</li>
<li><a id="org2d53c7d"></a>Hidden Layer Bias<br>
<div class="outline-text-5" id="text-org2d53c7d">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">hidden_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Bias for the hidden layer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span>
</pre></div>
</div>
</li>
<li><a id="org743be5c"></a>Softmax<br>
<div class="outline-text-5" id="text-org743be5c">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculate the softmax</span>

<span class="sd">    Args: </span>
<span class="sd">       scores: output scores from the hidden layer</span>
<span class="sd">    Returns: </span>
<span class="sd">       yhat: prediction (estimate of y)"""</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">/</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">ROWS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org44ebe2d"></a>Forward Propagation<br>
<div class="outline-text-5" id="text-org44ebe2d">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""makes a model prediction</span>

<span class="sd">    Args:</span>
<span class="sd">     data: x-values to train on</span>

<span class="sd">    Returns:</span>
<span class="sd">     output, first-layer output</span>
<span class="sd">    """</span>
    <span class="n">first_layer_output</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
                                  <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">second_layer_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">first_layer_output</span><span class="p">)</span>
                   <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">second_layer_output</span><span class="p">,</span> <span class="n">first_layer_output</span>
</pre></div>
</div>
</li>
<li><a id="org47123cf"></a>Gradients<br>
<div class="outline-text-5" id="text-org47123cf">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradients</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
              <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Gradients</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""does the gradient calculation for back-propagation</span>

<span class="sd">    This is broken out to be able to troubleshoot/compare it</span>

<span class="sd">   Args:</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    Returns:</span>
<span class="sd">     Gradients for input_weight, hidden_weight, input_bias, hidden_bias</span>
<span class="sd">    """</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">predicted</span> <span class="o">-</span> <span class="n">actual</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">difference</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">l1</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">difference</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">input_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_weights_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">input_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span>
                                    <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                    <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="n">hidden_bias_gradient</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span>
                                     <span class="n">axis</span><span class="o">=</span><span class="n">Axis</span><span class="o">.</span><span class="n">COLUMNS</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                                     <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">Gradients</span><span class="p">(</span><span class="n">input_weights</span><span class="o">=</span><span class="n">input_weights_gradient</span><span class="p">,</span>
                     <span class="n">hidden_weights</span><span class="o">=</span><span class="n">hidden_weights_gradient</span><span class="p">,</span>
                     <span class="n">input_bias</span><span class="o">=</span><span class="n">input_bias_gradient</span><span class="p">,</span>
                     <span class="n">hidden_bias</span><span class="o">=</span><span class="n">hidden_bias_gradient</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org5414b63"></a>Backward Propagation<br>
<div class="outline-text-5" id="text-org5414b63">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
             <span class="n">hidden_input</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Does back-propagation to update the weights</span>

<span class="sd">   Arg:s</span>
<span class="sd">     data: the input x value</span>
<span class="sd">     predicted: what our model predicted the labels for the data should be</span>
<span class="sd">     actual: what the actual labels should have been</span>
<span class="sd">     hidden_input: the input to the hidden layer</span>
<span class="sd">    """</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span>
                               <span class="n">predicted</span><span class="o">=</span><span class="n">predicted</span><span class="p">,</span>
                               <span class="n">actual</span><span class="o">=</span><span class="n">actual</span><span class="p">,</span>
                               <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
    <span class="c1"># I don't have setters for the properties so use the private variables</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="orgd7a6a80"></a>Call<br>
<div class="outline-text-5" id="text-orgd7a6a80">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""makes a prediction on the data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: input data for the prediction</span>

<span class="sd">    Returns:</span>
<span class="sd">     softmax of model output</span>
<span class="sd">    """</span>
    <span class="n">output</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgb8a68e3">
<h4 id="orgb8a68e3">Batch Generator</h4>
<div class="outline-text-4" id="text-orgb8a68e3">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Batches</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Generates batches of data</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the source of the data to generate (training data)</span>
<span class="sd">     word_to_index: dict mapping the word to the vocabulary index</span>
<span class="sd">     half_window: number of tokens on either side of word to grab</span>
<span class="sd">     batch_size: the number of entries per batch</span>
<span class="sd">     batches: number of batches to generate before quitting</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">word_to_index</span><span class="p">:</span> <span class="nb">dict</span>
    <span class="n">half_window</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">batches</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">repetitions</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>    
    <span class="n">_vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vectors</span><span class="p">:</span> <span class="nb">object</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgbbab027"></a>Vocabulary Size<br>
<div class="outline-text-5" id="text-orgbbab027">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Number of tokens in the vocabulary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span>
</pre></div>
</div>
</li>
<li><a id="orgb8c4776"></a>Vectors<br>
<div class="outline-text-5" id="text-orgb8c4776">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""our vector-generator started up"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vector_generator</span><span class="p">()</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vectors</span>
</pre></div>
</div>
</li>
<li><a id="org4418ee2"></a>Indices and Frequencies<br>
<div class="outline-text-5" id="text-org4418ee2">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">indices_and_frequencies</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context_words</span><span class="p">:</span> <span class="nb">list</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""combines word-indexes and frequency counts-dict</span>

<span class="sd">    Args:</span>
<span class="sd">     context_words: words to get the indices for</span>

<span class="sd">    Returns:</span>
<span class="sd">     list of (word-index, word-count) tuples built from context_words</span>
<span class="sd">    """</span>
    <span class="n">frequencies</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">context_words</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">[(</span><span class="n">indices</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">frequencies</span><span class="p">[</span><span class="n">context_words</span><span class="p">[</span><span class="n">index</span><span class="p">]])</span>
            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))]</span>
</pre></div>
</div>
</li>
<li><a id="org2e47eb6"></a>Vectors<br>
<div class="outline-text-5" id="text-org2e47eb6">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">vector_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Generates vectors infinitely</span>

<span class="sd">    x: fraction of context words represented by word</span>
<span class="sd">    y: array with 1 where center word is in the vocabulary and 0 elsewhere</span>

<span class="sd">    Yields:</span>
<span class="sd">     tuple of x, y </span>
<span class="sd">    """</span>
    <span class="n">location</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="n">center_word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">location</span><span class="p">]</span>
        <span class="n">y</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">center_word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">context_words</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span><span class="p">):</span> <span class="n">location</span><span class="p">]</span>
            <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[(</span><span class="n">location</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">location</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span>

        <span class="k">for</span> <span class="n">word_index</span><span class="p">,</span> <span class="n">frequency</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context_words</span><span class="p">):</span>
            <span class="n">x</span><span class="p">[</span><span class="n">word_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">frequency</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">context_words</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
        <span class="n">location</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"location in data is being set to 0"</span><span class="p">)</span>
            <span class="n">location</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
<li><a id="org9b5c42a"></a>Iterator Method<br>
<div class="outline-text-5" id="text-org9b5c42a">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""makes this into an iterator"""</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</li>
<li><a id="org5493dc1"></a>Next Method<br>
<div class="outline-text-5" id="text-org5493dc1">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Creates the batches and returns them</span>

<span class="sd">    Returns:</span>
<span class="sd">     x, y batches</span>
<span class="sd">    """</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">StopIteration</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">repetitions</span> <span class="o">+=</span> <span class="mi">1</span>    
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="k">return</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org794da07">
<h4 id="org794da07">The Trainer</h4>
<div class="outline-text-4" id="text-org794da07">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TheTrainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Something to train the model</span>

<span class="sd">    Args:</span>
<span class="sd">     model: thing to train</span>
<span class="sd">     batches: batch generator</span>
<span class="sd">     learning_impairment: rate to slow the model's learning</span>
<span class="sd">     impairment_point: how frequently to impair the learner</span>
<span class="sd">     emit_point: how frequently to emit messages</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span>
    <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span>
    <span class="n">learning_impairment</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.66</span>
    <span class="n">impairment_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span>
    <span class="n">emit_point</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span>
    <span class="n">_losses</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org587b902"></a>Losses<br>
<div class="outline-text-5" id="text-org587b902">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">losses</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Holder for the training losses"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span>
</pre></div>
</div>
</li>
<li><a id="org349e824"></a>Gradient Descent<br>
<div class="outline-text-5" id="text-org349e824">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>    
<span class="w">    </span><span class="sd">"""Trains the model using gradient descent</span>
<span class="sd">    """</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">x_y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x_y</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_loss</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_weights</span> <span class="o">=</span> <span class="n">Weights</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
                            <span class="n">hidden_input</span><span class="o">=</span><span class="n">hidden_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">impairment_point</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_impairment</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"new learning rate: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">learning_rate</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="p">((</span><span class="n">repetitions</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">emit_point</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">repetitions</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: loss=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="n">repetitions</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
    <span class="k">return</span> 
</pre></div>
</div>
</li>
<li><a id="org7e79a88"></a>Cross-Entropy-Loss<br>
<div class="outline-text-5" id="text-org7e79a88">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">predicted</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                       <span class="n">actual</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculates the cross-entropy loss</span>

<span class="sd">    Args:</span>
<span class="sd">     predicted: array with the model's guesses</span>
<span class="sd">     actual: array with the actual labels</span>

<span class="sd">    Returns:</span>
<span class="sd">     the cross-entropy loss</span>
<span class="sd">    """</span>
    <span class="n">log_probabilities</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predicted</span><span class="p">),</span> <span class="n">actual</span><span class="p">)</span>
                         <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">predicted</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">actual</span><span class="p">))</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probabilities</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org50eadc0">
<h3 id="org50eadc0">Testing It</h3>
<div class="outline-text-3" id="text-org50eadc0">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings</span> <span class="kn">import</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">TheTrainer</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">V</span><span class="p">)</span>


<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">V</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="n">V</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">tmp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
                   <span class="p">])</span>
<span class="n">tmp_sm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.73105858</span><span class="p">,</span> <span class="mf">0.88079708</span><span class="p">],</span>
                         <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.26894142</span><span class="p">,</span> <span class="mf">0.11920292</span><span class="p">]])</span>


<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_sm</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org22d7914">
<h4 id="org22d7914">Forward Propagation</h4>
<div class="outline-text-4" id="text-org22d7914">
<div class="highlight">
<pre><span></span><span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_V</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">tmp_x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">V</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.55379268</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.58960774</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.50722933</span><span class="p">]]</span>
<span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">[[</span><span class="mf">0.92477674</span><span class="p">],</span>
     <span class="p">[</span><span class="mf">1.02487333</span><span class="p">]]</span>
<span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_h</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgca20e6a">
<h4 id="orgca20e6a">Cross Entropy Loss</h4>
<div class="outline-text-4" id="text-orgca20e6a">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">tmp_V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tmp_cost</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">tmp_cost</span><span class="p">,</span> <span class="mf">9.9560</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org508d870">
<h4 id="org508d870">Back Propagation</h4>
<div class="outline-text-4" id="text-org508d870">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># get a batch of data</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>
<span class="n">tmp_z</span><span class="p">,</span> <span class="n">tmp_h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>
<span class="n">tmp_yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tmp_z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"tmp_yhat.shape: </span><span class="si">{</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">gradients</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">hidden_input</span><span class="o">=</span><span class="n">tmp_h</span><span class="p">)</span>
<span class="n">tmp_grad_W1</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">,</span> <span class="n">tmp_grad_b2</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_yhat</span><span class="p">,</span> <span class="n">tmp_y</span><span class="p">,</span> <span class="n">tmp_h</span><span class="p">,</span> <span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_weights</span><span class="p">,</span> <span class="n">tmp_grad_W1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_weights</span><span class="p">,</span> <span class="n">tmp_grad_W2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">input_bias</span><span class="p">,</span> <span class="n">tmp_grad_b1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">hidden_bias</span><span class="p">,</span> <span class="n">tmp_grad_b2</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">tmp_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_z</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_h</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_yhat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">4</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5778</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">50</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">tmp_grad_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="mi">5778</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4be8afb">
<h4 id="org4be8afb">Putting Some Stuff Together</h4>
<div class="outline-text-4" id="text-org4be8afb">
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>

<span class="c1"># using their initial weights</span>
<span class="n">tmp_W1</span><span class="p">,</span> <span class="n">tmp_W2</span><span class="p">,</span> <span class="n">tmp_b1</span><span class="p">,</span> <span class="n">tmp_b2</span> <span class="o">=</span> <span class="n">initialize_model</span><span class="p">(</span><span class="n">tmp_N</span><span class="p">,</span><span class="n">tmp_V</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">tmp_N</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">tmp_V</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_W2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">input_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b1</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_bias</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">tmp_b2</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">=</span> <span class="n">tmp_W1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">=</span> <span class="n">tmp_W2</span>
<span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">=</span> <span class="n">tmp_b1</span>
<span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">=</span> <span class="n">tmp_b2</span>

<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
<span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">)</span>

<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="o">.</span><span class="n">cross_entropy_loss</span><span class="p">(</span><span class="n">predicted</span><span class="o">=</span><span class="n">prediction</span><span class="p">,</span> <span class="n">actual</span><span class="o">=</span><span class="n">tmp_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">))</span>
</pre></div>
<pre class="example">
11.871189103548419
11.871189103548419
9.956016099656951
9.956016099656951
</pre>
<p>I changed the weights to use the uniform distribution which seems to work better, but weirdly it still does a little worse initially. The random-seed seems to be different for the old numpy random and their new generator.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org9e60158">
<h4 id="org9e60158">The Batches</h4>
<div class="outline-text-4" id="text-org9e60158">
<p>The original batch-generator had a couple of bugs in it. To avoid them pass in <code>original=True</code>.</p>
<div class="highlight">
<pre><span></span><span class="n">tmp_C</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">tmp_N</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tmp_batch_size</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">batches</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">tmp_C</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">)</span>


<span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span>
                                <span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">original</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_x</span><span class="p">,</span> <span class="n">old_x</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">tmp_y</span><span class="p">,</span> <span class="n">old_y</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>


<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="c1">#expect(numpy.allclose(tmp_x, old_x)).to(be_true)</span>
<span class="c1">#expect(numpy.allclose(tmp_y, old_y)).to(be_true)</span>

<span class="n">old_x</span><span class="p">,</span> <span class="n">old_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">old_generator</span><span class="p">)</span>
<span class="n">tmp_x</span><span class="p">,</span> <span class="n">tmp_y</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb9986ea">
<h4 id="orgb9986ea">Gradient Descent</h4>
<div class="outline-text-4" id="text-orgb9986ea">
<div class="highlight">
<pre><span></span><span class="n">hidden_layers</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train</span><span class="p">()</span>
</pre></div>
<pre class="example" id="orgc106365">
10: loss=12.949165499168524
20: loss=7.1739091478289225
30: loss=13.431976455238479
40: loss=4.0062314323745545
50: loss=11.595407087927406
60: loss=10.41983077447342
70: loss=7.843047289924249
80: loss=12.529314536141994
90: loss=14.122707806423126
new learning rate: 0.0198
100: loss=10.80530164111974
110: loss=4.624869443165228
120: loss=5.552813055551899
130: loss=8.483428176366933
140: loss=9.047299388851195
150: loss=4.841072955589429
</pre></div>
</div>
<div class="outline-4" id="outline-container-org5e7d89b">
<h4 id="org5e7d89b">Gradient Re-do</h4>
<div class="outline-text-4" id="text-org5e7d89b">
<p>Something's wrong with the trainer's gradient descent so I'm going to try and update the original function to do it.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">CBOW</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                     <span class="n">num_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batches</span><span class="p">:</span> <span class="n">Batches</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.03</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""This is the gradient_descent function</span>

<span class="sd">    Args: </span>
<span class="sd">       data:      text</span>
<span class="sd">       word2Ind:  words to Indices</span>
<span class="sd">       N:         dimension of hidden vector  </span>
<span class="sd">       V:         dimension of vocabulary </span>
<span class="sd">       num_iters: number of iterations  </span>

<span class="sd">    Returns: </span>
<span class="sd">       W1, W2, b1, b2:  updated matrices and biases   </span>
<span class="sd">    """</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
    <span class="n">iters</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Get yhat</span>
        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="c1"># Get cost</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">((</span><span class="n">iters</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"iters: </span><span class="si">{</span><span class="n">iters</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> cost: </span><span class="si">{</span><span class="n">cost</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
        <span class="n">grad_W1</span><span class="p">,</span> <span class="n">grad_W2</span><span class="p">,</span> <span class="n">grad_b1</span><span class="p">,</span> <span class="n">grad_b2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                             <span class="n">yhat</span><span class="p">,</span>
                                                             <span class="n">y</span><span class="p">,</span>
                                                             <span class="n">h</span><span class="p">)</span>

        <span class="c1"># Update weights and biases</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_weights</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_W2</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_input_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b1</span>
        <span class="n">model</span><span class="o">.</span><span class="n">_hidden_bias</span> <span class="o">-=</span>  <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_b2</span>

        <span class="c1">### END CODE HERE ###</span>

        <span class="n">iters</span> <span class="o">+=</span> <span class="mi">1</span> 
        <span class="k">if</span> <span class="n">iters</span> <span class="o">==</span> <span class="n">num_iters</span><span class="p">:</span> 
            <span class="k">break</span>
        <span class="k">if</span> <span class="n">iters</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha</span> <span class="o">*=</span> <span class="mf">0.66</span>

    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="c1"># batch_generator(data, word2Ind, C, batch_size)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org5566e69">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>So, something's wrong with the gradient descent.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="orgfb4068a">
iters: 10 cost: 0.407862
iters: 20 cost: 0.090807
iters: 30 cost: 0.050924
iters: 40 cost: 0.035379
iters: 50 cost: 0.027105
iters: 60 cost: 0.021969
iters: 70 cost: 0.018470
iters: 80 cost: 0.015932
iters: 90 cost: 0.014008
iters: 100 cost: 0.012499
iters: 110 cost: 0.011631
iters: 120 cost: 0.010911
iters: 130 cost: 0.010274
iters: 140 cost: 0.009708
iters: 150 cost: 0.009201
</pre>
<p>It looks like it's the batches.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org04a335e">
<h4 id="org04a335e">Troubleshooting the Batches</h4>
<div class="outline-text-4" id="text-org04a335e">
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>

<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">context</span> <span class="o">=</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span><span class="p">]</span> <span class="o">+</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">[</span><span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">start</span> <span class="o">+</span> <span class="n">half_window</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">packed_1</span> <span class="o">=</span> <span class="n">index_with_frequency</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span>
<span class="n">packed_2</span> <span class="o">=</span> <span class="n">batches</span><span class="o">.</span><span class="n">indices_and_frequencies</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">packed_1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">contain_exactly</span><span class="p">(</span><span class="o">*</span><span class="n">packed_2</span><span class="p">))</span>
</pre></div>
<p>So the indices and frequencies is okay.</p>
<div class="highlight">
<pre><span></span><span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">vectors</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">half_window</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">):</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>And the vectors look okay.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">repetition</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># batch = next(batches)</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">batches</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batches</span><span class="o">.</span><span class="n">batch_size</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">batch_y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">newx</span><span class="p">,</span> <span class="n">newy</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">batch_y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
            <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">newx</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
            <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
                <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="k">break</span>
</pre></div>
<p>So, weirdly, rolling the <code>__next__=</code> by hand seems to work.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span><span class="p">,</span> <span class="n">new</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">old_generator</span><span class="p">,</span> <span class="n">batches</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<pre class="example">
1
</pre>
<p>But not the batches.</p>
<div class="highlight">
<pre><span></span><span class="n">old_generator</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">tmp_C</span><span class="p">,</span> <span class="n">tmp_batch_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">tmp_batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">repetition</span><span class="p">,</span> <span class="n">repetitions</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">150</span>
<span class="k">for</span> <span class="n">old</span> <span class="ow">in</span> <span class="n">old_generator</span><span class="p">:</span>
    <span class="n">new</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">expect</span><span class="p">((</span><span class="n">old</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">new</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">all</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">AssertionError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">repetition</span><span class="p">)</span>
        <span class="k">break</span>
    <span class="n">repetition</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">repetition</span> <span class="o">==</span> <span class="n">repetitions</span><span class="p">:</span>
        <span class="k">break</span>
</pre></div>
<p>Actually, it looks like the old generator might be broken.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">batch_generator</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="c1">#batches = Batches(data=cleaner.processed, word_to_index=meta.word_to_index,</span>
<span class="c1">#                  half_window=half_window, batch_size=batch_size, batches=repetitions)</span>

<span class="n">grady_the_ent</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">repetitions</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">batches</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org5d03eca">
iters: 10 cost: 12.949165
iters: 20 cost: 7.173909
iters: 30 cost: 13.431976
iters: 40 cost: 4.006231
iters: 50 cost: 11.595407
iters: 60 cost: 10.419831
iters: 70 cost: 7.843047
iters: 80 cost: 12.529315
iters: 90 cost: 14.122708
iters: 100 cost: 10.805302
iters: 110 cost: 4.624869
iters: 120 cost: 5.552813
iters: 130 cost: 8.483428
iters: 140 cost: 9.047299
iters: 150 cost: 4.841073
</pre>
<p>The old generator wasn't creating new lists every time so it was just fitting the same batch of data every time in fact it had a while loop instead of a conditional so it was just creating one batch with the same x and y lists repeated over and over so it should really be the worse performance, not the really good performance the original generator gave. I didn't re-run the ones above but this next set is being run after fixing my implementation.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-12-16 14:15:54,530 graeae.timers.timer start: Started: 2020-12-16 14:15:54.530779
2020-12-16 14:16:18,600 graeae.timers.timer end: Ended: 2020-12-16 14:16:18.600880
2020-12-16 14:16:18,602 graeae.timers.timer end: Elapsed: 0:00:24.070101
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
11.99601105791401 8.827228045367379
</pre>
<p>Not a huge improvement, but it didn't run for a long time either.</p>
<div class="highlight">
<pre><span></span><span class="n">hidden_layer</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">half_window</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">repetitions</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">CBOW</span><span class="p">(</span><span class="n">hidden</span><span class="o">=</span><span class="n">hidden_layers</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">Batches</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">cleaner</span><span class="o">.</span><span class="n">processed</span><span class="p">,</span> <span class="n">word_to_index</span><span class="o">=</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">,</span>
                  <span class="n">half_window</span><span class="o">=</span><span class="n">half_window</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">batches</span><span class="o">=</span><span class="n">repetitions</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batches</span><span class="p">,</span> <span class="n">emit_point</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">trainer</span><span class="p">()</span>
</pre></div>
<pre class="example" id="org3594b9d">
2020-12-16 14:40:13,275 graeae.timers.timer start: Started: 2020-12-16 14:40:13.275964
new learning rate: 0.0198
100: loss=9.138356897918037
new learning rate: 0.013068000000000001
200: loss=9.077599951734605
new learning rate: 0.008624880000000001
300: loss=8.827228045367379
new learning rate: 0.005692420800000001
400: loss=8.556788482755191
new learning rate: 0.003756997728000001
500: loss=8.92744766914796
new learning rate: 0.002479618500480001
600: loss=9.052677036205138
new learning rate: 0.0016365482103168007
700: loss=8.914532962726918
new learning rate: 0.0010801218188090885
800: loss=8.885698480310062
new learning rate: 0.0007128804004139984
900: loss=9.042620463323736
2020-12-16 14:41:33,457 graeae.timers.timer end: Ended: 2020-12-16 14:41:33.457065
2020-12-16 14:41:33,458 graeae.timers.timer end: Elapsed: 0:01:20.181101
new learning rate: 0.000470501064273239
1000: loss=9.239992952104755
</pre>
<p>Hmm doesn't seem to be improving.</p>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">time_series</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">()</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Loss per Repetition"</span><span class="p">,</span>
                                   <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
                                   <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">time_series</span> <span class="o">*</span> <span class="n">line</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"training_1000"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="training_1000.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Since the losses are in a Series we can use its <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.idxmin.html">idxmin</a> method to see when the losses bottomed out.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">idxmin</span><span class="p">())</span>
</pre></div>
<pre class="example">
247
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">247</span><span class="p">],</span> <span class="n">losses</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<pre class="example">
8.186490214727549 9.239992952104755
</pre>
<p>So it did the best at 247 and then got a little worse as we went along.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">word_to_index</span><span class="p">)</span><span class="o">/</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
<pre class="example">
45.140625
</pre>
<p>We exhausted our data after 45 batches so I guess it's overfitting after a while.</p>
</div>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/cbow/" rel="tag">cbow</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/word-embeddings/" rel="tag">word embeddings</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../word-embeddings-shakespeare-data/" rel="prev" title="Word Embeddings: Shakespeare Data">Previous post</a></li>
<li class="next"><a href="../word-embeddings-visualizing-the-embeddings/" rel="next" title="Word Embeddings: Visualizing the Embeddings">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
