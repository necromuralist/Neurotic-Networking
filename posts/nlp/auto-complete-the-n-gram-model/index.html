<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Implementing the N-Gram Language model for auto-complete." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Auto-Complete: the N-Gram Model | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-the-n-gram-model/index.html" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../auto-complete-pre-process-the-data-ii/index.html" rel="prev" title="Auto-Complete: Pre-Process the Data II" type="text/html">
<link href="../auto-complete-perplexity/index.html" rel="next" title="Auto-Complete: Perplexity" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Auto-Complete: the N-Gram Model" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-the-n-gram-model/index.html" property="og:url">
<meta content="Implementing the N-Gram Language model for auto-complete." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-04T15:17:18-08:00" property="article:published_time">
<meta content="auto-complete" property="article:tag">
<meta content="n-gram" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../pages/">Pages</a></li>
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item dropdown"><a aria-expanded="false" aria-haspopup="true" class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">Monkey Pages</a>
<div class="dropdown-menu"><a class="dropdown-item" href="https://necromuralist.github.io/">The Cloistered Monkey</a> <a class="dropdown-item" href="https://necromuralist.github.io/Ape-Iron/">Ape Iron</a> <a class="dropdown-item" href="https://necromuralist.github.io/Bowling-For-Data/">Bowling For Data</a> <a class="dropdown-item" href="https://necromuralist.github.io/Beach-Pig-Thigh/">Beach-Pig Rump & Thigh</a> <a class="dropdown-item" href="https://necromuralist.github.io/Visions-Voices-Data/">Visions, Voices, Data</a></div>
</li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="#">Auto-Complete: the N-Gram Model</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="#" rel="bookmark"><time class="published dt-published" datetime="2020-12-04T15:17:18-08:00" itemprop="datePublished" title="2020-12-04 15:17">2020-12-04 15:17</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="index.html#org762b273">Develop an N-Gram Based Language Model</a>
<ul>
<li><a href="index.html#org089ee05">Imports</a></li>
<li><a href="index.html#orgb346920">Set Up</a></li>
</ul>
</li>
<li><a href="index.html#org9a2c370">Middle</a>
<ul>
<li><a href="index.html#orge269bdb">Count N-Grams</a>
<ul>
<li><a href="index.html#org3500948">Hints</a></li>
<li><a href="index.html#org42075e1">Test It</a></li>
</ul>
</li>
<li><a href="index.html#orge5b485d">Probability Estimates</a>
<ul>
<li><a href="index.html#org9845e6b">Hints</a></li>
<li><a href="index.html#orge53b8b4">Test Code</a></li>
</ul>
</li>
<li><a href="index.html#org4c969cf">Estimate probabilities for all words</a>
<ul>
<li><a href="index.html#org112eecd">Test It</a></li>
</ul>
</li>
<li><a href="index.html#orgd26c616">Count and probability matrices</a>
<ul>
<li><a href="index.html#org968355a">Show trigram counts</a></li>
</ul>
</li>
<li><a href="index.html#org5087a4d">Probability Matrix</a></li>
<li><a href="index.html#org3e2b3bc">Bundle It Up</a>
<ul>
<li><a href="index.html#orgaeee37e">Imports</a></li>
<li><a href="index.html#org2b89fb5">The N-Gram</a></li>
<li><a href="index.html#orgd22de05">N-Gram Probability</a></li>
<li><a href="index.html#orgc643492">Test It Out</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="index.html#orgc62ec0a">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org762b273">
<h2 id="org762b273">Develop an N-Gram Based Language Model</h2>
<div class="outline-text-2" id="text-org762b273">
<p>We'll continue on from the <a href="../auto-complete-pre-process-the-data-ii/index.html">previous post</a> in which we finished pre-processing the data to build our <a href="../auto-complete/index.html">Auto-Complete</a> system.</p>
<p>In this section, you will develop the n-grams language model.</p>
<ul class="org-ul">
<li>Assume the probability of the next word depends only on the previous n-gram.</li>
<li>The previous n-gram is the series of the previous 'n' words.</li>
</ul>
<p>The conditional probability for the word at position 't' in the sentence, given that the words preceding it are \(w_{t-1}, w_{t-2} \cdots w_{t-n}\) is:</p>
<p>\[ P(w_t | w_{t-1}\dots w_{t-n}) \tag{1} \]</p>
<p>You can estimate this probability by counting the occurrences of these series of words in the training data.</p>
<ul class="org-ul">
<li>The probability can be estimated as a ratio, where</li>
<li>The numerator is the number of times word 't' appears after words t-1 through t-n appear in the training data.</li>
<li>The denominator is the number of times word t-1 through t-n appears in the training data.</li>
</ul>
<p>\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} \]</p>
<ul class="org-ul">
<li>The function \(C(\cdots)\) denotes the number of occurences of the given sequence.</li>
<li>\(\hat{P}\) means the estimation of <i>P</i>.</li>
<li>Notice that denominator of the equation (2) is the number of occurence of the previous <i>n</i> words, and the numerator is the same sequence followed by the word \(w_t\).</li>
</ul>
<p>Later, you will modify the equation (2) by adding k-smoothing, which avoids errors when any counts are zero.</p>
<p>The equation (2) tells us that to estimate probabilities based on n-grams, you need the counts of n-grams (for denominator) and (n+1)-grams (for numerator).</p>
</div>
<div class="outline-3" id="outline-container-org089ee05">
<h3 id="org089ee05">Imports</h3>
<div class="outline-text-3" id="text-org089ee05">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">expect</span><span class="p">,</span> <span class="n">have_keys</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>

<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb346920">
<h3 id="orgb346920">Set Up</h3>
<div class="outline-text-3" id="text-orgb346920">
<div class="highlight">
<pre><span></span><span class="n">TABLE</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">tabulate</span><span class="p">,</span> <span class="n">tablefmt</span><span class="o">=</span><span class="s2">"orgtbl"</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="s2">"keys"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org9a2c370">
<h2 id="org9a2c370">Middle</h2>
<div class="outline-text-2" id="text-org9a2c370"></div>
<div class="outline-3" id="outline-container-orge269bdb">
<h3 id="orge269bdb">Count N-Grams</h3>
<div class="outline-text-3" id="text-orge269bdb">
<p>Next, you will implement a function that computes the counts of n-grams for an arbitrary number \(n\).</p>
<p>When computing the counts for n-grams, prepare the sentence beforehand by prepending <i>n-1</i> starting markers "&lt;s\&gt;" to indicate the beginning of the sentence.</p>
<ul class="org-ul">
<li>For example, in the bi-gram model (N=2), a sequence with two start tokens "&lt;s\&gt;&lt;s\&gt;" should predict the first word of a sentence.</li>
<li>So, if the sentence is "I like food", modify it to be "&lt;s\&gt;&lt;s\&gt; I like food".</li>
<li>Also prepare the sentence for counting by appending an end token "&lt;e\&gt;" so that the model can predict when to finish a sentence.</li>
</ul>
<p>Technical note: In this implementation, you will store the counts as a dictionary.</p>
<ul class="org-ul">
<li>The key of each key-value pair in the dictionary is a <b>tuple</b> of n words (and not a list)</li>
<li>The value in the key-value pair is the number of occurrences.</li>
<li>The reason for using a tuple as a key instead of a list is because a list in Python is a mutable object (it can be changed after it is first created). A tuple is "immutable", so it cannot be altered after it is first created. This makes a tuple suitable as a data type for the key in a dictionary.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org3500948">
<h4 id="org3500948">Hints</h4>
<div class="outline-text-4" id="text-org3500948">
<ul class="org-ul">
<li>To prepend or append, you can create lists and concatenate them using the + operator</li>
<li>To create a list of a repeated value, you can follow this syntax: <code>['a'] * 3</code> to get <code>['a','a','a']</code></li>
<li>To set the range for index 'i', think of this example: An n-gram where n=2 (bigram), and the sentence is length N=5 (including two start tokens and one end token). So the index positions are <code>[0,1,2,3,4]</code>. The largest index 'i' where a bigram can start is at position i=3, because the word tokens at position 3 and 4 will form the bigram.</li>
<li>Remember that the <code>range()</code> function excludes the value that is used for the maximum of the range. <code>range(3)</code> produces (0,1,2) but excludes 3.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1">### GRADED FUNCTION: count_n_grams ###</span>
<span class="k">def</span> <span class="nf">count_n_grams</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">start_token</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="n">end_token</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'&lt;e&gt;'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Count all n-grams in the data</span>

<span class="sd">    Args:</span>
<span class="sd">       data: List of lists of words</span>
<span class="sd">       n: number of words in a sequence</span>

<span class="sd">    Returns:</span>
<span class="sd">       A dictionary that maps a tuple of n-words to its frequency</span>
<span class="sd">    """</span>

    <span class="c1"># Initialize dictionary of n-grams and their counts</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Go through each sentence in the data</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span> <span class="c1"># complete this line</span>

        <span class="c1"># prepend start token n times, and  append &lt;e&gt; one time</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="p">[</span><span class="n">start_token</span><span class="p">]</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="p">[</span><span class="n">end_token</span><span class="p">]</span>

        <span class="c1"># convert list to tuple</span>
        <span class="c1"># So that the sequence of words can be used as</span>
        <span class="c1"># a key in the dictionary</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

        <span class="c1"># Use 'i' to indicate the start of the n-gram</span>
        <span class="c1"># from index 0</span>
        <span class="c1"># to the last index where the end of the n-gram</span>
        <span class="c1"># is within the sentence.</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span> <span class="c1"># complete this line</span>

            <span class="c1"># Get the n-gram from i to i+n</span>
            <span class="n">n_gram</span> <span class="o">=</span> <span class="n">sentence</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="n">n</span><span class="p">]</span>

            <span class="c1"># check if the n-gram is in the dictionary</span>
            <span class="k">if</span> <span class="n">n_gram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span> <span class="c1"># complete this line</span>

                <span class="c1"># Increment the count for this n-gram</span>
                <span class="n">n_grams</span><span class="p">[</span><span class="n">n_gram</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Initialize this n-gram count to 1</span>
                <span class="n">n_grams</span><span class="p">[</span><span class="n">n_gram</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">n_grams</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org42075e1">
<h4 id="org42075e1">Test It</h4>
<div class="outline-text-4" id="text-org42075e1">
<div class="highlight">
<pre><span></span><span class="c1"># **** Set Up ****</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>

<span class="c1"># **** Unigram ****</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Uni-gram:"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'i'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'like'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'cat'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'&lt;e&gt;'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'this'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'dog'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'is'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>

<span class="c1"># **** Bi-Gram ****</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Bi-gram:"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'&lt;s&gt;'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'cat'</span><span class="p">,</span> <span class="s1">'&lt;e&gt;'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'this'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">):</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<pre class="example">
Uni-gram:
{('&lt;s&gt;',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('&lt;e&gt;',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}
Bi-gram:
{('&lt;s&gt;', '&lt;s&gt;'): 2, ('&lt;s&gt;', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '&lt;e&gt;'): 2, ('&lt;s&gt;', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge5b485d">
<h3 id="orge5b485d">Probability Estimates</h3>
<div class="outline-text-3" id="text-orge5b485d">
<p>Next, estimate the probability of a word given the prior 'n' words using the n-gram counts.</p>
<p>\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n)}{C(w_{t-1}\dots w_{t-n})} \tag{2} \]</p>
<p>This formula doesn't work when a count of an n-gram is zero..</p>
<ul class="org-ul">
<li>Suppose we encounter an n-gram that did not occur in the training data.</li>
<li>Then, the equation (2) cannot be evaluated (it becomes zero divided by zero).</li>
</ul>
<p>A way to handle zero counts is to add k-smoothing.</p>
<ul class="org-ul">
<li>K-smoothing adds a positive constant <i>k</i> to each numerator and \(k \times |V|\) in the denominator, where \(|V|\) is the number of words in the vocabulary.</li>
</ul>
<p>\[ \hat{P}(w_t | w_{t-1}\dots w_{t-n}) = \frac{C(w_{t-1}\dots w_{t-n}, w_n) + k}{C(w_{t-1}\dots w_{t-n}) + k|V|} \tag{3} \]</p>
<p>For n-grams that have a zero count, the equation (3) becomes \(\frac{1}{|V|}\).</p>
<ul class="org-ul">
<li>This means that any n-gram with zero count has the same probability of \(\frac{1}{|V|}\).</li>
</ul>
<p>Define a function that computes the probability estimate (3) from n-gram counts and a constant <i>k</i>.</p>
<ul class="org-ul">
<li>The function takes in a dictionary 'n_gram_counts', where the key is the n-gram and the value is the count of that n-gram.</li>
<li>The function also takes another dictionary n_plus1_gram_counts, which you'll use to find the count for the previous n-gram plus the current word.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org9845e6b">
<h4 id="org9845e6b">Hints</h4>
<div class="outline-text-4" id="text-org9845e6b">
<ul class="org-ul">
<li>To define a tuple containing a single value, add a comma after that value. For example: <code>('apple',)</code> is a tuple containing a single string 'apple'</li>
<li>To concatenate two tuples, use the '+' operator</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1">### GRADED FUNCTION: estimate_probability ###</span>
<span class="k">def</span> <span class="nf">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       word: next word</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of n-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of words in the vocabulary</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A probability</span>
<span class="sd">    """</span>
    <span class="c1"># convert list to tuple to use it as a dictionary key</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># Set the denominator</span>
    <span class="c1"># If the previous n-gram exists in the dictionary of n-gram counts,</span>
    <span class="c1"># Get its count.  Otherwise set the count to zero</span>
    <span class="c1"># Use the dictionary that has counts for n-grams</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="n">n_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Calculate the denominator using the count of the previous n gram</span>
    <span class="c1"># and apply k-smoothing</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span>

    <span class="c1"># Define n plus 1 gram as the previous n-gram plus the current word as a tuple</span>
    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>

    <span class="c1"># Set the count to the count in the dictionary,</span>
    <span class="c1"># otherwise 0 if not in the dictionary</span>
    <span class="c1"># use the dictionary that has counts for the n-gram plus current word</span>
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Define the numerator use the count of the n-gram plus current word,</span>
    <span class="c1"># and apply smoothing</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="n">k</span>

    <span class="c1"># Calculate the probability as the numerator divided by denominator</span>
    <span class="n">probability</span> <span class="o">=</span> <span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span>

    <span class="c1">### END CODE HERE ###</span>

    <span class="k">return</span> <span class="n">probability</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge53b8b4">
<h4 id="orge53b8b4">Test Code</h4>
<div class="outline-text-4" id="text-orge53b8b4">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span><span class="s2">"cat"</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_words</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.3333</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The estimated probability of word 'cat' given the previous n-gram 'a' is: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org4c969cf">
<h3 id="org4c969cf">Estimate probabilities for all words</h3>
<div class="outline-text-3" id="text-org4c969cf">
<p>The function defined below loops over all words in the vocabulary to calculate probabilities for all possible words.</p>
<ul class="org-ul">
<li>This function is provided for you.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probabilities</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary: List of words</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A dictionary mapping from next words to the probability.</span>
<span class="sd">    """</span>

    <span class="c1"># convert list to tuple to use it as a dictionary key</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>

    <span class="c1"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span>
    <span class="c1"># &lt;s&gt; is not needed since it should not appear as the next word</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">,</span> <span class="s2">"&lt;unk&gt;"</span><span class="p">]</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>

    <span class="n">probabilities</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="p">,</span> 
                                           <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> 
                                           <span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="n">probabilities</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability</span>

    <span class="k">return</span> <span class="n">probabilities</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org112eecd">
<h4 id="org112eecd">Test It</h4>
<div class="outline-text-4" id="text-org112eecd">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">estimate_probabilities</span><span class="p">(</span><span class="s2">"a"</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="p">{</span><span class="s1">'cat'</span><span class="p">:</span> <span class="mf">0.2727272727272727</span><span class="p">,</span>
             <span class="s1">'i'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'this'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'a'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'is'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'like'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'dog'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;e&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;unk&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">}</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="o">**</span><span class="n">expected</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
</pre></div>
<pre class="example">
{'&lt;e&gt;': 0.09090909090909091,
 '&lt;unk&gt;': 0.09090909090909091,
 'a': 0.09090909090909091,
 'cat': 0.2727272727272727,
 'dog': 0.09090909090909091,
 'i': 0.09090909090909091,
 'is': 0.09090909090909091,
 'like': 0.09090909090909091,
 'this': 0.09090909090909091}
</pre>
<div class="highlight">
<pre><span></span><span class="n">trigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">estimate_probabilities</span><span class="p">([</span><span class="s2">"&lt;s&gt;"</span><span class="p">,</span> <span class="s2">"&lt;s&gt;"</span><span class="p">],</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">trigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span>  <span class="p">{</span><span class="s1">'cat'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'i'</span><span class="p">:</span> <span class="mf">0.18181818181818182</span><span class="p">,</span>
             <span class="s1">'this'</span><span class="p">:</span> <span class="mf">0.18181818181818182</span><span class="p">,</span>
             <span class="s1">'a'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'is'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'like'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'dog'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;e&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;unk&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">}</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="o">**</span><span class="n">expected</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
</pre></div>
<pre class="example">
{'&lt;e&gt;': 0.09090909090909091,
 '&lt;unk&gt;': 0.09090909090909091,
 'a': 0.09090909090909091,
 'cat': 0.09090909090909091,
 'dog': 0.09090909090909091,
 'i': 0.18181818181818182,
 'is': 0.09090909090909091,
 'like': 0.09090909090909091,
 'this': 0.18181818181818182}
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgd26c616">
<h3 id="orgd26c616">Count and probability matrices</h3>
<div class="outline-text-3" id="text-orgd26c616">
<p>As we have seen so far, the n-gram counts computed above are sufficient for computing the probabilities of the next word.</p>
<ul class="org-ul">
<li>It can be more intuitive to present them as count or probability matrices.</li>
<li>The functions defined in the next cells return count or probability matrices.</li>
<li>This function is provided for you.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_count_matrix</span><span class="p">(</span><span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">):</span>
    <span class="c1"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span>
    <span class="c1"># &lt;s&gt; is omitted since it should not appear as the next word</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">,</span> <span class="s2">"&lt;unk&gt;"</span><span class="p">]</span>

    <span class="c1"># obtain unique n-grams</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">n_plus1_gram</span> <span class="ow">in</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">n_gram</span> <span class="o">=</span> <span class="n">n_plus1_gram</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">n_grams</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n_gram</span><span class="p">)</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">n_grams</span><span class="p">))</span>

    <span class="c1"># mapping from n-gram to row</span>
    <span class="n">row_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">n_gram</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_gram</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_grams</span><span class="p">)}</span>
    <span class="c1"># mapping from next word to column</span>
    <span class="n">col_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)}</span>

    <span class="n">nrow</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_grams</span><span class="p">)</span>
    <span class="n">ncol</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="n">count_matrix</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nrow</span><span class="p">,</span> <span class="n">ncol</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">n_plus1_gram</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">n_gram</span> <span class="o">=</span> <span class="n">n_plus1_gram</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">n_plus1_gram</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">row_index</span><span class="p">[</span><span class="n">n_gram</span><span class="p">]</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">col_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">count_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">count</span>

    <span class="n">count_matrix</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">n_grams</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">count_matrix</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">'bigram counts'</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">TABLE</span><span class="p">(</span><span class="n">make_count_matrix</span><span class="p">(</span><span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">)))</span>
</pre></div>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">cat</th>
<th class="org-right" scope="col">a</th>
<th class="org-right" scope="col">like</th>
<th class="org-right" scope="col">this</th>
<th class="org-right" scope="col">dog</th>
<th class="org-right" scope="col">i</th>
<th class="org-right" scope="col">is</th>
<th class="org-right" scope="col">&lt;e&gt;</th>
<th class="org-right" scope="col">&lt;unk&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">('dog',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('cat',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('is',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('a',)</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('i',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('like',)</td>
<td class="org-right">0</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('this',)</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
</div>
<div class="outline-4" id="outline-container-org968355a">
<h4 id="org968355a">Show trigram counts</h4>
<div class="outline-text-4" id="text-org968355a">
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\n</span><span class="s1">trigram counts'</span><span class="p">)</span>
<span class="n">trigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">TABLE</span><span class="p">(</span><span class="n">make_count_matrix</span><span class="p">(</span><span class="n">trigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">)))</span>
</pre></div>
<p>trigram counts</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">cat</th>
<th class="org-right" scope="col">a</th>
<th class="org-right" scope="col">like</th>
<th class="org-right" scope="col">this</th>
<th class="org-right" scope="col">dog</th>
<th class="org-right" scope="col">i</th>
<th class="org-right" scope="col">is</th>
<th class="org-right" scope="col">&lt;e&gt;</th>
<th class="org-right" scope="col">&lt;unk&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">('&lt;s&gt;', 'i')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('i', 'like')</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;', 'this')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('like', 'a')</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;', '&lt;s&gt;')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('is', 'like')</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('dog', 'is')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('this', 'dog')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>
<tr>
<td class="org-left">('a', 'cat')</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">2</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org5087a4d">
<h3 id="org5087a4d">Probability Matrix</h3>
<div class="outline-text-3" id="text-org5087a4d">
<p>The following function calculates the probabilities of each word given the previous n-gram, and stores this in matrix form.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_probability_matrix</span><span class="p">(</span><span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">count_matrix</span> <span class="o">=</span> <span class="n">make_count_matrix</span><span class="p">(</span><span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">)</span>
    <span class="n">count_matrix</span> <span class="o">+=</span> <span class="n">k</span>
    <span class="n">prob_matrix</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="n">count_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="s2">"rows"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prob_matrix</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
                 <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"bigram probabilities"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">TABLE</span><span class="p">(</span><span class="n">make_probability_matrix</span><span class="p">(</span><span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
<p>bigram probabilities</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">cat</th>
<th class="org-right" scope="col">a</th>
<th class="org-right" scope="col">like</th>
<th class="org-right" scope="col">this</th>
<th class="org-right" scope="col">dog</th>
<th class="org-right" scope="col">i</th>
<th class="org-right" scope="col">is</th>
<th class="org-right" scope="col">&lt;e&gt;</th>
<th class="org-right" scope="col">&lt;unk&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">('dog',)</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;',)</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.181818</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.181818</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('cat',)</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.272727</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('is',)</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('a',)</td>
<td class="org-right">0.272727</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('i',)</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('like',)</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.272727</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('this',)</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"trigram probabilities"</span><span class="p">)</span>
<span class="n">trigram_counts</span> <span class="o">=</span> <span class="n">count_n_grams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">TABLE</span><span class="p">(</span><span class="n">make_probability_matrix</span><span class="p">(</span><span class="n">trigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
</pre></div>
<p>trigram probabilities</p>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right">
<col class="org-right"></colgroup>
<thead>
<tr>
<th class="org-left" scope="col">&nbsp;</th>
<th class="org-right" scope="col">cat</th>
<th class="org-right" scope="col">a</th>
<th class="org-right" scope="col">like</th>
<th class="org-right" scope="col">this</th>
<th class="org-right" scope="col">dog</th>
<th class="org-right" scope="col">i</th>
<th class="org-right" scope="col">is</th>
<th class="org-right" scope="col">&lt;e&gt;</th>
<th class="org-right" scope="col">&lt;unk&gt;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">('&lt;s&gt;', 'i')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('i', 'like')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;', 'this')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('like', 'a')</td>
<td class="org-right">0.272727</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('&lt;s&gt;', '&lt;s&gt;')</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.181818</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.181818</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
</tr>
<tr>
<td class="org-left">('is', 'like')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('dog', 'is')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('this', 'dog')</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
<td class="org-right">0.2</td>
<td class="org-right">0.1</td>
<td class="org-right">0.1</td>
</tr>
<tr>
<td class="org-left">('a', 'cat')</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.0909091</td>
<td class="org-right">0.272727</td>
<td class="org-right">0.0909091</td>
</tr>
</tbody>
</table>
<p>Confirm that you obtain the same results as for the `estimate_probabilities` function that you implemented.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3e2b3bc">
<h3 id="org3e2b3bc">Bundle It Up</h3>
<div class="outline-text-3" id="text-org3e2b3bc">
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">gram</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">start</span><span class="o">-</span><span class="n">tokens</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">end</span><span class="o">-</span><span class="n">tokens</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">sentences</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">grams</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">counts</span><span class="o">&gt;&gt;</span>


<span class="o">&lt;&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">gram</span><span class="o">-</span><span class="n">probability</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">grams</span><span class="o">-</span><span class="n">model</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">n</span><span class="o">-</span><span class="n">plus</span><span class="o">-</span><span class="n">one</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">vocabulary</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">vocabulary</span><span class="o">-</span><span class="n">size</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">probability</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">probabilities</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgaeee37e">
<h4 id="orgaeee37e">Imports</h4>
<div class="outline-text-4" id="text-orgaeee37e">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>

<span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2b89fb5">
<h4 id="org2b89fb5">The N-Gram</h4>
<div class="outline-text-4" id="text-org2b89fb5">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NGrams</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""The N-Gram Language Model</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the training data</span>
<span class="sd">     n: the size of the n-grams</span>
<span class="sd">     start_token: string to represent the start of a sentence</span>
<span class="sd">     end_token: string to represent the end of a sentence</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">start_token</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"&lt;s&gt;"</span>
    <span class="n">end_token</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"&lt;e&gt;"</span>
    <span class="n">_start_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_end_tokens</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_sentences</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_n_grams</span><span class="p">:</span> <span class="nb">list</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgcc121a3"></a>Start Tokens<br>
<div class="outline-text-5" id="text-orgcc121a3">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">start_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""List of 'n' start tokens"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">start_token</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_tokens</span>
</pre></div>
</div>
</li>
<li><a id="orge2215a6"></a>End Tokens<br>
<div class="outline-text-5" id="text-orge2215a6">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">end_tokens</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""List of 1 end-tokens"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_end_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_end_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">end_token</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_end_tokens</span>
</pre></div>
</div>
</li>
<li><a id="orgb40f3af"></a>Sentences<br>
<div class="outline-text-5" id="text-orgb40f3af">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">sentences</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""The data augmented with tags and converted to tuples"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sentences</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">start_tokens</span> <span class="o">+</span> <span class="n">sentence</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_tokens</span><span class="p">)</span>
                           <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sentences</span>
</pre></div>
</div>
</li>
<li><a id="org8102ce5"></a>N-Grams<br>
<div class="outline-text-5" id="text-org8102ce5">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">n_grams</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""The n-grams from the data</span>

<span class="sd">    Warning:</span>
<span class="sd">     this flattens the n-grams so there isn't any sentence structure</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">([</span>
            <span class="p">[</span><span class="n">sentence</span><span class="p">[</span><span class="n">cut</span><span class="p">:</span> <span class="n">cut</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">]</span> <span class="k">for</span> <span class="n">cut</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))]</span>
            <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sentences</span>
        <span class="p">])</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span>
</pre></div>
</div>
</li>
<li><a id="org06dade0"></a>Count<br>
<div class="outline-text-5" id="text-org06dade0">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">counts</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Counter</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""A count of all n-grams in the data</span>

<span class="sd">    Returns:</span>
<span class="sd">       A dictionary that maps a tuple of n-words to its frequency</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>        
        <span class="bp">self</span><span class="o">.</span><span class="n">_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_grams</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counts</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgd22de05">
<h4 id="orgd22de05">N-Gram Probability</h4>
<div class="outline-text-4" id="text-orgd22de05">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NGramProbability</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Probability model for n-grams</span>

<span class="sd">    Args:</span>
<span class="sd">     data: the source for the n-grams</span>
<span class="sd">     n: the size of the n-grams</span>
<span class="sd">     k: smoothing factor</span>
<span class="sd">     augment_vocabulary: hack because the two probability functions use different vocabularies</span>
<span class="sd">    """</span>
    <span class="n">data</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">n</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span>
    <span class="n">augment_vocabulary</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
    <span class="n">_n_grams</span><span class="p">:</span> <span class="n">NGrams</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_n_plus_one</span><span class="p">:</span> <span class="n">NGrams</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary</span><span class="p">:</span> <span class="nb">set</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">_probabilities</span><span class="p">:</span> <span class="nb">dict</span><span class="o">=</span><span class="kc">None</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org675c010"></a>N-Grams<br>
<div class="outline-text-5" id="text-org675c010">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">n_grams</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NGrams</span><span class="p">:</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_grams</span>
</pre></div>
</div>
</li>
<li><a id="org21faa4e"></a>N-Plus-One Grams<br>
<div class="outline-text-5" id="text-org21faa4e">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">n_plus_one</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">NGrams</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""N+1 Grams"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_plus_one</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_plus_one</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_plus_one</span>
</pre></div>
</div>
</li>
<li><a id="orged72829"></a>The Vocabulary<br>
<div class="outline-text-5" id="text-orged72829">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">set</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Unique words in the dictionary"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_vocabulary</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">+=</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">,</span> <span class="s2">"&lt;unk&gt;"</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary</span>
</pre></div>
</div>
</li>
<li><a id="orgb721da1"></a>Vocabulary Size<br>
<div class="outline-text-5" id="text-orgb721da1">
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">vocabulary_size</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Number of unique tokens in the data"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_vocabulary_size</span>
</pre></div>
</div>
</li>
<li><a id="orgfebf9ef"></a>Probability<br>
<div class="outline-text-5" id="text-orgfebf9ef">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">probability</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Calculates the probability of the word given the previous n-gram"""</span>
    <span class="c1"># just in case it's a list</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_grams</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span>

    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_plus_one</span><span class="o">.</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
    <span class="k">return</span> <span class="n">numerator</span><span class="o">/</span><span class="n">denominator</span>
</pre></div>
</div>
</li>
<li><a id="orgd753d12"></a>Probabilities<br>
<div class="outline-text-5" id="text-orgd753d12">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">probabilities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""Finds the probability of each word in the vocabulary</span>

<span class="sd">    Args:</span>
<span class="sd">     previous_n_gram: the preceding tuple to calculate probabilities</span>

<span class="sd">    Returns:</span>
<span class="sd">     word:&lt;probability word follows previous n-gram&gt; for the vocabulary</span>
<span class="sd">    """</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="n">word</span><span class="o">=</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="o">=</span><span class="n">previous_n_gram</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgc643492">
<h4 id="orgc643492">Test It Out</h4>
<div class="outline-text-4" id="text-orgc643492">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGrams</span>

<span class="c1"># **** Set Up ****</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>

<span class="c1"># **** Unigram ****</span>

<span class="n">expected</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'i'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'like'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'cat'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span>
            <span class="p">(</span><span class="s1">'&lt;e&gt;'</span><span class="p">,):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'this'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'dog'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'is'</span><span class="p">,):</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">uni_grams</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">uni_grams</span><span class="o">.</span><span class="n">counts</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>

<span class="c1"># **** Bi-Gram ****</span>

<span class="n">expected</span> <span class="o">=</span> <span class="p">{(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'&lt;s&gt;'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'i'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">(</span><span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="s1">'cat'</span><span class="p">,</span> <span class="s1">'&lt;e&gt;'</span><span class="p">):</span> <span class="mi">2</span><span class="p">,</span>
            <span class="p">(</span><span class="s1">'&lt;s&gt;'</span><span class="p">,</span> <span class="s1">'this'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">):</span> <span class="mi">1</span><span class="p">,</span>
            <span class="p">(</span><span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">):</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">bi_grams</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">bi_grams</span><span class="o">.</span><span class="n">counts</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGramProbability</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>

<span class="c1"># the examples for the two probability functions don't behave the same</span>
<span class="c1"># so for this case don't augment the vocabulary with empty and unknown tokens</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NGramProbability</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">augment_vocabulary</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">probability</span><span class="p">(</span><span class="s2">"cat"</span><span class="p">,</span> <span class="s2">"a"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.3333</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The estimated probability of word 'cat' given the previous n-gram 'a' is: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1"># the probabilities test examples assume that you did augment the vocabulary</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NGramProbability</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">probabilities</span><span class="p">(</span><span class="s2">"a"</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span>  <span class="p">{</span><span class="s1">'cat'</span><span class="p">:</span> <span class="mf">0.2727272727272727</span><span class="p">,</span>
             <span class="s1">'i'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'this'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'a'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'is'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'like'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'dog'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;e&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;unk&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">}</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="o">**</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">NGramProbability</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">probabilities</span><span class="p">([</span><span class="s2">"&lt;s&gt;"</span><span class="p">,</span> <span class="s2">"&lt;s&gt;"</span><span class="p">])</span>

<span class="n">expected</span> <span class="o">=</span>  <span class="p">{</span><span class="s1">'cat'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'i'</span><span class="p">:</span> <span class="mf">0.18181818181818182</span><span class="p">,</span>
             <span class="s1">'this'</span><span class="p">:</span> <span class="mf">0.18181818181818182</span><span class="p">,</span>
             <span class="s1">'a'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'is'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'like'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'dog'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;e&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">,</span>
             <span class="s1">'&lt;unk&gt;'</span><span class="p">:</span> <span class="mf">0.09090909090909091</span><span class="p">}</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">have_keys</span><span class="p">(</span><span class="o">**</span><span class="n">expected</span><span class="p">))</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc62ec0a">
<h2 id="orgc62ec0a">End</h2>
<div class="outline-text-2" id="text-orgc62ec0a">
<p>Now that we have the N-Gram model we'll move on to checking its <a href="../auto-complete-perplexity/index.html">Perplexity</a>.</p>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/auto-complete/index.html" rel="tag">auto-complete</a></li>
<li><a class="tag p-category" href="../../../categories/n-gram/index.html" rel="tag">n-gram</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/index.html" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../auto-complete-pre-process-the-data-ii/index.html" rel="prev" title="Auto-Complete: Pre-Process the Data II">Previous post</a></li>
<li class="next"><a href="../auto-complete-perplexity/index.html" rel="next" title="Auto-Complete: Perplexity">Next post</a></li>
</ul>
</nav>
</aside>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script></article>
<!--End of body content-->
<footer id="footer">Scribbles by <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>
<div id="license" xmlns:cc="http://creativecommons.org/ns#">This work is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" rel="license noopener noreferrer" style="display:inline-block;" target="_blank">CC BY 4.0 <img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a></div>
</footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
