<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Building the Most Frequent Class baseline for WSJ POS tagging." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Parts-of-Speech Tagging: Most Frequent Class Baseline | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/parts-of-speech-tagging-most-frequent-class-baseline/index.html" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../parts-of-speech-tagging-training/index.html" rel="prev" title="Parts-of-Speech Tagging: Training" type="text/html">
<link href="../parts-of-speech-tagging-hidden-markov-model/index.html" rel="next" title="Parts-of-Speech Tagging: Hidden Markov Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Parts-of-Speech Tagging: Most Frequent Class Baseline" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/parts-of-speech-tagging-most-frequent-class-baseline/index.html" property="og:url">
<meta content="Building the Most Frequent Class baseline for WSJ POS tagging." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-11-17T18:23:32-08:00" property="article:published_time">
<meta content="nlp" property="article:tag">
<meta content="pos tagging" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item dropdown"><a aria-expanded="false" aria-haspopup="true" class="nav-link dropdown-toggle" data-toggle="dropdown" href="#">Monkey Pages</a>
<div class="dropdown-menu"><a class="dropdown-item" href="https://necromuralist.github.io/">The Cloistered Monkey</a> <a class="dropdown-item" href="https://necromuralist.github.io/Ape-Iron/">Ape Iron</a> <a class="dropdown-item" href="https://necromuralist.github.io/Bowling-For-Data/">Bowling For Data</a> <a class="dropdown-item" href="https://necromuralist.github.io/Beach-Pig-Thigh/">Beach-Pig Rump & Thigh</a> <a class="dropdown-item" href="https://necromuralist.github.io/Visions-Voices-Data/">Visions, Voices, Data</a></div>
</li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href="#">Parts-of-Speech Tagging: Most Frequent Class Baseline</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="#" rel="bookmark"><time class="published dt-published" datetime="2020-11-17T18:23:32-08:00" itemprop="datePublished" title="2020-11-17 18:23">2020-11-17 18:23</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="index.html#org005cb4d">Begin</a>
<ul>
<li><a href="index.html#org9738ecd">Imports</a></li>
<li><a href="index.html#org28933e7">Set Up</a>
<ul>
<li><a href="index.html#org28aab70">The Environment</a></li>
<li><a href="index.html#org41fe3fe">The Data</a></li>
<li><a href="index.html#orgdfe58d6">The Trained Model</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="index.html#org0643866">Middle</a>
<ul>
<li><a href="index.html#org70f156f">Most Frequent Class Baseline</a>
<ul>
<li><a href="index.html#org0cccdda">Processed vs Vocabulary</a></li>
</ul>
</li>
<li><a href="index.html#org66e09d2">The Baseline Implementation</a></li>
<li><a href="index.html#org57cab91">Take Two</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org005cb4d">
<h2 id="org005cb4d">Begin</h2>
<div class="outline-text-2" id="text-org005cb4d"></div>
<div class="outline-3" id="outline-container-org9738ecd">
<h3 id="org9738ecd">Imports</h3>
<div class="outline-text-3" id="text-org9738ecd">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="c1"># this repo</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.parts_of_speech.preprocessing</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.parts_of_speech.training</span> <span class="kn">import</span> <span class="n">TheTrainer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org28933e7">
<h3 id="org28933e7">Set Up</h3>
<div class="outline-text-3" id="text-org28933e7"></div>
<div class="outline-4" id="outline-container-org28aab70">
<h4 id="org28aab70">The Environment</h4>
<div class="outline-text-4" id="text-org28aab70">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org41fe3fe">
<h4 id="org41fe3fe">The Data</h4>
<div class="outline-text-4" id="text-org41fe3fe">
<div class="highlight">
<pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgdfe58d6">
<h4 id="orgdfe58d6">The Trained Model</h4>
<div class="outline-text-4" id="text-orgdfe58d6">
<div class="highlight">
<pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">processed_training</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org0643866">
<h2 id="org0643866">Middle</h2>
<div class="outline-text-2" id="text-org0643866"></div>
<div class="outline-3" id="outline-container-org70f156f">
<h3 id="org70f156f">Most Frequent Class Baseline</h3>
<div class="outline-text-3" id="text-org70f156f">
<p>The main purpose of part-of-speech tagging is to disambiguate words (Jurasky) - most words only have one part-of-speech tag, but the most commonly used words have more than one. To decide what the correct part-of-speech tag is for a word, you have to know its context and how to interpret it based on the context. Another way to choose a tag is to give each word the tag that it is most commonly associated with. This will then serve as a baseline for any more sophisticated algorithm. That's what we're going to do here.</p>
</div>
<div class="outline-4" id="outline-container-org0cccdda">
<h4 id="org0cccdda">Processed vs Vocabulary</h4>
<div class="outline-text-4" id="text-org0cccdda">
<p>Our <code>preprocessed</code> data is the words in the testing data and our <code>vocabulary</code> data is the words in the training set. We can't predict anything for the words in the testing set that aren't in the training set so let's see how much of an overlap there is.</p>
<div class="highlight">
<pre><span></span><span class="n">preprocessed</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">test_words</span><span class="p">)</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">)</span>

<span class="n">in_common</span> <span class="o">=</span> <span class="n">preprocessed</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"y in x: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">in_common</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">)</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2"> %"</span><span class="p">)</span>
</pre></div>
<pre class="example">
y in x: 100.000 %
</pre>
<p>So, we shouldn't lose any words just because we never saw them during training.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">processed_training</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
<pre class="example">
[('In', 'IN'), ('an', 'DT')]
</pre>
<p>Our <code>processed_training</code> attribute is a list of <code>&lt;word&gt;, &lt;tag&gt;</code> tuples from the training set.</p>
<div class="highlight">
<pre><span></span><span class="n">words_tags</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">loader</span><span class="o">.</span><span class="n">processed_training</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">words_tags</span><span class="p">]</span>
<span class="n">word_tag_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
</pre></div>
<p><code>word_tag_counts</code> is the count of the number of tags each word had.</p>
<div class="highlight">
<pre><span></span><span class="n">unambiguous</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span><span class="n">count</span> <span class="ow">in</span> <span class="n">word_tag_counts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">count</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Percent of unambiguous training words: "</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nb">len</span><span class="p">(</span><span class="n">unambiguous</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">word_tag_counts</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> %"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Percent of unambiguous training words: 75.11 %
</pre>
<p>And now for the subset that is in both the training and testing set.</p>
<div class="highlight">
<pre><span></span><span class="n">common_unambiguous</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">unambiguous</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">in_common</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"percent of words in common that are unambiguous:"</span>
      <span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">common_unambiguous</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">in_common</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
percent of words in common that are unambiguous: 58.19
</pre>
<p>Sort of low.</p>
<p>This means that if we just labeled the unambiguous words we'd be right about 58% of the time. Let's just double-check.</p>
<div class="highlight">
<pre><span></span><span class="n">guesses</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">tag</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">loader</span><span class="o">.</span><span class="n">test_data</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">unambiguous</span><span class="p">}</span>

<span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">)</span>
<span class="n">correct_guess</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">preprocessed</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">guesses</span><span class="p">:</span>
        <span class="n">guess</span> <span class="o">=</span> <span class="n">guesses</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
        <span class="n">correct_guess</span> <span class="o">+=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">guess</span> <span class="o">==</span> <span class="n">loader</span><span class="o">.</span><span class="n">test_data</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">continue</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Correct: </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">correct_guess</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s2"> 0.2f</span><span class="si">}</span><span class="s2">%"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Correct:  58.17%
</pre>
<p>So this is the amount we get if we just use the unambiguous words. Our baseline should be even better.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org66e09d2">
<h3 id="org66e09d2">The Baseline Implementation</h3>
<div class="outline-text-3" id="text-org66e09d2">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">predict_pos</span><span class="p">(</span><span class="n">prep</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">emission_counts</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">states</span><span class="p">):</span>
<span class="w">    </span><span class="sd">'''</span>
<span class="sd">    Input: </span>
<span class="sd">       prep: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span>
<span class="sd">       y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span>
<span class="sd">       emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span>
<span class="sd">       vocab: a dictionary where keys are words in vocabulary and value is an index</span>
<span class="sd">       states: a sorted list of all possible tags for this assignment</span>
<span class="sd">    Output: </span>
<span class="sd">       accuracy: Number of times you classified a word correctly</span>
<span class="sd">    '''</span>

    <span class="c1"># Initialize the number of correct predictions to zero</span>
    <span class="n">num_correct</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Get the (tag, word) tuples, stored as a set</span>
    <span class="n">all_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">emission_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="c1"># Get the number of (word, POS) tuples in the corpus 'y'</span>
    <span class="n">total</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">y_tup</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prep</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span> 

        <span class="c1"># Split the (word, POS) string into a list of two items</span>
        <span class="n">y_tup_l</span> <span class="o">=</span> <span class="n">y_tup</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>

        <span class="c1"># Verify that y_tup contain both word and POS</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_tup_l</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>

            <span class="c1"># Set the true POS label for this word</span>
            <span class="n">true_label</span> <span class="o">=</span> <span class="n">y_tup_l</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If the y_tup didn't contain word and POS, go to next word</span>
            <span class="k">continue</span>

        <span class="n">count_final</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">pos_final</span> <span class="o">=</span> <span class="s1">''</span>

        <span class="c1"># If the word is in the vocabulary...</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">states</span><span class="p">:</span>

                <span class="c1"># define the key as the tuple containing the POS and word</span>
                <span class="n">key</span> <span class="o">=</span> <span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span>

                <span class="c1"># check if the (pos, word) key exists in the emission_counts dictionary</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">emission_counts</span><span class="p">:</span> <span class="c1"># complete this line</span>
                <span class="c1"># get the emission count of the (pos,word) tuple </span>
                    <span class="n">count</span> <span class="o">=</span> <span class="n">emission_counts</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
                    <span class="c1"># keep track of the POS with the largest count</span>
                    <span class="k">if</span> <span class="n">count</span> <span class="o">&gt;</span> <span class="n">count_final</span><span class="p">:</span> <span class="c1"># complete this line</span>
                        <span class="c1"># update the final count (largest count)</span>
                        <span class="n">count_final</span> <span class="o">=</span> <span class="n">count</span>

                        <span class="c1"># update the final POS</span>
                        <span class="n">pos_final</span> <span class="o">=</span> <span class="n">pos</span>
            <span class="c1"># If the final POS (with the largest count) matches the true POS:</span>
            <span class="k">if</span> <span class="n">true_label</span> <span class="o">==</span> <span class="n">pos_final</span><span class="p">:</span> <span class="c1"># complete this line</span>
                <span class="c1"># Update the number of correct predictions</span>
                <span class="n">num_correct</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">num_correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">tag_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">accuracy_predict_pos</span> <span class="o">=</span> <span class="n">predict_pos</span><span class="p">(</span><span class="n">prep</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">test_words</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">test_data_raw</span><span class="p">,</span>
                                   <span class="n">emission_counts</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">emission_counts</span><span class="p">,</span>
                                   <span class="n">vocab</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy of prediction using predict_pos is </span><span class="si">{</span><span class="n">accuracy_predict_pos</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Accuracy of prediction using predict_pos is 0.8913
</pre>
<p>So our baseline prediction is 89% - any algorithm we create should do as well or better (but probably better for it to be worth doing).</p>
</div>
</div>
<div class="outline-3" id="outline-container-org57cab91">
<h3 id="org57cab91">Take Two</h3>
<div class="outline-text-3" id="text-org57cab91">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">emission_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""Calculate the baseline accuracy</span>
<span class="sd">    Args: </span>
<span class="sd">       preprocessed: a preprocessed version of 'y'. A list with the 'word' component of the tuples.</span>
<span class="sd">       y: a corpus composed of a list of tuples where each tuple consists of (word, POS)</span>
<span class="sd">       emission_counts: a dictionary where the keys are (tag,word) tuples and the value is the count</span>
<span class="sd">       vocabulary: a dictionary where keys are words in vocabulary and value is an index</span>
<span class="sd">       states: a sorted list of all possible tags for this assignment</span>

<span class="sd">    Returns: </span>
<span class="sd">       accuracy: Number of times you classified a word correctly</span>
<span class="sd">    """</span>
    <span class="c1"># filter (``preprocessed`` has unknown words replaced by special tags so we</span>
    <span class="c1"># need to use it to filter separately from the words in ``y``)</span>
    <span class="c1"># because using the raw word instead of the special tag will throw away</span>
    <span class="c1"># the row but we tagged it specifically to keep it</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[(</span><span class="n">word</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">preprocessed</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
              <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">)]</span>

    <span class="c1"># our final data sets    </span>
    <span class="n">true_tags</span> <span class="o">=</span> <span class="p">(</span><span class="n">tag</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">tag</span><span class="p">)</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># each guess is the POS tag for the word with the highest occurrence in the training data</span>
    <span class="n">guesses</span> <span class="o">=</span> <span class="p">(</span>
        <span class="nb">max</span><span class="p">((</span><span class="n">emission_counts</span><span class="o">.</span><span class="n">get</span><span class="p">((</span><span class="n">pos</span><span class="p">,</span> <span class="n">word</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">pos</span><span class="p">)</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="n">states</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">)</span>

    <span class="c1"># accuracy is sum of correct guesses/total rows in y</span>
    <span class="k">return</span> <span class="p">(</span><span class="nb">sum</span><span class="p">(</span>
        <span class="n">actual</span> <span class="o">==</span> <span class="n">guess</span> <span class="k">for</span> <span class="n">actual</span><span class="p">,</span> <span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">guess</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">true_tags</span><span class="p">,</span> <span class="n">guesses</span><span class="p">))</span>
            <span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">states</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">tag_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">preprocessed</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">test_words</span><span class="p">,</span>
                   <span class="n">y</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">test_data_tuples</span><span class="p">,</span>
                   <span class="n">emission_counts</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">emission_counts</span><span class="p">,</span>
                   <span class="n">vocabulary</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">vocabulary</span><span class="p">,</span>
                   <span class="n">states</span><span class="o">=</span><span class="n">states</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy of prediction using predict_pos is </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Accuracy of prediction using predict_pos is 0.8921
</pre></div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/index.html" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/pos-tagging/index.html" rel="tag">pos tagging</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../parts-of-speech-tagging-training/index.html" rel="prev" title="Parts-of-Speech Tagging: Training">Previous post</a></li>
<li class="next"><a href="../parts-of-speech-tagging-hidden-markov-model/index.html" rel="next" title="Parts-of-Speech Tagging: Hidden Markov Model">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Scribbles by <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>
<div id="license" xmlns:cc="http://creativecommons.org/ns#">This work is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" rel="license noopener noreferrer" style="display:inline-block;" target="_blank">CC BY 4.0 <img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a></div>
</footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
