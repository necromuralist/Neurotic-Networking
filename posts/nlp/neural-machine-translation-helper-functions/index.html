<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Helper functions to pre-process some inputs" name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neural Machine Translation: Helper Functions | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/neural-machine-translation-helper-functions/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../../first-course/logistic-regression-with-neural-networks/" rel="prev" title="Logistic Regression With Neural Networks" type="text/html">
<link href="../../gans/mnist-gan/" rel="next" title="MNIST GAN" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Neural Machine Translation: Helper Functions" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/neural-machine-translation-helper-functions/" property="og:url">
<meta content="Helper functions to pre-process some inputs" property="og:description">
<meta content="article" property="og:type">
<meta content="2021-02-27T14:41:04-08:00" property="article:published_time">
<meta content="attention" property="article:tag">
<meta content="encoder-decoder" property="article:tag">
<meta content="machine translation" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Neural Machine Translation: Helper Functions</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2021-02-27T14:41:04-08:00" itemprop="datePublished" title="2021-02-27 14:41">2021-02-27 14:41</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org190e138">Helper Functions</a>
<ul>
<li><a href="#org9b69148">Imports</a></li>
</ul>
</li>
<li><a href="#org7c158bb">Helper functions</a>
<ul>
<li><a href="#org7b3572b">Input encoder</a></li>
<li><a href="#org534697c">Pre-attention decoder</a></li>
<li><a href="#orgcd144f3">Preparing the attention input</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org190e138">
<h2 id="org190e138">Helper Functions</h2>
<div class="outline-text-2" id="text-org190e138">
<p>We will first implement a few functions that we will use later on. These will be for:</p>
<ul class="org-ul">
<li>the input encoder</li>
<li>the pre-attention decoder</li>
<li>preparation of the queries, keys, values, and mask.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org9b69148">
<h3 id="org9b69148">Imports</h3>
<div class="outline-text-3" id="text-org9b69148">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">trax.fastmath</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">fastmath_numpy</span>

<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7c158bb">
<h2 id="org7c158bb">Helper functions</h2>
<div class="outline-text-2" id="text-org7c158bb"></div>
<div class="outline-3" id="outline-container-org7b3572b">
<h3 id="org7b3572b">Input encoder</h3>
<div class="outline-text-3" id="text-org7b3572b">
<p>The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">Serial</a> network which uses:</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the <code>n_encoder_layers</code> parameter.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">input_encoder</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">n_encoder_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">""" Input encoder runs on the input sentence and creates</span>
<span class="sd">    activations that will be the keys and values for attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       input_vocab_size: vocab size of the input</span>
<span class="sd">       d_model:  depth of embedding (n_units in the LSTM cell)</span>
<span class="sd">       n_encoder_layers: number of LSTM layers in the encoder</span>

<span class="sd">    Returns:</span>
<span class="sd">       tl.Serial: The input encoder</span>
<span class="sd">    """</span>
    <span class="n">input_encoder</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span> 
        <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_encoder_layers</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">input_encoder</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_input_encoder_fn</span><span class="p">(</span><span class="n">input_encoder_fn</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">input_encoder_fn</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">input_vocab_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">n_encoder_layers</span> <span class="o">=</span> <span class="mi">6</span>

    <span class="n">encoder</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_encoder_layers</span><span class="p">)</span>

    <span class="n">lstms</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">'  LSTM_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_encoder_layers</span><span class="p">)</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Serial[</span><span class="se">\n</span><span class="s2">  Embedding_</span><span class="si">{</span><span class="n">input_vocab_size</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">lstms</span><span class="si">}</span><span class="se">\n</span><span class="s2">]"</span>

    <span class="n">proposed</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>

    <span class="c1"># Test all layers are in the expected sequence</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">proposed</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong model. </span><span class="se">\n</span><span class="s2">Proposed:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">proposed</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">expected</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Test the number of layers</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Test </span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_encoder_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'The number of sublayers does not match </span><span class="si">%s</span><span class="s1"> &lt;&gt;'</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">),</span> <span class="s2">" </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="p">(</span><span class="n">n_encoder_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"The enconder is not an object of "</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">)</span>


    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_input_encoder_fn</span><span class="p">(</span><span class="n">input_encoder</span><span class="p">)</span>
</pre></div>
<pre class="example">
[92m All tests passed
</pre></div>
</div>
<div class="outline-3" id="outline-container-org534697c">
<h3 id="org534697c">Pre-attention decoder</h3>
<div class="outline-text-3" id="text-org534697c">
<p>The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: This pads a token to the beginning of your target tokens (e.g. <code>[8, 34, 12]</code> shifted right is <code>[0, 8, 34, 12]</code>). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">pre_attention_decoder</span><span class="p">(</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">""" Pre-attention decoder runs on the targets and creates</span>
<span class="sd">    activations that are used as queries in attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       mode: 'train' or 'eval'</span>
<span class="sd">       target_vocab_size: vocab size of the target</span>
<span class="sd">       d_model:  depth of embedding (n_units in the LSTM cell)</span>
<span class="sd">    Returns:</span>
<span class="sd">       tl.Serial: The pre-attention decoder</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">ShiftRight</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_pre_attention_decoder_fn</span><span class="p">(</span><span class="n">pre_attention_decoder_fn</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">pre_attention_decoder_fn</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">mode</span> <span class="o">=</span> <span class="s1">'train'</span>
    <span class="n">target_vocab_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="n">decoder</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Serial[</span><span class="se">\n</span><span class="s2">  ShiftRight(1)</span><span class="se">\n</span><span class="s2">  Embedding_</span><span class="si">{</span><span class="n">target_vocab_size</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="s2">  LSTM_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="s2">]"</span>

    <span class="n">proposed</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">decoder</span><span class="p">)</span>

    <span class="c1"># Test all layers are in the expected sequence</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">proposed</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong model. </span><span class="se">\n</span><span class="s2">Proposed:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">proposed</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">expected</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Test the number of layers</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Test </span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
            <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'The number of sublayers does not match </span><span class="si">%s</span><span class="s1"> &lt;&gt;'</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">),</span> <span class="s2">" </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"The enconder is not an object of "</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">)</span>


    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<p>They changed the behavior of the <code>Fn</code> (or something in there) so that it always wraps the ShiftRight in a Serial layer, so it doesn't match the test anymore. Testing strings is kind of gimpy anywayâ€¦</p>
<p>It looks like they're using a decorator to check the shape which then wraps it in a Serial layer. See trax.layers.assert_shape.AssertFunction</p>
<div class="highlight">
<pre><span></span><span class="n">test_pre_attention_decoder_fn</span><span class="p">(</span><span class="n">pre_attention_decoder</span><span class="p">)</span>
</pre></div>
<pre class="example">
Wrong model. 
Proposed:
Serial[
  Serial[
    ShiftRight(1)
  ]
  Embedding_10_2
  LSTM_2
] 
Expected:
Serial[
  ShiftRight(1)
  Embedding_10_2
  LSTM_2
]
[92m 2  Tests passed
[91m 1  Tests failed
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgcd144f3">
<h3 id="orgcd144f3">Preparing the attention input</h3>
<div class="outline-text-3" id="text-orgcd144f3">
<p>This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. From the data preparation steps in Section 1 of this assignment, you should know which tokens in the input correspond to padding.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">prepare_attention_input</span><span class="p">(</span><span class="n">encoder_activations</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
                            <span class="n">decoder_activations</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
                            <span class="n">inputs</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Prepare queries, keys, values and mask for attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder</span>
<span class="sd">       decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder</span>
<span class="sd">       inputs fastnp.array(batch_size, padded_input_length): padded input tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">       queries, keys, values and mask for attention.</span>
<span class="sd">    """</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">encoder_activations</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">encoder_activations</span>
    <span class="n">queries</span> <span class="o">=</span> <span class="n">decoder_activations</span>    
    <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">mask</span> <span class="o">+=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">decoder_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_prepare_attention_input</span><span class="p">(</span><span class="n">prepare_attention_input</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">prepare_attention_input</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">#This unit test consider a batch size = 2, number_of_tokens = 3 and embedding_size = 4</span>

    <span class="n">enc_act</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
               <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
    <span class="n">dec_act</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> 
               <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
    <span class="n">inputs</span> <span class="o">=</span>  <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="n">exp_mask</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]],</span> 
                             <span class="p">[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]]])</span>

    <span class="n">exp_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">enc_act</span><span class="p">)</span>

    <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">enc_act</span><span class="p">,</span> <span class="n">dec_act</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">dec_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Queries does not match the decoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">enc_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Keys does not match the encoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">enc_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Values does not match the encoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">exp_mask</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Mask does not match expected tensor. </span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">exp_mask</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Output:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">mask</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"One of the output object are not of type "</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">interpreters</span><span class="o">.</span><span class="n">xla</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_prepare_attention_input</span><span class="p">(</span><span class="n">prepare_attention_input</span><span class="p">)</span>
</pre></div>
<pre class="example">
[92m All tests passed
</pre></div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/attention/" rel="tag">attention</a></li>
<li><a class="tag p-category" href="../../../categories/encoder-decoder/" rel="tag">encoder-decoder</a></li>
<li><a class="tag p-category" href="../../../categories/machine-translation/" rel="tag">machine translation</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../../first-course/logistic-regression-with-neural-networks/" rel="prev" title="Logistic Regression With Neural Networks">Previous post</a></li>
<li class="next"><a href="../../gans/mnist-gan/" rel="next" title="MNIST GAN">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
