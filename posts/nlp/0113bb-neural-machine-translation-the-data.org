#+BEGIN_COMMENT
.. title: Neural Machine Translation: The Data
.. slug: neural-machine-translation-the-data
.. date: 2021-02-14 14:53:32 UTC-08:00
.. tags: nlp,machine translation
.. category: NLP
.. link: 
.. description: The data for our machine translation model.
.. type: text

#+END_COMMENT
#+OPTIONS: ^:{}
#+TOC: headlines 3
#+PROPERTY: header-args :session ~/.local/share/jupyter/runtime/kernel-6319a987-ab57-46c8-a978-90b1c679884a-ssh.json
#+BEGIN_SRC python :results none :exports none
%load_ext autoreload
%autoreload 2
#+END_SRC
* The Data
  This is the first post in a series that will look at creating a Long-Short-Term-Memory (LSTM) model with attention for Machine Learning. The {{% lancelot title="previous post" %}}neural-machine-translation{{% /lancelot %}} was an overview that holds the links to all the posts in the series.
** Imports
#+begin_src python :results none
# python
from pathlib import Path

import random

# pypi
from termcolor import colored

import numpy
import trax
#+end_src
* Middle
** Loading the Data  
  Next, we will import the dataset we will use to train the model. If you are running out of space, you can just use a small dataset from [[http://opus.nlpl.eu/][Opus]], a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as ~opus/medical~ which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from [[https://paracrawl.eu/][ParaCrawl]], a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via [[https://www.tensorflow.org/datasets][Tensorflow Datasets (TFDS)]]
 and you can browse through the other available datasets [[https://www.tensorflow.org/datasets/catalog/overview][here]]. As you'll see below, you can easily access this dataset from TFDS with ~trax.data.TFDS~. The result is a python generator function yielding tuples. Use the ~keys~ argument to select what appears at which position in the tuple. For example, ~keys=('en', 'de')~ below will return pairs as (English sentence, German sentence).

 The [[https://www.tensorflow.org/datasets/catalog/para_crawl#para_crawlende][=para_crawl/ende=]] dataset is 4.04 GiB while the [[https://www.tensorflow.org/datasets/catalog/opus#opusmedical_default_config][=opus/medical=]] dataset is 188.85 MiB.

**Note:** Trying to download the ParaCrawl dataset using trax creates an out of resource error. You can try downloading the source from:

https://s3.amazonaws.com/web-language-models/paracrawl/release4/en-de.bicleaner07.txt.gz

Although I haven't figured out how to get it into the trax data yet.

*** The Training Data
The first time you run this it will download the dataset, after that it will just load it from the file.

#+begin_src python :results output :exports both
path = Path("~/data/").expanduser()

data_set = "opus/medical"
# data_set = "para_crawl/ende"

train_stream_fn = trax.data.TFDS(data_set,
                                 data_dir=path,
                                 keys=('en', 'de'),
                                 eval_holdout_size=0.01,
                                 train=True)
#+end_src

*** The Evaluation Data
    Since we already downloaded the data in the previous code-block, this will just load the evaluation set from the downloaded data.
    
#+begin_src python :results none
eval_stream_fn = trax.data.TFDS('opus/medical',
                                data_dir=path,
                                keys=('en', 'de'),
                                eval_holdout_size=0.01,
                                train=False)
#+end_src


 Notice that TFDS returns a generator *function*, not a generator. This is because in Python, you cannot reset generators so you cannot go back to a previously yielded value. During deep learning training, you use Stochastic Gradient Descent and don't actually need to go back -- but it is sometimes good to be able to do that, and that's where the functions come in. Let's print a a sample pair from our train and eval data. Notice that the raw output is represented in bytes (denoted by the ~b'~ prefix) and these will be converted to strings internally in the next steps.

#+begin_src python :results output :exports both
train_stream = train_stream_fn()
print(colored('train data (en, de) tuple:', 'red'), next(train_stream))
print()
#+end_src

#+RESULTS:
: [31mtrain data (en, de) tuple:[0m (b'Decreased Appetite\n', b'Verminderter Appetit\n')
: 

#+begin_src python :results output :exports both
eval_stream = eval_stream_fn()
print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))
#+end_src

#+RESULTS:
: [31meval data (en, de) tuple:[0m (b'Lutropin alfa Subcutaneous use.\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\n')

**  Tokenization and Formatting

 Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:

 **Tokenizing the sentences using subword representations:** We want to represent each sentence as an array of integers instead of strings. For our application, we will use *subword* representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for --"fear", "fearless", "fearsome", "some", and "less"--, you can simply store --"fear", "some", and "less"-- then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we have provided you the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) retrieved from https://storage.googleapis.com/trax-ml/vocabs/ende_32k.subword (I'm using the web-interface, but you could also just download it and put it in a directory).

#+begin_src python :results none
VOCAB_FILE = 'ende_32k.subword'
VOCAB_DIR = "gs://trax-ml/vocabs/" # google storage

# Tokenize the dataset.
tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)
tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)
#+end_src


**Append an end-of-sentence token to each sentence:** We will assign a token (i.e. in this case ~1~) to mark the end of a sentence. This will be useful in inference/prediction so we'll know that the model has completed the translation.

** Integer assigned as end-of-sentence (EOS)

#+begin_src python :results none
EOS = 1
#+end_src

#+begin_src python :results none
def append_eos(stream):
    """helper to add end of sentence token to sentences in the stream

    Yields:
     next tuple of numpy arrays with EOS token added (inputs, targets)
    """
    for (inputs, targets) in stream:
        inputs_with_eos = list(inputs) + [EOS]
        targets_with_eos = list(targets) + [EOS]
        yield numpy.array(inputs_with_eos), numpy.array(targets_with_eos)
    return
#+end_src

#+begin_src python :results none
tokenized_train_stream = append_eos(tokenized_train_stream)
tokenized_eval_stream = append_eos(tokenized_eval_stream)
#+end_src

*** Filter long sentences
    We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the ~trax.data.FilterByLength()~ method and you can see its syntax below.

 Filter too long sentences to not run out of memory. length_keys=[0, 1] means we filter both English and German sentences, so both much be not longer that 256 tokens for training / 512 for eval.

#+begin_src python :results none
filtered_train_stream = trax.data.FilterByLength(
    max_length=256, length_keys=[0, 1])(tokenized_train_stream)
filtered_eval_stream = trax.data.FilterByLength(
    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)
#+end_src 

#+begin_src python :results output :exports both
train_input, train_target = next(filtered_train_stream)
print(colored(f'Single tokenized example input:', 'red' ), train_input)
print(colored(f'Single tokenized example target:', 'red'), train_target)
#+end_src

#+RESULTS:
: [31mSingle tokenized example input:[0m [   71     4  3678 17363  8195     4  9227   469    19 20605   360  5575
:     68    49 20441    53  7408  1004  1195     4   433  9227   469    68
:     13   384 23306     5 20441  3550 30650  4729   992     1]
: [31mSingle tokenized example target:[0m [  752 22482 13831 15849   177   142    10  9227   469    25    10  8980
:  24481    35  4064 20618  4290 18098     5   113   143 14327    16   780
:   1004    15  2127  3695    69    10  9227   469   683   296   113    88
:    384 23306     5 14327    16  3550 30650  4729   992     1]

**  tokenize & detokenize helper functions

 Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your trax models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: 

 * word2Ind:  a dictionary mapping the word to its index.
 * ind2Word: a dictionary mapping the index to its word.
 * word2Count: a dictionary mapping the word to the number of times it appears. 
 * num_words: total number of words that have appeared. 

#+begin_src python :results none
def tokenize(input_str: str,
             vocab_file: str=None, vocab_dir: str=None, EOS: int=EOS) -> numpy.ndarray:
    """Encodes a string to an array of integers

    Args:
        input_str: human-readable string to encode
        vocab_file: filename of the vocabulary text file
        vocab_dir: path to the vocabulary file
  
    Returns:
        tokenized version of the input string
    """
    # Use the trax.data.tokenize method. It takes streams and returns streams,
    # we get around it by making a 1-element stream with `iter`.
    inputs =  next(trax.data.tokenize(iter([input_str]),
                                      vocab_file=vocab_file,
                                      vocab_dir=vocab_dir))
    
    # Mark the end of the sentence with EOS
    inputs = list(inputs) + [EOS]
    
    # Adding the batch dimension to the front of the shape
    batch_inputs = numpy.reshape(numpy.array(inputs), [1, -1])
    
    return batch_inputs
#+end_src

#+begin_src python :results none
def detokenize(integers: numpy.ndarray,
               vocab_file: str=None,
               vocab_dir: str=None,
               EOS: int=EOS) -> str:
    """Decodes an array of integers to a human readable string

    Args:
        integers: array of integers to decode
        vocab_file: filename of the vocabulary text file
        vocab_dir: path to the vocabulary file
  
    Returns:
        str: the decoded sentence.
    """
    # Remove the dimensions of size 1
    integers = list(numpy.squeeze(integers))
    
    # Remove the EOS to decode only the original tokens
    if EOS in integers:
        integers = integers[:integers.index(EOS)] 
    
    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)
#+end_src

Let's see how we might use these functions:

 Detokenize an input-target pair of tokenized sentences

#+begin_src python :results output :exports both
print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))
print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))
print()
#+end_src

#+RESULTS:
: [31mSingle detokenized example input:[0m In the pregnant rat the AUC for calculated free drug at this dose was approximately 18 times the human AUC at a 20 mg dose.
: 
: [31mSingle detokenized example target:[0m Bei tr√§chtigen Ratten war die AUC f√ºr die berechnete ungebundene Substanz bei dieser Dosis etwa 18-mal h√∂her als die AUC beim Menschen bei einer 20 mg Dosis.
: 

Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.
 See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.

#+begin_src python :results output :exports both 
print(colored("tokenize('hello'): ", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))
print(colored("detokenize([17332, 140, 1]): ", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))
#+end_src

#+RESULTS:
: [32mtokenize('hello'): [0m [[17332   140     1]]
: [32mdetokenize([17332, 140, 1]): [0m hello

** Bucketing

 Bucketing the tokenized sentences is an important technique used to speed up training in NLP. Here is a [[https://medium.com/@rashmi.margani/how-to-speed-up-the-training-of-the-sequence-model-using-bucketing-techniques-9e302b0fd976][nice article describing it in detail]] but the gist is very simple. Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket.


 We batch the sentences with similar length together and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows us to waste less computation when processing padded sequences.

 In Trax, it is implemented in the [[https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378][bucket_by_length]] function.

*** Bucketing to create streams of batches.

Buckets are defined in terms of boundaries and batch sizes. Batch_sizes[i] determines the batch size for items with length < boundaries[i]. So below, we'll take a batch of 256 sentences of length < 8, 128 if length is between 8 and 16, and so on -- and only 2 if length is over 512.

#+begin_src python :results none
boundaries =  [8,   16,  32, 64, 128, 256, 512]
batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]
#+end_src

Create the generators.

#+begin_src python :results none
train_batch_stream = trax.data.BucketByLength(
    boundaries, batch_sizes,
    length_keys=[0, 1]  # As before: count inputs and targets to length.
)(filtered_train_stream)

eval_batch_stream = trax.data.BucketByLength(
    boundaries, batch_sizes,
    length_keys=[0, 1]
)(filtered_eval_stream)
#+end_src

 Add masking for the padding (0s) using [[https://trax-ml.readthedocs.io/en/latest/trax.data.html][add_loss_weights]] (we're using =AddLossWeights= but the documentation just says "see add_loss_weights").

#+begin_src python :results none
train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)
eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)
#+end_src
** Exploring the data

 We will now be displaying some of our data. You will see that the functions defined above (i.e. ~tokenize()~ and ~detokenize()~) do the same things you have been doing again and again throughout the specialization. We gave these so you can focus more on building the model from scratch. Let us first get the data generator and get one batch of the data.

#+begin_src python :results none
input_batch, target_batch, mask_batch = next(train_batch_stream)
#+end_src

Let's see the data type of a batch.

#+begin_src python :results output :exports both
print("input_batch data type: ", type(input_batch))
print("target_batch data type: ", type(target_batch))
#+end_src

#+RESULTS:
: input_batch data type:  <class 'numpy.ndarray'>
: target_batch data type:  <class 'numpy.ndarray'>

Let's see the shape of this particular batch (batch length, sentence length).

#+begin_src python :results output :exports both
print("input_batch shape: ", input_batch.shape)
print("target_batch shape: ", target_batch.shape)
#+end_src

#+RESULTS:
: input_batch shape:  (32, 64)
: target_batch shape:  (32, 64)

 The ~input_batch~ and ~target_batch~ are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage. 

 We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let's pick a random sentence and print its tokenized representation.

Pick a random index less than the batch size.

#+begin_src python :results none
index = random.randrange(len(input_batch))
#+end_src

Use the index to grab an entry from the input and target batch.

#+begin_src python :results output :exports both
print(colored('THIS IS THE ENGLISH SENTENCE: \n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\n')
print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n ', 'red'), input_batch[index], '\n')
print(colored('THIS IS THE GERMAN TRANSLATION: \n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\n')
print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n', 'red'), target_batch[index], '\n')
#+end_src

#+RESULTS:
#+begin_example
[31mTHIS IS THE ENGLISH SENTENCE: 
[0m The patients should be continually monitored to adjust the infusion rate so that their antithrombin activity is at least 80% of the normal level for the duration of the treatment.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: 
 [0m [   29  4996   117    32 24399   212 28294     9 20854     4 23629  8668
    22   598    79    17    94  2646 11928  6100    23  2647    16    68
   757  1732   208     7     4  2965   465    19     4 15384     7     4
  2248  3550 30650  4729   992     1     0     0     0     0     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0] 

[31mTHIS IS THE GERMAN TRANSLATION: 
[0m Die Patienten sind kontinuierlich zu √ºberwachen, damit die Infusionsrate so angepasst werden kann, dass die Antithrombinaktivit√§t √ºber die gesamte Behandlungsdauer mindestens 80% des Normalwertes betr√§gt.
 

[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: 
[0m [   57  5122    75 17164 27041   529    18 13029     2   341    10 10614
  8668  3192   598    79 15633    58   136     2    42    10  2731 11928
  6100  7726 17152  3925    99    10  2517 26230 29856  5515  1732   208
    38 14136 11841    14  7861  3550 30650  4729   992     1     0     0
     0     0     0     0     0     0     0     0     0     0     0     0
     0     0     0     0] 
#+end_example

* End
  Now that we have our data prepared it's time to move on to {{% lancelot title="defining the Attention Model" %}}neural-machine-translation-the-attention-model{{% /lancelot %}}.
