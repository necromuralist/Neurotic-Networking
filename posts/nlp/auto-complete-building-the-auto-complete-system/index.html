<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Putting together the N-Gram auto-complete system." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Auto-Complete: Building the Auto-Complete System | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-building-the-auto-complete-system/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../auto-complete-perplexity/" rel="prev" title="Auto-Complete: Perplexity" type="text/html">
<link href="../word-embeddings-with-the-cbow-model/" rel="next" title="Word Embeddings with the CBOW Model" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Auto-Complete: Building the Auto-Complete System" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/auto-complete-building-the-auto-complete-system/" property="og:url">
<meta content="Putting together the N-Gram auto-complete system." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-12-04T15:21:47-08:00" property="article:published_time">
<meta content="auto-complete" property="article:tag">
<meta content="n-gram" property="article:tag">
<meta content="nlp" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Auto-Complete: Building the Auto-Complete System</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-12-04T15:21:47-08:00" itemprop="datePublished" title="2020-12-04 15:21">2020-12-04 15:21</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org8bc2b85">Build the Auto-Complete System</a>
<ul>
<li><a href="#org96ef40a">Imports</a></li>
<li><a href="#org1e9aede">Set Up</a>
<ul>
<li><a href="#org5d8f22e">The Environment</a></li>
<li><a href="#orgb9f94f8">The Data</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1d40af3">Middle</a>
<ul>
<li><a href="#orge1929bb">Probabilities Again</a></li>
<li><a href="#orgb62b43d">Suggest-a-Word</a>
<ul>
<li><a href="#orgb3bdb21">Hints</a></li>
<li><a href="#orgf5d2397">Test It Out</a></li>
</ul>
</li>
<li><a href="#org82a9650">Multiple Suggestions</a>
<ul>
<li><a href="#org0688a1c">Test It</a></li>
</ul>
</li>
<li><a href="#orgb6ca502">Multiple Word Suggestions</a></li>
</ul>
</li>
<li><a href="#org67dd051">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org8bc2b85">
<h2 id="org8bc2b85">Build the Auto-Complete System</h2>
<div class="outline-text-2" id="text-org8bc2b85">
<p>In the <a href="../auto-complete-perplexity/">previous post</a> we tested the perplexity of our N-Gram Language model. In this, the final post in the series that we began <a href="../auto-complete/">with this post</a>, we'll implement the final system.</p>
</div>
<div class="outline-3" id="outline-container-org96ef40a">
<h3 id="org96ef40a">Imports</h3>
<div class="outline-text-3" id="text-org96ef40a">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>
<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.autocomplete</span> <span class="kn">import</span> <span class="n">NGrams</span><span class="p">,</span> <span class="n">Tokenizer</span><span class="p">,</span> <span class="n">TrainTestSplit</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1e9aede">
<h3 id="org1e9aede">Set Up</h3>
<div class="outline-text-3" id="text-org1e9aede"></div>
<div class="outline-4" id="outline-container-org5d8f22e">
<h4 id="org5d8f22e">The Environment</h4>
<div class="outline-text-4" id="text-org5d8f22e">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">,</span> <span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb9f94f8">
<h4 id="orgb9f94f8">The Data</h4>
<div class="outline-text-4" id="text-orgb9f94f8">
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_AUTOCOMPLETE"</span><span class="p">]</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">reader</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org1d40af3">
<h2 id="org1d40af3">Middle</h2>
<div class="outline-text-2" id="text-org1d40af3"></div>
<div class="outline-3" id="outline-container-orge1929bb">
<h3 id="orge1929bb">Probabilities Again</h3>
<div class="outline-text-3" id="text-orge1929bb">
<p>Once again the function we're defining here expects this probability function so I'm going to have to paste it in here.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                         <span class="n">previous_n_gram</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> 
                         <span class="n">n_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">n_plus1_gram_counts</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
                         <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                         <span class="n">k</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of a next word using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       word: next word</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of n-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary_size: number of words in the vocabulary</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A probability</span>
<span class="sd">    """</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>
    <span class="n">previous_n_gram_count</span> <span class="o">=</span> <span class="n">n_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">n_plus1_gram</span> <span class="o">=</span> <span class="n">previous_n_gram</span> <span class="o">+</span> <span class="p">(</span><span class="n">word</span><span class="p">,)</span>  
    <span class="n">n_plus1_gram_count</span> <span class="o">=</span> <span class="n">n_plus1_gram_counts</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">n_plus1_gram</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>       
    <span class="k">return</span> <span class="p">(</span><span class="n">n_plus1_gram_count</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">previous_n_gram_count</span> <span class="o">+</span> <span class="n">k</span> <span class="o">*</span> <span class="n">vocabulary_size</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">estimate_probabilities</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Estimate the probabilities of next words using the n-gram counts with k-smoothing</span>

<span class="sd">    Args:</span>
<span class="sd">       previous_n_gram: A sequence of words of length n</span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary: List of words</span>
<span class="sd">       k: positive constant, smoothing parameter</span>

<span class="sd">    Returns:</span>
<span class="sd">       A dictionary mapping from next words to the probability.</span>
<span class="sd">    """</span>

    <span class="c1"># convert list to tuple to use it as a dictionary key</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">)</span>

    <span class="c1"># add &lt;e&gt; &lt;unk&gt; to the vocabulary</span>
    <span class="c1"># &lt;s&gt; is not needed since it should not appear as the next word</span>
    <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">vocabulary</span> <span class="o">+</span> <span class="p">[</span><span class="s2">"&lt;e&gt;"</span><span class="p">,</span> <span class="s2">"&lt;unk&gt;"</span><span class="p">]</span>
    <span class="n">vocabulary_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span>

    <span class="n">probabilities</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>
        <span class="n">probability</span> <span class="o">=</span> <span class="n">estimate_probability</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">previous_n_gram</span><span class="p">,</span> 
                                           <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> 
                                           <span class="n">vocabulary_size</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
        <span class="n">probabilities</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability</span>

    <span class="k">return</span> <span class="n">probabilities</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb62b43d">
<h3 id="orgb62b43d">Suggest-a-Word</h3>
<div class="outline-text-3" id="text-orgb62b43d">
<p>Compute probabilities for all possible next words and suggest the most likely one.</p>
<ul class="org-ul">
<li>This function also take an optional argument `start_with`, which specifies the first few letters of the next words.</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgb3bdb21">
<h4 id="orgb3bdb21">Hints</h4>
<div class="outline-text-4" id="text-orgb3bdb21">
<ul class="org-ul">
<li><code>estimate_probabilities</code> returns a dictionary where the key is a word and the value is the word's probability.</li>
<li>Use <code>str1.startswith(str2)</code> to determine if a string starts with the letters of another string. For example, <code>'learning'.startswith('lea')</code> returns True, whereas <code>'learning'.startswith('ear')</code> returns False. There are two additional parameters in <code>str.startswith()</code>, but you can use the default values for those parameters in this case.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C11 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="c1"># GRADED FUNCTION: suggest_a_word</span>
<span class="k">def</span> <span class="nf">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Get suggestion for the next word</span>

<span class="sd">    Args:</span>
<span class="sd">       previous_tokens: The sentence you input where each token is a word. Must have length &gt; n </span>
<span class="sd">       n_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       n_plus1_gram_counts: Dictionary of counts of (n+1)-grams</span>
<span class="sd">       vocabulary: List of words</span>
<span class="sd">       k: positive constant, smoothing parameter</span>
<span class="sd">       start_with: If not None, specifies the first few letters of the next word</span>

<span class="sd">    Returns:</span>
<span class="sd">       A tuple of </span>
<span class="sd">         - string of the most likely next word</span>
<span class="sd">         - corresponding probability</span>
<span class="sd">    """</span>

    <span class="c1"># length of previous words</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">n_gram_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">])</span> 

    <span class="c1"># From the words that the user already typed</span>
    <span class="c1"># get the most recent 'n' words as the previous n-gram</span>
    <span class="n">previous_n_gram</span> <span class="o">=</span> <span class="n">previous_tokens</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span>

    <span class="c1"># Estimate the probabilities that each word in the vocabulary</span>
    <span class="c1"># is the next word,</span>
    <span class="c1"># given the previous n-gram, the dictionary of n-gram counts,</span>
    <span class="c1"># the dictionary of n plus 1 gram counts, and the smoothing constant</span>
    <span class="n">probabilities</span> <span class="o">=</span> <span class="n">estimate_probabilities</span><span class="p">(</span><span class="n">previous_n_gram</span><span class="p">,</span>
                                           <span class="n">n_gram_counts</span><span class="p">,</span> <span class="n">n_plus1_gram_counts</span><span class="p">,</span>
                                           <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

    <span class="c1"># Initialize suggested word to None</span>
    <span class="c1"># This will be set to the word with highest probability</span>
    <span class="n">suggestion</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Initialize the highest word probability to 0</span>
    <span class="c1"># this will be set to the highest probability </span>
    <span class="c1"># of all words to be suggested</span>
    <span class="n">max_prob</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">### START CODE HERE (Replace instances of 'None' with your code) ###</span>

    <span class="c1"># For each word and its probability in the probabilities dictionary:</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">probabilities</span><span class="o">.</span><span class="n">items</span><span class="p">():</span> <span class="c1"># complete this line</span>

        <span class="c1"># If the optional start_with string is set</span>
        <span class="k">if</span> <span class="n">start_with</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="c1"># complete this line</span>

            <span class="c1"># Check if the beginning of word does not match with the letters in 'start_with'</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">word</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">start_with</span><span class="p">):</span> <span class="c1"># complete this line</span>

                <span class="c1"># if they don't match, skip this word (move onto the next word)</span>
                <span class="k">continue</span> <span class="c1"># complete this line</span>

        <span class="c1"># Check if this word's probability</span>
        <span class="c1"># is greater than the current maximum probability</span>
        <span class="k">if</span> <span class="n">prob</span> <span class="o">&gt;</span> <span class="n">max_prob</span><span class="p">:</span> <span class="c1"># complete this line</span>

            <span class="c1"># If so, save this word as the best suggestion (so far)</span>
            <span class="n">suggestion</span> <span class="o">=</span> <span class="n">word</span>

            <span class="c1"># Save the new maximum probability</span>
            <span class="n">max_prob</span> <span class="o">=</span> <span class="n">prob</span>

    <span class="c1">### END CODE HERE</span>

    <span class="k">return</span> <span class="n">suggestion</span><span class="p">,</span> <span class="n">max_prob</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf5d2397">
<h4 id="orgf5d2397">Test It Out</h4>
<div class="outline-text-4" id="text-orgf5d2397">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>

<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"like"</span><span class="p">]</span>
<span class="n">word</span><span class="p">,</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like',</span><span class="se">\n\t</span><span class="s2">and the suggested word is `</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">` with a probability of </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expected_word</span><span class="p">,</span> <span class="n">expected_probability</span> <span class="o">=</span> <span class="s2">"a"</span><span class="p">,</span> <span class="mf">0.2727</span>
<span class="n">expect</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_word</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">probability</span><span class="p">,</span> <span class="n">expected_probability</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>

<span class="c1"># test your code when setting the starts_with</span>
<span class="n">tmp_starts_with</span> <span class="o">=</span> <span class="s1">'c'</span>
<span class="n">word</span><span class="p">,</span> <span class="n">probability</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="n">tmp_starts_with</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like', the suggestion must start with `</span><span class="si">{</span><span class="n">tmp_starts_with</span><span class="si">}</span><span class="s2">`</span><span class="se">\n\t</span><span class="s2">and the suggested word is `</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">` with a probability of </span><span class="si">{</span><span class="n">probability</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">expected_word</span><span class="p">,</span> <span class="n">expected_probability</span> <span class="o">=</span> <span class="s2">"cat"</span><span class="p">,</span> <span class="mf">0.0909</span>
<span class="n">expect</span><span class="p">(</span><span class="n">word</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected_word</span><span class="p">))</span>
<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">probability</span><span class="p">,</span> <span class="n">expected_probability</span><span class="p">,</span> <span class="n">abs_tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are 'i like',
        and the suggested word is `a` with a probability of 0.2727

The previous words are 'i like', the suggestion must start with `c`
        and the suggested word is `cat` with a probability of 0.0909
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org82a9650">
<h3 id="org82a9650">Multiple Suggestions</h3>
<div class="outline-text-3" id="text-org82a9650">
<p>The function defined below loops over various n-gram models to get multiple suggestions.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model_counts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_gram_counts_list</span><span class="p">)</span>
    <span class="n">suggestions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">model_counts</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">n_gram_counts</span> <span class="o">=</span> <span class="n">n_gram_counts_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">n_plus1_gram_counts</span> <span class="o">=</span> <span class="n">n_gram_counts_list</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">suggestion</span> <span class="o">=</span> <span class="n">suggest_a_word</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts</span><span class="p">,</span>
                                    <span class="n">n_plus1_gram_counts</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span>
                                    <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="n">start_with</span><span class="p">)</span>
        <span class="n">suggestions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">suggestion</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">suggestions</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org0688a1c">
<h4 id="org0688a1c">Test It</h4>
<div class="outline-text-4" id="text-org0688a1c">
<div class="highlight">
<pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="p">[[</span><span class="s1">'i'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">],</span>
             <span class="p">[</span><span class="s1">'this'</span><span class="p">,</span> <span class="s1">'dog'</span><span class="p">,</span> <span class="s1">'is'</span><span class="p">,</span> <span class="s1">'like'</span><span class="p">,</span> <span class="s1">'a'</span><span class="p">,</span> <span class="s1">'cat'</span><span class="p">]]</span>
<span class="n">unique_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">unigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">bigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">trigram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">quadgram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>
<span class="n">qintgram_counts</span> <span class="o">=</span> <span class="n">NGrams</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span>

<span class="n">n_gram_counts_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">unigram_counts</span><span class="p">,</span> <span class="n">bigram_counts</span><span class="p">,</span> <span class="n">trigram_counts</span><span class="p">,</span> <span class="n">quadgram_counts</span><span class="p">,</span> <span class="n">qintgram_counts</span><span class="p">]</span>
<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"like"</span><span class="p">]</span>
<span class="n">tmp_suggest3</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">unique_words</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are 'i like', the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest3</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are 'i like', the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">a</td>
<td class="org-right">0.2727272727272727</td>
</tr>
<tr>
<td class="org-left">a</td>
<td class="org-right">0.2</td>
</tr>
<tr>
<td class="org-left">like</td>
<td class="org-right">0.1111111111111111</td>
</tr>
<tr>
<td class="org-left">like</td>
<td class="org-right">0.1111111111111111</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb6ca502">
<h3 id="orgb6ca502">Multiple Word Suggestions</h3>
<div class="outline-text-3" id="text-orgb6ca502">
<div class="highlight">
<pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">splitter</span> <span class="o">=</span> <span class="n">TrainTestSplit</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenized</span><span class="p">)</span>
<span class="n">train_data_processed</span> <span class="o">=</span> <span class="n">splitter</span><span class="o">.</span><span class="n">training</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">n_gram_counts_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">NGrams</span><span class="p">(</span><span class="n">train_data_processed</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">counts</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">)]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">train_data_processed</span><span class="p">)))</span>
<span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"am"</span><span class="p">,</span> <span class="s2">"to"</span><span class="p">]</span>
<span class="n">tmp_suggest4</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest4</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['i', 'am', 'to'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">be</td>
<td class="org-right">0.015552924847940564</td>
</tr>
<tr>
<td class="org-left">please</td>
<td class="org-right">5.4935999560512006e-05</td>
</tr>
<tr>
<td class="org-left">please</td>
<td class="org-right">5.494354550699157e-05</td>
</tr>
<tr>
<td class="org-left">sucks</td>
<td class="org-right">2.747403703500192e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"i"</span><span class="p">,</span> <span class="s2">"want"</span><span class="p">,</span> <span class="s2">"to"</span><span class="p">,</span> <span class="s2">"go"</span><span class="p">]</span>
<span class="n">tmp_suggest5</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest5</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['i', 'want', 'to', 'go'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.006007762241480142</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.0019077728115120462</td>
</tr>
<tr>
<td class="org-left">to</td>
<td class="org-right">0.00030196552102778083</td>
</tr>
<tr>
<td class="org-left">home</td>
<td class="org-right">0.00016478989288656962</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">]</span>
<span class="n">tmp_suggest6</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest6</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">you</td>
<td class="org-right">0.010055522861602231</td>
</tr>
<tr>
<td class="org-left">you</td>
<td class="org-right">0.0014810345300458024</td>
</tr>
<tr>
<td class="org-left">you</td>
<td class="org-right">5.494656446605676e-05</td>
</tr>
<tr>
<td class="org-left">sucks</td>
<td class="org-right">2.747403703500192e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">,</span> <span class="s2">"you"</span><span class="p">]</span>
<span class="n">tmp_suggest7</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest7</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">'re</td>
<td class="org-right">0.012929170630459223</td>
</tr>
<tr>
<td class="org-left">?</td>
<td class="org-right">0.0011416145691764065</td>
</tr>
<tr>
<td class="org-left">?</td>
<td class="org-right">0.0007132863295931524</td>
</tr>
<tr>
<td class="org-left">&lt;e&gt;</td>
<td class="org-right">5.494656446605676e-05</td>
</tr>
</tbody>
</table>
<div class="highlight">
<pre><span></span><span class="n">previous_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"hey"</span><span class="p">,</span> <span class="s2">"how"</span><span class="p">,</span> <span class="s2">"are"</span><span class="p">,</span> <span class="s2">"you"</span><span class="p">]</span>
<span class="n">tmp_suggest8</span> <span class="o">=</span> <span class="n">get_suggestions</span><span class="p">(</span><span class="n">previous_tokens</span><span class="p">,</span> <span class="n">n_gram_counts_list</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">start_with</span><span class="o">=</span><span class="s2">"d"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"The previous words are </span><span class="si">{</span><span class="n">previous_tokens</span><span class="si">}</span><span class="s2">, the suggestions are:"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tmp_suggest8</span><span class="p">)</span>
</pre></div>
<pre class="example">
The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:
</pre>
<table border="2" cellpadding="6" cellspacing="0" frame="hsides" rules="groups">
<colgroup>
<col class="org-left">
<col class="org-right"></colgroup>
<tbody>
<tr>
<td class="org-left">do</td>
<td class="org-right">0.004734930381388913</td>
</tr>
<tr>
<td class="org-left">doing</td>
<td class="org-right">0.000679532481652623</td>
</tr>
<tr>
<td class="org-left">doing</td>
<td class="org-right">0.0001646045375984198</td>
</tr>
<tr>
<td class="org-left">deserving</td>
<td class="org-right">2.747328223302838e-05</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org67dd051">
<h2 id="org67dd051">End</h2>
<div class="outline-text-2" id="text-org67dd051">
<p>So, now we have our system. Here are all the prior posts in this series.</p>
<ul class="org-ul">
<li><a href="../auto-complete/">Overview</a></li>
<li><a href="../auto-complete-pre-process-the-data-i/">Pre-Processing I</a></li>
<li><a href="../auto-complete-pre-process-the-data-ii/">Pre-Processing II</a></li>
<li><a href="../auto-complete-the-n-gram-model/">The N-Gram Model</a></li>
<li><a href="../auto-complete-perplexity/">Perplexity</a></li>
</ul>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/auto-complete/" rel="tag">auto-complete</a></li>
<li><a class="tag p-category" href="../../../categories/n-gram/" rel="tag">n-gram</a></li>
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../auto-complete-perplexity/" rel="prev" title="Auto-Complete: Perplexity">Previous post</a></li>
<li class="next"><a href="../word-embeddings-with-the-cbow-model/" rel="next" title="Word Embeddings with the CBOW Model">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer">Scribbles by <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a>
<div id="license" xmlns:cc="http://creativecommons.org/ns#">This work is licensed under <a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" rel="license noopener noreferrer" style="display:inline-block;" target="_blank">CC BY 4.0 <img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"></a></div>
</footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
