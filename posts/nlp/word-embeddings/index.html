<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Word Embeddings for Natural Language Processing." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Word Embeddings | Neurotic Networking</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="../../../assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="../../../rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings/" rel="canonical"><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]-->
<link href="../../../apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="../../../favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="../../../favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="../../../site.webmanifest" rel="manifest">
<meta content="Cloistered Monkey" name="author">
<link href="../tweet-classifier-class/" rel="prev" title="Tweet Classifier Class" type="text/html">
<link href="../pca-exploration/" rel="next" title="PCA Exploration" type="text/html">
<meta content="Neurotic Networking" property="og:site_name">
<meta content="Word Embeddings" property="og:title">
<meta content="https://necromuralist.github.io/Neurotic-Networking/posts/nlp/word-embeddings/" property="og:url">
<meta content="Word Embeddings for Natural Language Processing." property="og:description">
<meta content="article" property="og:type">
<meta content="2020-09-29T19:25:16-07:00" property="article:published_time">
<meta content="nlp" property="article:tag">
<meta content="word embeddings" property="article:tag">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="../../../"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="../../../archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="../../../categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="../../../rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right">
<li class="nav-item"><a class="nav-link" href="index.org" id="sourcelink">Source</a></li>
</ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title" itemprop="headline name"><a class="u-url" href=".">Word Embeddings</a></h1>
<div class="metadata">
<p class="byline author vcard p-author h-card"><span class="byline-name fn p-name" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2020-09-29T19:25:16-07:00" itemprop="datePublished" title="2020-09-29 19:25">2020-09-29 19:25</time></a></p>
<p class="sourceline"><a class="sourcelink" href="index.org">Source</a></p>
</div>
</header>
<div class="e-content entry-content" itemprop="articleBody text">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org720499e">Beginning</a>
<ul>
<li><a href="#org2f3d11f">Set Up</a></li>
<li><a href="#orgcff46b1">The Embeddings</a></li>
</ul>
</li>
<li><a href="#orgc57b4f6">Middle*</a></li>
<li><a href="#orge8f5016">Inspecting the Embeddings</a>
<ul>
<li><a href="#org024c15b">Word Distance</a></li>
<li><a href="#org4822825">Linear Algebra on Word Embeddings</a></li>
<li><a href="#orgffe640b">Predicting Capitals</a></li>
<li><a href="#org8f517d4">More Countries</a></li>
<li><a href="#org3918fe0">Sentence Vectors</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org720499e">
<h2 id="org720499e">Beginning</h2>
<div class="outline-text-2" id="text-org720499e">
<p>This is a walk through a lab for week 3 of Coursera's Natural Language Processing course. It's going to use some pretrained word embeddings to develop some sense of how to use them.</p>
</div>
<div class="outline-3" id="outline-container-org2f3d11f">
<h3 id="org2f3d11f">Set Up</h3>
<div class="outline-text-3" id="text-org2f3d11f"></div>
<div class="outline-4" id="outline-container-org31574ee">
<h4 id="org31574ee">Imports</h4>
<div class="outline-text-4" id="text-org31574ee">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">equal</span><span class="p">,</span>
    <span class="n">expect</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org184cebd">
<h4 id="org184cebd">Plotting</h4>
<div class="outline-text-4" id="text-org184cebd">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
<span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"word-embeddings"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">plot_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"TWITTER_PLOT"</span><span class="p">])</span>
<span class="k">assert</span> <span class="n">plot_path</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>
<span class="k">with</span> <span class="n">plot_path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">Plot</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgcff46b1">
<h3 id="orgcff46b1">The Embeddings</h3>
<div class="outline-text-3" id="text-orgcff46b1">
<p>Like I mentioned above, I'm going to use pre-trained word embeddings that have been pickled so I'll load them here.</p>
<div class="highlight">
<pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">"WORD_EMBEDDINGS"</span><span class="p">])</span>
<span class="k">with</span> <span class="n">path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">reader</span><span class="p">:</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">reader</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="mi">243</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc57b4f6">
<h2 id="orgc57b4f6">Middle*</h2>
</div>
<div class="outline-2" id="outline-container-orge8f5016">
<h2 id="orge8f5016">Inspecting the Embeddings</h2>
<div class="outline-text-2" id="text-orge8f5016">
<p>The <code>embeddings</code> is a dictionary of words to word-vectors that represent them. Here's the first 5 words.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">embeddings</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">())[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
<pre class="example">
&lt;class 'dict'&gt;
['country', 'city', 'China', 'Iraq', 'oil']
</pre>
<div class="highlight">
<pre><span></span><span class="n">vector</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"country"</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">vector</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
<pre class="example">
&lt;class 'numpy.ndarray'&gt;
(300,)
</pre>
<p>Each word-embedding vector has 300 entries.</p>
</div>
<div class="outline-4" id="outline-container-orgf137808">
<h4 id="orgf137808">Plotting</h4>
<div class="outline-text-4" id="text-orgf137808">
<p>Since there are 300 columns you can't easily visualize them without using PCA or some other method, but this is more about getting an intuition as to how the linear-algebra works, so instead we're going to reduce a subset of words to only two columns so that we can plot them.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'oil'</span><span class="p">,</span> <span class="s1">'gas'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'city'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">,</span> <span class="s1">'country'</span><span class="p">,</span> <span class="s1">'continent'</span><span class="p">,</span> <span class="s1">'petroleum'</span><span class="p">,</span> <span class="s1">'joyful'</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">plot_columns</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_columns</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">]</span>
<span class="n">plot_data</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
<span class="n">origins</span> <span class="o">=</span> <span class="n">plot_data</span> <span class="o">*</span> <span class="mi">0</span>
<span class="n">origins</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="n">words</span>
<span class="n">combined_plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">origins</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">])</span>

<span class="n">segment_plot</span> <span class="o">=</span> <span class="n">combined_plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">scatter_plot</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">segment_plot</span> <span class="o">*</span> <span class="n">scatter_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Embeddings Columns 3 and 2"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">font_scale</span>
<span class="p">)</span>
<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"embeddings_segments"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="embeddings_segments.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>You can see that words like "village" and "town" are similar while "city" and "oil" are opposites for whatever reason. Oddly, "joyful" and "country" are also very similar (although I'm only looking at two out of three-hundred columns so that might not be the case once the other columns enter into place).</p>
</div>
</div>
<div class="outline-3" id="outline-container-org024c15b">
<h3 id="org024c15b">Word Distance</h3>
<div class="outline-text-3" id="text-org024c15b">
<p>This is supposed to be a visualization of the difference vectors between <i>sad</i> and <i>happy</i> and <i>town</i> and <i>village</i>, but as far as I can see holoviews doesn't have the equivalent of matplotlib's arrow which lets you use the base coordinate and distance in each dimension to draw arrows, so it's kind of a fake version where I use the points directly. Oh, well.</p>
<div class="highlight">
<pre><span></span><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'sad'</span><span class="p">,</span> <span class="s1">'happy'</span><span class="p">,</span> <span class="s1">'town'</span><span class="p">,</span> <span class="s1">'village'</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">])</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_columns</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"x"</span><span class="p">,</span> <span class="s2">"y"</span><span class="p">]</span>
<span class="n">plot_data</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">words</span>
</pre></div>
<p>This is the fake part - when you take the difference between two "points" it gives you a vector with the base at the origin so you have to add the base point back in to move it from the origin, but then all you're doing is undoing the subtraction, giving you what you started with.</p>
<div class="highlight">
<pre><span></span><span class="n">difference</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span>
    <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"happy"</span><span class="p">]</span> <span class="o">-</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">]</span> <span class="o">+</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">],</span>
    <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"town"</span><span class="p">]</span> <span class="o">-</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"village"</span><span class="p">]</span> <span class="o">+</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">"village"</span><span class="p">]</span>
<span class="p">])</span>

<span class="n">difference</span><span class="p">[</span><span class="s2">"Word"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"sad"</span><span class="p">,</span> <span class="s2">"village"</span><span class="p">]</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">))</span>

<span class="n">difference</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">difference</span><span class="p">,</span>
                            <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_data</span><span class="o">.</span><span class="n">Word</span><span class="o">==</span><span class="s2">"sad"</span><span class="p">],</span>
                            <span class="n">plot_data</span><span class="p">[</span><span class="n">plot_data</span><span class="o">.</span><span class="n">Word</span><span class="o">==</span><span class="s2">"village"</span><span class="p">]])</span>


<span class="n">with_origin</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">origins</span><span class="p">[</span><span class="n">origins</span><span class="o">.</span><span class="n">Word</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">words</span><span class="p">)],</span> <span class="n">plot_data</span><span class="p">])</span>
<span class="n">scatter</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">segments</span> <span class="o">=</span> <span class="n">with_origin</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>
<span class="n">distances</span> <span class="o">=</span> <span class="n">difference</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"x"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"y"</span><span class="p">,</span> <span class="n">by</span><span class="o">=</span><span class="s2">"Word"</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">distances</span> <span class="o">*</span> <span class="n">segments</span> <span class="o">*</span> <span class="n">scatter</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Vector Differences"</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">font_scale</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"vector_differences"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="vector_differences.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
<div class="outline-3" id="outline-container-org4822825">
<h3 id="org4822825">Linear Algebra on Word Embeddings</h3>
<div class="outline-text-3" id="text-org4822825"></div>
<div class="outline-4" id="outline-container-orgddb4a5f">
<h4 id="orgddb4a5f">The <b>norm</b></h4>
<div class="outline-text-4" id="text-orgddb4a5f">
<p>First I'll check out the <a href="https://www.wikiwand.com/en/Norm_(mathematics)">norm</a> of some word vectors using <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a>. This calculates the Euclidean Distance between vectors (but oddly we won't use it here).</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">"town"</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">[</span><span class="s2">"sad"</span><span class="p">]))</span>
</pre></div>
<pre class="example">
2.3858097
2.9004838
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgffe640b">
<h3 id="orgffe640b">Predicting Capitals</h3>
<div class="outline-text-3" id="text-orgffe640b">
<p>Here we'll see how to use the embeddings to predict what country a city is the capital of. To encode the concept of "capital" into a vector we'll use the difference between a specific country and its real capital (in this case <i>France</i> and <i>Paris</i>).</p>
<div class="highlight">
<pre><span></span><span class="n">capital</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"France"</span><span class="p">]</span> <span class="o">-</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"Paris"</span><span class="p">]</span>
</pre></div>
<p>Now that we have the concept of a capital encoded as a word embedding we can add it to the embedding of "Madrid" to get a vector near where "Spain" would be. Note that although there is a "Spain" in the embeddings we're going to use this to see if we can find it without knowing that Madrid is the capital of Spain.</p>
<div class="highlight">
<pre><span></span><span class="n">country</span> <span class="o">=</span> <span class="n">embeddings</span><span class="p">[</span><span class="s2">"Madrid"</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span>
</pre></div>
<p>To make a prediction we have to find the embeddings that are closest to a country. We're going to convert the embeddings to a pandas DataFrame and since our embeddings are a dictionary of arrays we'll have to do a little unpacking first.</p>
<div class="highlight">
<pre><span></span><span class="n">keys</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">embeddings</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">keys</span><span class="p">)</span>
</pre></div>
<p>Now we'll make a function to find the closest embeddings for a word vector.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">closest_word</span><span class="p">(</span><span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">"""Find the word closest to a given vector</span>

<span class="sd">    Args:</span>
<span class="sd">     vector: the vector to match</span>

<span class="sd">    Returns:</span>
<span class="sd">     name of the closest embedding</span>
<span class="sd">    """</span>
    <span class="n">differences</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">-</span> <span class="n">vector</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">differences</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="n">distances</span> <span class="o">=</span> <span class="p">(</span><span class="n">differences</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">"columns"</span><span class="p">)</span>
    <span class="n">expect</span><span class="p">(</span><span class="n">distances</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">differences</span><span class="p">),)))</span>

    <span class="k">return</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)]</span><span class="o">.</span><span class="n">name</span>
</pre></div>
<p>Now we can check what word most closesly matches <i>Madrid + (France - Paris)</i>.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">country</span><span class="p">))</span>
</pre></div>
<pre class="example">
Spain
</pre>
<p>Like magic.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org8f517d4">
<h3 id="org8f517d4">More Countries</h3>
<div class="outline-text-3" id="text-org8f517d4">
<p>What happens if we use a different know country and its capital instead of France and Paris?</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Italy'</span><span class="p">]</span> <span class="o">-</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Rome'</span><span class="p">]</span>
                   <span class="o">+</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Madrid'</span><span class="p">]))</span>
</pre></div>
<pre class="example">
Spain
</pre>
<p>So swapping the capital derivation didn't change the prediction. Now we'll go back to using <code>France - Paris</code> but try different cities.</p>
<div class="highlight">
<pre><span></span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="s2">"Tokyo Moscow"</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> is the capital of </span><span class="si">{</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Tokyo is the capital of Japan
Moscow is the capital of Russia
</pre>
<p>That seems to be working, but here's a case where our search fails.</p>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">'Lisbon'</span><span class="p">]</span> <span class="o">+</span> <span class="n">capital</span><span class="p">))</span>
</pre></div>
<pre class="example">
Lisbon
</pre>
<p>For some reason "Lisbon" is closer to itself than portugal. I tried it with Germany and Italy instead of France as the template capital but it still didn't work. If you try random cities from the embeddings you'll see that a fair amount of them fail.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3918fe0">
<h3 id="org3918fe0">Sentence Vectors</h3>
<div class="outline-text-3" id="text-org3918fe0">
<p>To use this for sentences you construct a vector with all the vectors for each word and then sum up all the columns to get back to a single vector.</p>
<div class="highlight">
<pre><span></span><span class="n">sentence</span> <span class="o">=</span> <span class="s2">"Canada oil city town"</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="p">[</span><span class="n">embeddings</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">]</span>
<span class="n">summed</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">vectors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">closest_word</span><span class="p">(</span><span class="n">summed</span><span class="p">))</span>
</pre></div>
<pre class="example">
city
</pre>
<p>Not exciting, but that's how you do it.</p>
</div>
</div>
</div>
</div>
<aside class="postpromonav">
<nav>
<ul class="tags" itemprop="keywords">
<li><a class="tag p-category" href="../../../categories/nlp/" rel="tag">nlp</a></li>
<li><a class="tag p-category" href="../../../categories/word-embeddings/" rel="tag">word embeddings</a></li>
</ul>
<ul class="pager hidden-print">
<li class="previous"><a href="../tweet-classifier-class/" rel="prev" title="Tweet Classifier Class">Previous post</a></li>
<li class="next"><a href="../pca-exploration/" rel="next" title="PCA Exploration">Next post</a></li>
</ul>
</nav>
</aside>
</article>
<!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="../../../assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
