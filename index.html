<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/" rel="canonical">
<link href="index-21.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
<link href="posts/gans/evaluating-gans/" rel="prefetch" type="text/html">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="."><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item active"><a class="nav-link" href=".">Cloistered Monkey <span class="sr-only">(active)</span></a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/evaluating-gans/">Evaluating GANs</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/evaluating-gans/" rel="bookmark"><time class="published dt-published" datetime="2021-05-16T14:09:17-07:00" itemprop="datePublished" title="2021-05-16 14:09">2021-05-16 14:09</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<p>Write your post here.</p>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/controllable-generation/">Controllable Generation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/controllable-generation/" rel="bookmark"><time class="published dt-published" datetime="2021-05-02T16:23:09-07:00" itemprop="datePublished" title="2021-05-02 16:23">2021-05-02 16:23</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div class="outline-2" id="outline-container-org7cc5cb6">
<h2 id="org7cc5cb6">Controllable Generation</h2>
<div class="outline-text-2" id="text-org7cc5cb6">
<p>In this notebook, we're going to implement a GAN controllability method using gradients from a classifier. By training a classifier to recognize a relevant feature, we can use it to change the generator's inputs (z-vectors) to make it generate images with more or less of that feature.</p>
<p>We will be started we off with a pre-trained generator and classifier, so that we can focus on the controllability aspects.</p>
<p>The classifier has the same archicture as the earlier critic (remember that the discriminator/critic is simply a classifier used to classify real and fake).</p>
</div>
<div class="outline-3" id="outline-container-org947c08e">
<h3 id="org947c08e">CelebA</h3>
<div class="outline-text-3" id="text-org947c08e">
<p>Instead of the MNIST dataset, we will be using <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA</a>. CelebA is a dataset of annotated celebrity images. Since they are colored (not black-and-white), the images have three channels for red, green, and blue (RGB). We'll be using the pre-built <a href="https://pytorch.org/vision/stable/datasets.html?highlight=celeba#torchvision.datasets.CelebA">pytorch Celeba dataset</a>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgb78e197">
<h3 id="orgb78e197">Imports</h3>
<div class="outline-text-3" id="text-orgb78e197">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">argparse</span> <span class="kn">import</span> <span class="n">Namespace</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">pickle</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">CelebA</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgf2b5bd0">
<h3 id="orgf2b5bd0">Set Up</h3>
<div class="outline-text-3" id="text-orgf2b5bd0"></div>
<div class="outline-4" id="outline-container-org22fc2e0">
<h4 id="org22fc2e0">The Timer</h4>
<div class="outline-text-4" id="text-org22fc2e0">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org193c28e">
<h4 id="org193c28e">The Random Seed</h4>
<div class="outline-text-4" id="text-org193c28e">
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org63066c5">
<h4 id="org63066c5">Plotting</h4>
<div class="outline-text-4" id="text-org63066c5">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"controllable-generation"</span>
<span class="n">OUTPUT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"files/posts/gans/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">/"</span>

<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="n">OUTPUT</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5407ffc">
<h4 id="org5407ffc">Paths</h4>
<div class="outline-text-4" id="text-org5407ffc">
<div class="highlight">
<pre><span></span><span class="n">base_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/gans/celeba/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">base_path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()</span>

<span class="n">prebuilt_models</span> <span class="o">=</span> <span class="n">Namespace</span><span class="p">(</span>
    <span class="n">celeba</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">/</span><span class="s2">"pretrained_celeba.pth"</span><span class="p">,</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">base_path</span><span class="o">/</span><span class="s2">"pretrained_classifier.pth"</span>
<span class="p">)</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/pytorch-data/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">data_path</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
    <span class="n">data_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">prebuilt_models</span><span class="o">.</span><span class="n">celeba</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">prebuilt_models</span><span class="o">.</span><span class="n">classifier</span><span class="o">.</span><span class="n">is_file</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org4aaf992">
<h4 id="org4aaf992">Helpers</h4>
<div class="outline-text-4" id="text-org4aaf992">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">save_tensor_images</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                       <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                       <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">/"</span><span class="p">,</span>
                       <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="sd">"""Plot an Image Tensor</span>

<span class="sd">    Args:</span>
<span class="sd">     image_tensor: tensor with the values for the image to plot</span>
<span class="sd">     filename: name to save the file under</span>
<span class="sd">     folder: path to put the file in</span>
<span class="sd">     title: title for the image</span>
<span class="sd">     num_images: how many images from the tensor to use</span>
<span class="sd">     size: the dimensions for each image</span>
<span class="sd">    """</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">image_unflat</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">image_unflat</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="n">nrow</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">folder</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[[file:</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">]]"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orga550d35">
<h3 id="orga550d35">The Generator</h3>
<div class="outline-text-3" id="text-orga550d35">
<p>This is mostly the same as the other Generators but the images are now color so the channels are different and the model has more initial hidden nodes (and one extra hidden block).</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""Generator for the celeba images</span>

<span class="sd">    Args:</span>
<span class="sd">       z_dim: the dimension of the noise vector, a scalar</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span>
<span class="sd">             (CelebA is rgb, so 3 is our default)</span>
<span class="sd">       hidden_dim: the inner dimension, a scalar</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_gen_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                       <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Create a sequence of operations corresponding to a generator block of DCGAN</span>

<span class="sd">        - a transposed convolution</span>
<span class="sd">        - a batchnorm (except in the final layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>
<span class="sd">       Returns:</span>
<span class="sd">        sequence of layers</span>
<span class="sd">       """</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Complete a forward pass of the generator</span>

<span class="sd">       Args:</span>
<span class="sd">       Parameters:</span>
<span class="sd">           noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        generated images.</span>

<span class="sd">       """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">noise</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">noise</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgc64c8c4">
<h4 id="orgc64c8c4">Noise Alias</h4>
<div class="outline-text-4" id="text-orgc64c8c4">
<p>I still don't get thisâ€¦</p>
<div class="highlight">
<pre><span></span><span class="n">get_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8020e00">
<h4 id="org8020e00">Classifier</h4>
<div class="outline-text-4" id="text-org8020e00">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Classifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The Classifier (Discriminator)</span>

<span class="sd">    Args:</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used, a scalar</span>
<span class="sd">             (CelebA is rgb, so 3 is our default)</span>
<span class="sd">       n_classes: the total number of classes in the dataset, an integer scalar</span>
<span class="sd">       hidden_dim: the inner dimension, a scalar</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_classifier_block</span><span class="p">(</span><span class="n">im_chan</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_classifier_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_classifier_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_classifier_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_classifier_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                              <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                              <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Create a sequence of operations corresponding to a classifier block</span>

<span class="sd">        - a convolution</span>
<span class="sd">        - a batchnorm (except in the final layer)</span>
<span class="sd">        - an activation (except in the final layer).</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>

<span class="sd">       Returns:</span>
<span class="sd">        Sequence of layers</span>
<span class="sd">       """</span>
        <span class="k">if</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Complete a forward pass of the classifier</span>

<span class="sd">       Args:</span>
<span class="sd">           image: a flattened image tensor with im_chan channels</span>

<span class="sd">       Returns:</span>
<span class="sd">        an n_classes-dimension tensor representing fake/real.</span>
<span class="sd">       """</span>
        <span class="n">class_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">class_pred</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">class_pred</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org73206dd">
<h2 id="org73206dd">Middle</h2>
<div class="outline-text-2" id="text-org73206dd"></div>
<div class="outline-3" id="outline-container-org0aefe3e">
<h3 id="org0aefe3e">Specifying Parameters</h3>
<div class="outline-text-3" id="text-org0aefe3e">
<p>Before we begin training, we need to specify a few parameters:</p>
<ul class="org-ul">
<li>z<sub>dim</sub>: the dimension of the noise vector</li>
<li>batch<sub>size</sub>: the number of images per forward/backward pass</li>
<li>device: the device type</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">'cuda'</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgec5f4f0">
<h3 id="orgec5f4f0">Train a Classifier</h3>
<div class="outline-text-3" id="text-orgec5f4f0">
<p><b>Note:</b> the <code>Celeba</code> class will sometimes raise an exception:</p>
<pre class="example">
Traceback (most recent call last):
  File "/home/neurotic/download_celeba.py", line 27, in &lt;module&gt;
    CelebA(data_path, split='train', download=True, transform=transform),
  File "/home/neurotic/.conda/envs/neurotic-pytorch/lib/python3.9/site-packages/torchvision/datasets/celeba.py", line 77, in __init__
    self.download()
  File "/home/neurotic/.conda/envs/neurotic-pytorch/lib/python3.9/site-packages/torchvision/datasets/celeba.py", line 131, in download
    with zipfile.ZipFile(os.path.join(self.root, self.base_folder, "img_align_celeba.zip"), "r") as f:
  File "/home/neurotic/.conda/envs/neurotic-pytorch/lib/python3.9/zipfile.py", line 1257, in __init__
    self._RealGetContents()
  File "/home/neurotic/.conda/envs/neurotic-pytorch/lib/python3.9/zipfile.py", line 1324, in _RealGetContents
    raise BadZipFile("File is not a zip file")
zipfile.BadZipFile: File is not a zip file
</pre>
<p>According to <a href="https://github.com/pytorch/vision/issues/2262">this bug report</a> the problem is that the files are stored on Google Drive which has a limit on the amount of data you can download per day and if it's been exceeded then when you try to download =img<sub>align</sub><sub>celeba.zip</sub>" instead of the zip file you get an HTML page (of the same name) with this message:</p>
<pre class="example">
Sorry, you can't view or download this file at this time.

Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.
</pre>
<p>The data is available on <a href="https://www.kaggle.com/jessicali9530/celeba-dataset?select=img_align_celeba">kaggle</a> so if you download it from them and put the file where the bad file is it should work - except of course, it doesn't. It turns out that some of the text files were also replaced with warnings that the download limit was exceeded so I needed to download those as well, but the files on kaggle are formatted as comma-separated files while the original files are space-separated, but even replacing the commas with spaces won't pass the MD5 check - maybe the line endings are different too? Anyway, the images work so I just waited a day and downloaded the text files from the google drive, which seemed to fix it.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train_classifier</span><span class="p">(</span><span class="n">filename</span><span class="p">:</span> <span class="n">Path</span><span class="p">,</span> <span class="n">data_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">,</span> <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                     <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">display_step</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                     <span class="n">classifier</span><span class="p">:</span> <span class="n">Classifier</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="sd">"""Trains the Classifier</span>

<span class="sd">    Args:</span>
<span class="sd">     filename: path to save the state-dict to</span>
<span class="sd">     data_path: path to the celeba data</span>

<span class="sd">    Returns:</span>
<span class="sd">     classifier losses</span>
<span class="sd">    """</span>
    <span class="n">label_indices</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">)</span>

    <span class="n">display_step</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">beta_1</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">beta_2</span> <span class="o">=</span> <span class="mf">0.999</span>
    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
        <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)),</span>
    <span class="p">])</span>

    <span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">CelebA</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">data_path</span><span class="p">),</span> <span class="n">split</span><span class="o">=</span><span class="s1">'train'</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">classifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">label_indices</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">class_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">))</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>

    <span class="n">cur_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">classifier_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># classifier_val_losses = []</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[:,</span> <span class="n">label_indices</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

            <span class="n">class_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">class_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="n">class_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">class_pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">class_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Calculate the gradients</span>
            <span class="n">class_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Update the weights</span>
            <span class="n">classifier_losses</span> <span class="o">+=</span> <span class="p">[</span><span class="n">class_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="c1"># Keep track of the average classifier loss</span>

            <span class="c1">## Visualization code ##</span>
            <span class="k">if</span> <span class="n">classifier_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span><span class="s2">"classifier"</span><span class="p">:</span> <span class="n">classifier</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()},</span> <span class="n">filename</span><span class="p">)</span>
                <span class="n">best_loss</span> <span class="o">=</span> <span class="n">classifier_losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">cur_step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">cur_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">class_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">classifier_losses</span><span class="p">[</span><span class="o">-</span><span class="n">display_step</span><span class="p">:])</span> <span class="o">/</span> <span class="n">display_step</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step </span><span class="si">{</span><span class="n">cur_step</span><span class="si">}</span><span class="s2">: Classifier loss: </span><span class="si">{</span><span class="n">class_mean</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                <span class="n">step_bins</span> <span class="o">=</span> <span class="mi">20</span>
            <span class="n">cur_step</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">classifier_losses</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">classifier_state_dict</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/gans/celeba/trained_classifier.pth"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">classifier_losses</span> <span class="o">=</span> <span class="n">train_classifier</span><span class="p">(</span><span class="n">classifier_state_dict</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
<pre class="example">
Started: 2021-05-10 16:52:26.506156
Step 500: Classifier loss: 0.2693843246996403
Step 1000: Classifier loss: 0.24250468423962593
Step 1500: Classifier loss: 0.2307623517513275
Step 2000: Classifier loss: 0.22525288465619087
Step 2500: Classifier loss: 0.22275795283913613
Step 3000: Classifier loss: 0.2154263758957386
Step 3500: Classifier loss: 0.21474265044927596
Step 4000: Classifier loss: 0.21102887812256813
Step 4500: Classifier loss: 0.20789319404959677
Step 5000: Classifier loss: 0.20887315857410432
Step 5500: Classifier loss: 0.20212965056300164
Step 6000: Classifier loss: 0.20280044555664062
Step 6500: Classifier loss: 0.20041452285647393
Step 7000: Classifier loss: 0.19656063199043275
Step 7500: Classifier loss: 0.19845477828383445
Step 8000: Classifier loss: 0.19205777409672736
Step 8500: Classifier loss: 0.19296112078428268
Step 9000: Classifier loss: 0.19257169529795648
Step 9500: Classifier loss: 0.18672975289821625
Step 10000: Classifier loss: 0.18999777460098266
Step 10500: Classifier loss: 0.18432555946707727
Step 11000: Classifier loss: 0.18430076670646667
Step 11500: Classifier loss: 0.18558891993761062
Step 12000: Classifier loss: 0.17852741411328316
Step 12500: Classifier loss: 0.18172698724269867
Step 13000: Classifier loss: 0.1773110607266426
Step 13500: Classifier loss: 0.17672735232114792
Step 14000: Classifier loss: 0.17991324526071548
Step 14500: Classifier loss: 0.17025035387277604
Step 15000: Classifier loss: 0.17529139894247056
Step 15500: Classifier loss: 0.17104978796839715
Step 16000: Classifier loss: 0.17034502020478248
Step 16500: Classifier loss: 0.17325083956122397
Step 17000: Classifier loss: 0.1642009498178959
Step 17500: Classifier loss: 0.16845661264657974
Step 18000: Classifier loss: 0.1664019832611084
Step 18500: Classifier loss: 0.1633680825829506
Step 19000: Classifier loss: 0.16797509816288947
Step 19500: Classifier loss: 0.15925687023997306
Step 20000: Classifier loss: 0.16292004188895226
Step 20500: Classifier loss: 0.16216046965122222
Step 21000: Classifier loss: 0.15743515598773955
Step 21500: Classifier loss: 0.16243972438573837
Step 22000: Classifier loss: 0.1545857997238636
Step 22500: Classifier loss: 0.15797922548651694
Step 23000: Classifier loss: 0.15804208835959435
Step 23500: Classifier loss: 0.15213489854335785
Step 24000: Classifier loss: 0.15730918619036674
Step 24500: Classifier loss: 0.1511693196296692
Step 25000: Classifier loss: 0.1527680770754814
Step 25500: Classifier loss: 0.15510675182938577
Step 26000: Classifier loss: 0.14683012741804122
Step 26500: Classifier loss: 0.15305917632579805
Step 27000: Classifier loss: 0.14754199008643626
Step 27500: Classifier loss: 0.14820717003941536
Step 28000: Classifier loss: 0.15238315638899802
Step 28500: Classifier loss: 0.14171919177472592
Step 29000: Classifier loss: 0.14881789454817773
Step 29500: Classifier loss: 0.1449408364146948
Step 30000: Classifier loss: 0.1441956951916218
Step 30500: Classifier loss: 0.1483478535115719
Step 31000: Classifier loss: 0.13893532317876817
Step 31500: Classifier loss: 0.1450331158787012
Step 32000: Classifier loss: 0.14139907719194889
Step 32500: Classifier loss: 0.1396861730515957
Step 33000: Classifier loss: 0.1451952086240053
Step 33500: Classifier loss: 0.1358419010192156
Step 34000: Classifier loss: 0.14111693547666074
Step 34500: Classifier loss: 0.1400791739821434
Step 35000: Classifier loss: 0.1358947957903147
Step 35500: Classifier loss: 0.14151665523648263
Step 36000: Classifier loss: 0.1336766537129879
Step 36500: Classifier loss: 0.13722201707959175
Step 37000: Classifier loss: 0.1379301232844591
Step 37500: Classifier loss: 0.13219603390991688
Step 38000: Classifier loss: 0.13811730867624283
Step 38500: Classifier loss: 0.13158722695708275
Step 39000: Classifier loss: 0.13359902986884117
Step 39500: Classifier loss: 0.1366793801188469
Step 40000: Classifier loss: 0.12849617034196853
Step 40500: Classifier loss: 0.13549049003422262
Step 41000: Classifier loss: 0.12929423077404498
Step 41500: Classifier loss: 0.13080933578312398
Step 42000: Classifier loss: 0.13500430592894555
Step 42500: Classifier loss: 0.12454062223434448
Step 43000: Classifier loss: 0.13214491476118564
Step 43500: Classifier loss: 0.1284936859458685
Step 44000: Classifier loss: 0.12763021168112754
Step 44500: Classifier loss: 0.13298917169868946
Step 45000: Classifier loss: 0.12208985219895839
Step 45500: Classifier loss: 0.129048362582922
Step 46000: Classifier loss: 0.12678204217553138
Step 46500: Classifier loss: 0.12455842156708241
Step 47000: Classifier loss: 0.1303500325381756
Step 47500: Classifier loss: 0.12025414818525314
Step 48000: Classifier loss: 0.12684993542730807
Step 48500: Classifier loss: 0.1252559674978256
Step 49000: Classifier loss: 0.12153738121688366
Step 49500: Classifier loss: 0.12777481034398078
Step 50000: Classifier loss: 0.118936713129282
Step 50500: Classifier loss: 0.12405500474572181
Ended: 2021-05-10 18:56:36.980805
Elapsed: 2:04:10.474649
</pre>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Loss</span><span class="o">=</span><span class="n">classifier_losses</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Classifier Loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"classifier_loss"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/controllable-generation/classifier_loss.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
<div class="outline-4" id="outline-container-org2be56be">
<h4 id="org2be56be">Take Two</h4>
<div class="outline-text-4" id="text-org2be56be">
<div class="highlight">
<pre><span></span><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">class_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">classifier_state_dict</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">))[</span><span class="s2">"classifier"</span><span class="p">]</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">class_dict</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">classifier_losses</span> <span class="o">=</span> <span class="n">train_classifier</span><span class="p">(</span><span class="n">classifier_state_dict</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span>
                                         <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                                         <span class="n">classifier</span><span class="o">=</span><span class="n">classifier</span><span class="p">)</span>
</pre></div>
<pre class="example">
Started: 2021-05-11 16:02:16.181203
Step 500: Classifier loss: 0.1181784438341856
Step 1000: Classifier loss: 0.12448641647398472
Step 1500: Classifier loss: 0.1214247584193945
Step 2000: Classifier loss: 0.1198666417747736
Step 2500: Classifier loss: 0.1255625690817833
Step 3000: Classifier loss: 0.11589906251430511
Step 3500: Classifier loss: 0.12224359685182572
Step 4000: Classifier loss: 0.11944249965250492
Step 4500: Classifier loss: 0.1175859476029873
Step 5000: Classifier loss: 0.12318077574670315
Step 5500: Classifier loss: 0.11450052106380462
Step 6000: Classifier loss: 0.11944048409163951
Step 6500: Classifier loss: 0.11928777326643467
Step 7000: Classifier loss: 0.11463624723255635
Step 7500: Classifier loss: 0.12107200682163238
Step 8000: Classifier loss: 0.11355004295706748
Step 8500: Classifier loss: 0.11719673483073711
Step 9000: Classifier loss: 0.11821492326259612
Step 9500: Classifier loss: 0.11198448015749454
Step 10000: Classifier loss: 0.11870198084414005
Step 10500: Classifier loss: 0.11221958647668362
Step 11000: Classifier loss: 0.11476752410829068
Step 11500: Classifier loss: 0.11772396117448806
Step 12000: Classifier loss: 0.10936097744107247
Step 12500: Classifier loss: 0.11677812692523003
Step 13000: Classifier loss: 0.11107682411372662
Step 13500: Classifier loss: 0.11222303664684295
Step 14000: Classifier loss: 0.11760448211431504
Step 14500: Classifier loss: 0.10662877394258977
Step 15000: Classifier loss: 0.11471863305568696
Step 15500: Classifier loss: 0.11056565625965595
Step 16000: Classifier loss: 0.11046012189984322
Step 16500: Classifier loss: 0.1158019468486309
Step 17000: Classifier loss: 0.10568901741504669
Step 17500: Classifier loss: 0.11223984396457672
Step 18000: Classifier loss: 0.11002579489350318
Step 18500: Classifier loss: 0.10752195838093757
Step 19000: Classifier loss: 0.11419818633794784
Step 19500: Classifier loss: 0.10464896529912948
Step 20000: Classifier loss: 0.11005591739714146
Step 20500: Classifier loss: 0.10996675519645215
Step 21000: Classifier loss: 0.10543355357646943
Step 21500: Classifier loss: 0.11205300988256932
Step 22000: Classifier loss: 0.1038715885579586
Step 22500: Classifier loss: 0.10818033437430859
Step 23000: Classifier loss: 0.10912492156028747
Step 23500: Classifier loss: 0.10302072758972645
Step 24000: Classifier loss: 0.11008756360411644
Step 24500: Classifier loss: 0.10342664630711079
Step 25000: Classifier loss: 0.10618587562441825
Step 25500: Classifier loss: 0.10913233712315559
Step 26000: Classifier loss: 0.10061963592469693
Step 26500: Classifier loss: 0.10828037586808205
Step 27000: Classifier loss: 0.10266246040165425
Step 27500: Classifier loss: 0.1047897623181343
Step 28000: Classifier loss: 0.10866250747442245
Step 28500: Classifier loss: 0.09820086953043938
Step 29000: Classifier loss: 0.10674160474538803
Step 29500: Classifier loss: 0.10230921612679958
Step 30000: Classifier loss: 0.1021555609256029
Step 30500: Classifier loss: 0.10775842162966728
Step 31000: Classifier loss: 0.09722121758759021
Step 31500: Classifier loss: 0.10439497400820255
Step 32000: Classifier loss: 0.10229390095174312
Step 32500: Classifier loss: 0.10003190772235393
Step 33000: Classifier loss: 0.10617333140969276
Step 33500: Classifier loss: 0.09686395044624806
Step 34000: Classifier loss: 0.10285020883381367
Step 34500: Classifier loss: 0.10199978332221508
Step 35000: Classifier loss: 0.09819360673427582
Step 35500: Classifier loss: 0.10397693109512329
Step 36000: Classifier loss: 0.09642438031733036
Step 36500: Classifier loss: 0.10087257397174836
Step 37000: Classifier loss: 0.10197833214700222
Step 37500: Classifier loss: 0.09598418261110783
Step 38000: Classifier loss: 0.10283542364835739
Step 38500: Classifier loss: 0.09644483177363873
Step 39000: Classifier loss: 0.09908602401614189
Step 39500: Classifier loss: 0.10129908196628094
Step 40000: Classifier loss: 0.0939527989178896
Step 40500: Classifier loss: 0.1016722819507122
Step 41000: Classifier loss: 0.09578396078944207
Step 41500: Classifier loss: 0.09706279496848583
Step 42000: Classifier loss: 0.10207961940765381
Step 42500: Classifier loss: 0.09211373472213745
Step 43000: Classifier loss: 0.09958744782209396
Step 43500: Classifier loss: 0.09534277887642384
Step 44000: Classifier loss: 0.0952163600474596
Step 44500: Classifier loss: 0.10136887782812118
Step 45000: Classifier loss: 0.09021547995507717
Step 45500: Classifier loss: 0.09812712541222572
Step 46000: Classifier loss: 0.09560927426815033
Step 46500: Classifier loss: 0.09358323478698731
Step 47000: Classifier loss: 0.09991893386840821
Step 47500: Classifier loss: 0.0899157041311264
Step 48000: Classifier loss: 0.096542285323143
Step 48500: Classifier loss: 0.09535252919793129
Step 49000: Classifier loss: 0.09194727616012097
Step 49500: Classifier loss: 0.09831891848146915
Step 50000: Classifier loss: 0.0901611197590828
Step 50500: Classifier loss: 0.09490065774321556
Ended: 2021-05-11 18:06:19.399986
Elapsed: 2:04:03.218783
</pre>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Loss</span><span class="o">=</span><span class="n">classifier_losses</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Classifier Loss Session 2"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"classifier_loss_2"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/controllable-generation/classifier_loss_2.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
<div class="outline-4" id="outline-container-org2c34d19">
<h4 id="org2c34d19">Take Three</h4>
<div class="outline-text-4" id="text-org2c34d19">
<div class="highlight">
<pre><span></span><span class="n">n_classes</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">class_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">classifier_state_dict</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">))[</span><span class="s2">"classifier"</span><span class="p">]</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">class_dict</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">classifier_losses</span> <span class="o">=</span> <span class="n">train_classifier</span><span class="p">(</span><span class="n">classifier_state_dict</span><span class="p">,</span> <span class="n">data_path</span><span class="p">,</span>
                                         <span class="n">epochs</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                                         <span class="n">classifier</span><span class="o">=</span><span class="n">classifier</span><span class="p">)</span>
</pre></div>
<pre class="example">
Started: 2021-05-11 21:11:32.420506
Step 500: Classifier loss: 0.09006546361744404
Step 1000: Classifier loss: 0.09647199404239655
Step 1500: Classifier loss: 0.092734768897295
Step 2000: Classifier loss: 0.09196118661761284
Step 2500: Classifier loss: 0.09789373110234738
Step 3000: Classifier loss: 0.08788785541057587
Step 3500: Classifier loss: 0.0945415845811367
Step 4000: Classifier loss: 0.09308994428813458
Step 4500: Classifier loss: 0.09022623193264008
Step 5000: Classifier loss: 0.09615834753215313
Step 5500: Classifier loss: 0.08742603194713593
Step 6000: Classifier loss: 0.09316775412857532
Step 6500: Classifier loss: 0.09275233449041843
Step 7000: Classifier loss: 0.08822614887356758
Step 7500: Classifier loss: 0.09528714890778064
Step 8000: Classifier loss: 0.08681275172531605
Step 8500: Classifier loss: 0.09150236696004868
Step 9000: Classifier loss: 0.09338522186875343
Step 9500: Classifier loss: 0.08638478130102158
Step 10000: Classifier loss: 0.09388372772932052
Step 10500: Classifier loss: 0.08720742921531201
Step 11000: Classifier loss: 0.09009483934938908
Step 11500: Classifier loss: 0.0929495030939579
Step 12000: Classifier loss: 0.08460890363156795
Step 12500: Classifier loss: 0.0924714410007
Step 13000: Classifier loss: 0.08704712373018265
Step 13500: Classifier loss: 0.08819058662652969
Step 14000: Classifier loss: 0.09366303083300591
Step 14500: Classifier loss: 0.08295501434803008
Step 15000: Classifier loss: 0.09084490737318993
Step 15500: Classifier loss: 0.08707242746651173
Step 16000: Classifier loss: 0.08690852355957031
Step 16500: Classifier loss: 0.09254233407974242
Step 17000: Classifier loss: 0.08242024271190167
Step 17500: Classifier loss: 0.08904271678626538
Step 18000: Classifier loss: 0.08771026766300201
Step 18500: Classifier loss: 0.08471861970424652
Step 19000: Classifier loss: 0.09134728060662746
Step 19500: Classifier loss: 0.08233513435721397
Step 20000: Classifier loss: 0.08778411850333213
Step 20500: Classifier loss: 0.08791485584527255
Step 21000: Classifier loss: 0.08345357306301594
Step 21500: Classifier loss: 0.08975999920070171
Step 22000: Classifier loss: 0.08225472408533097
Step 22500: Classifier loss: 0.08668080273270606
Step 23000: Classifier loss: 0.08786206224560737
Step 23500: Classifier loss: 0.08155409483611584
Step 24000: Classifier loss: 0.08907847443222999
Step 24500: Classifier loss: 0.08202618369460106
Step 25000: Classifier loss: 0.08517973597347736
Step 25500: Classifier loss: 0.08817093770205975
Step 26000: Classifier loss: 0.08008052316308022
Step 26500: Classifier loss: 0.08741954331099987
Step 27000: Classifier loss: 0.08247932478785515
Step 27500: Classifier loss: 0.08377225384116173
Step 28000: Classifier loss: 0.08846944206953049
Step 28500: Classifier loss: 0.07859189368784428
Step 29000: Classifier loss: 0.08617163190245629
Step 29500: Classifier loss: 0.0824531610161066
Step 30000: Classifier loss: 0.08195052224397659
Step 30500: Classifier loss: 0.08803890940546989
Step 31000: Classifier loss: 0.07793828934431075
Step 31500: Classifier loss: 0.08464510484039783
Step 32000: Classifier loss: 0.08275749842077494
Step 32500: Classifier loss: 0.0805082704871893
Step 33000: Classifier loss: 0.08703124921023846
Step 33500: Classifier loss: 0.0772736611738801
Step 34000: Classifier loss: 0.08353734220564366
Step 34500: Classifier loss: 0.08343685203790664
Step 35000: Classifier loss: 0.07905932680517436
Step 35500: Classifier loss: 0.08568261863291264
Step 36000: Classifier loss: 0.07762402860075235
Step 36500: Classifier loss: 0.08223582464456558
Step 37000: Classifier loss: 0.08341778349876404
Step 37500: Classifier loss: 0.07801838412880897
Step 38000: Classifier loss: 0.0842266542762518
Step 38500: Classifier loss: 0.07764634099602699
Step 39000: Classifier loss: 0.08104524739086628
Step 39500: Classifier loss: 0.08389902476221323
Step 40000: Classifier loss: 0.07612183248996734
Step 40500: Classifier loss: 0.08296740844845772
Step 41000: Classifier loss: 0.0781253460124135
Step 41500: Classifier loss: 0.07980525248497725
Step 42000: Classifier loss: 0.08405549557507039
Step 42500: Classifier loss: 0.0743530157059431
Step 43000: Classifier loss: 0.08219673927128315
Step 43500: Classifier loss: 0.07845095673948527
Step 44000: Classifier loss: 0.07780187250673772
Step 44500: Classifier loss: 0.08399353076517582
Step 45000: Classifier loss: 0.07365029990673065
Step 45500: Classifier loss: 0.0808380290567875
Step 46000: Classifier loss: 0.0786423703506589
Step 46500: Classifier loss: 0.07693013155460357
Step 47000: Classifier loss: 0.08244228959083558
Step 47500: Classifier loss: 0.07365631985664367
Step 48000: Classifier loss: 0.07970952866971492
Step 48500: Classifier loss: 0.07868134459108114
Step 49000: Classifier loss: 0.07539512529224157
Step 49500: Classifier loss: 0.08191524033248425
Step 50000: Classifier loss: 0.07361406400799751
Step 50500: Classifier loss: 0.07847459720075131
Ended: 2021-05-11 23:15:24.444278
Elapsed: 2:03:52.023772
</pre>
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Loss</span><span class="o">=</span><span class="n">classifier_losses</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Classifier Loss Session 3"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"classifier_loss_3"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/controllable-generation/classifier_loss_2.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
</div>
<div class="outline-3" id="outline-container-org3310b64">
<h3 id="org3310b64">Loading the Pretrained Models</h3>
<div class="outline-text-3" id="text-org3310b64">
<p>We will then load the pretrained generator and classifier using the following code. (If we trained our own classifier, we can load that one here instead.)</p>
<div class="highlight">
<pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">gen_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">prebuilt_models</span><span class="o">.</span><span class="n">celeba</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">))[</span><span class="s2">"gen"</span><span class="p">]</span>
<span class="n">gen</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">gen_dict</span><span class="p">)</span>
<span class="n">gen</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">Classifier</span><span class="p">(</span><span class="n">n_classes</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">class_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">prebuilt_models</span><span class="o">.</span><span class="n">classifier</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">))[</span><span class="s2">"classifier"</span><span class="p">]</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">class_dict</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">classifier</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge8f3af8">
<h3 id="orge8f3af8">Training</h3>
<div class="outline-text-3" id="text-orge8f3af8">
<p>Now we can start implementing a method for controlling our GAN.</p>
</div>
<div class="outline-4" id="outline-container-org9b12555">
<h4 id="org9b12555">Update Noise</h4>
<div class="outline-text-4" id="text-org9b12555">
<p>For training, we need to write the code to update the noise to produce more of our desired feature. We do this by performing stochastic gradient ascent. We use stochastic gradient ascent to find the local maxima, as opposed to stochastic gradient descent which finds the local minima. Gradient ascent is gradient descent over the negative of the value being optimized. Their formulas are essentially the same, however, instead of subtracting the weighted value, stochastic gradient ascent adds it; it can be calculated by \(new = old + (âˆ‡ old * weight)\), where âˆ‡ is the gradient of <code>old</code>. We perform stochastic gradient ascent to try and maximize the amount of the feature we want. If we wanted to reduce the amount of the feature, we would perform gradient descent. However, in this assignment we are interested in maximize our feature using gradient ascent, since many features in the dataset are not present much more often than they're present and we are trying to add a feature to the images, not remove.</p>
<p>Given the noise with its gradient already calculated through the classifier, we want to return the new noise vector.</p>
<ol class="org-ol">
<li>Remember the equation for gradient ascent: \(new = old + (âˆ‡ old * weight)\).</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">calculate_updated_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Update noise vectors with stochastic gradient ascent.</span>

<span class="sd">    Args:</span>
<span class="sd">     noise: the current noise vectors. </span>
<span class="sd">           We have already called the backwards function on the target class</span>
<span class="sd">           so we can access the gradient of the output class with respect </span>
<span class="sd">           to the noise by using noise.grad</span>
<span class="sd">     weight: the scalar amount by which we should weight the noise gradient</span>

<span class="sd">    Returns:</span>
<span class="sd">     updated noise</span>
<span class="sd">    """</span>
    <span class="n">new_noise</span> <span class="o">=</span> <span class="n">noise</span> <span class="o">+</span> <span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_noise</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orgcf43b6a"></a>UNIT TEST<br>
<div class="outline-text-5" id="text-orgcf43b6a">
<p>Check that the basic function works.</p>
<div class="highlight">
<pre><span></span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">noise</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">fake_classes</span> <span class="o">=</span> <span class="p">(</span><span class="n">noise</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">fake_classes</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">new_noise</span> <span class="o">=</span> <span class="n">calculate_updated_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">new_noise</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">new_noise</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">new_noise</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">==</span> <span class="mf">2.0010</span>
<span class="k">assert</span> <span class="n">new_noise</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">==</span> <span class="mf">2.0010</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">new_noise</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span> <span class="o">+</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>Check that it works for generated images</p>
<div class="highlight">
<pre><span></span><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">get_noise</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
<span class="n">fake_classes</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">fake</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">fake_classes</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">noise</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">calculate_updated_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
<span class="n">fake_classes_new</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">fake</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">fake_classes_new</span> <span class="o">&gt;</span> <span class="n">fake_classes</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org84e0240">
<h3 id="org84e0240">Generation</h3>
<div class="outline-text-3" id="text-org84e0240">
<p>Now, we can use the classifier along with stochastic gradient ascent to make noise that generates more of a certain feature. In the code given to us here, we can generate smiling faces. Feel free to change the target index and control some of the other features in the list! We will notice that some features are easier to detect and control than others.</p>
<p>The list we have here are the features labeled in CelebA, which we used to train our classifier. If we wanted to control another feature, we would need to get data that is labeled with that feature and train a classifier on that feature.</p>
<p>First generate a bunch of images with the generator.</p>
<div class="highlight">
<pre><span></span><span class="n">n_images</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">fake_image_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">grad_steps</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># Number of gradient steps to take</span>
<span class="n">skip</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># Number of gradient steps to skip in the visualization</span>

<span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"5oClockShadow"</span><span class="p">,</span> <span class="s2">"ArchedEyebrows"</span><span class="p">,</span> <span class="s2">"Attractive"</span><span class="p">,</span> <span class="s2">"BagsUnderEyes"</span><span class="p">,</span> <span class="s2">"Bald"</span><span class="p">,</span> <span class="s2">"Bangs"</span><span class="p">,</span>
<span class="s2">"BigLips"</span><span class="p">,</span> <span class="s2">"BigNose"</span><span class="p">,</span> <span class="s2">"BlackHair"</span><span class="p">,</span> <span class="s2">"BlondHair"</span><span class="p">,</span> <span class="s2">"Blurry"</span><span class="p">,</span> <span class="s2">"BrownHair"</span><span class="p">,</span> <span class="s2">"BushyEyebrows"</span><span class="p">,</span> <span class="s2">"Chubby"</span><span class="p">,</span>
<span class="s2">"DoubleChin"</span><span class="p">,</span> <span class="s2">"Eyeglasses"</span><span class="p">,</span> <span class="s2">"Goatee"</span><span class="p">,</span> <span class="s2">"GrayHair"</span><span class="p">,</span> <span class="s2">"HeavyMakeup"</span><span class="p">,</span> <span class="s2">"HighCheekbones"</span><span class="p">,</span> <span class="s2">"Male"</span><span class="p">,</span> 
<span class="s2">"MouthSlightlyOpen"</span><span class="p">,</span> <span class="s2">"Mustache"</span><span class="p">,</span> <span class="s2">"NarrowEyes"</span><span class="p">,</span> <span class="s2">"NoBeard"</span><span class="p">,</span> <span class="s2">"OvalFace"</span><span class="p">,</span> <span class="s2">"PaleSkin"</span><span class="p">,</span> <span class="s2">"PointyNose"</span><span class="p">,</span> 
<span class="s2">"RecedingHairline"</span><span class="p">,</span> <span class="s2">"RosyCheeks"</span><span class="p">,</span> <span class="s2">"Sideburn"</span><span class="p">,</span> <span class="s2">"Smiling"</span><span class="p">,</span> <span class="s2">"StraightHair"</span><span class="p">,</span> <span class="s2">"WavyHair"</span><span class="p">,</span> <span class="s2">"WearingEarrings"</span><span class="p">,</span> 
<span class="s2">"WearingHat"</span><span class="p">,</span> <span class="s2">"WearingLipstick"</span><span class="p">,</span> <span class="s2">"WearingNecklace"</span><span class="p">,</span> <span class="s2">"WearingNecktie"</span><span class="p">,</span> <span class="s2">"Weng"</span><span class="p">]</span>

<span class="c1">### Change me! ###</span>
<span class="n">target_indices</span> <span class="o">=</span> <span class="n">feature_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">"Weng"</span><span class="p">)</span> <span class="c1"># Feel free to change this value to any string from feature_names!</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">get_noise</span><span class="p">(</span><span class="n">n_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grad_steps</span><span class="p">):</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">fake_image_history</span> <span class="o">+=</span> <span class="p">[</span><span class="n">fake</span><span class="p">]</span>
    <span class="n">fake_classes_score</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">fake</span><span class="p">)[:,</span> <span class="n">target_indices</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">fake_classes_score</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">calculate_updated_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">grad_steps</span><span class="p">)</span>

<span class="n">pyplot</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_images</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">grad_steps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">save_tensor_images</span><span class="p">(</span><span class="n">image_tensor</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">fake_image_history</span><span class="p">[::</span><span class="n">skip</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> 
<span class="n">filename</span><span class="o">=</span><span class="s2">"weng.png"</span><span class="p">,</span> <span class="n">folder</span><span class="o">=</span><span class="n">OUTPUT</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Weng"</span><span class="p">,</span>
<span class="n">num_images</span><span class="o">=</span><span class="n">n_images</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="n">n_images</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="weng.png" src="posts/gans/controllable-generation/weng.png"></p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org9eba848">
<h3 id="org9eba848">Entanglement and Regularization</h3>
<div class="outline-text-3" id="text-org9eba848">
<p>We may also notice that sometimes more features than just the target feature change. This is because some features are entangled. To fix this, we can try to isolate the target feature more by holding the classes outside of the target class constant. One way we can implement this is by penalizing the differences from the original class with L2 regularization. This L2 regularization would apply a penalty for this difference using the L2 norm and this would just be an additional term on the loss function.</p>
<p>Here, we'll have to implement the score function: the higher, the better. The score is calculated by adding the target score and a penalty â€“ note that the penalty is meant to lower the score, so it should have a negative value.</p>
<p>For every non-target class, take the difference between the current noise and the old noise. The greater this value is, the more features outside the target have changed. We will calculate the magnitude of the change, take the mean, and negate it. Finally, add this penalty to the target score. The target score is the mean of the target class in the current noise.</p>
<ol class="org-ol">
<li>The higher the score, the better!</li>
<li>We want to calculate the loss per image, so we'll need to pass a dim argument to <a href="https://pytorch.org/docs/stable/generated/torch.norm.html"><code>torch.norm</code></a>.</li>
<li>Calculating the magnitude of the change requires we to take the norm of the difference between the classifications, not the difference of the norms.</li>
</ol>
<p><b>Note:</b> <code>torch.norm</code> is deprecated, they want you to use <a href="https://pytorch.org/docs/stable/linalg.html#torch.linalg.norm">torch.linalg.norm</a> instead.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_score</span><span class="p">(</span><span class="n">current_classifications</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">original_classifications</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">target_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">other_indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
              <span class="n">penalty_weight</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Score the current classifications, L2 Norm penalty</span>

<span class="sd">    Args:</span>
<span class="sd">       current_classifications: the classifications associated with the current noise</span>
<span class="sd">       original_classifications: the classifications associated with the original noise</span>
<span class="sd">       target_indices: the index of the target class</span>
<span class="sd">       other_indices: the indices of the other classes</span>
<span class="sd">       penalty_weight: the amount that the penalty should be weighted in the overall score</span>

<span class="sd">    Returns: </span>
<span class="sd">     the score of the current classification with L2 Norm penalty</span>
<span class="sd">    """</span>
    <span class="c1"># Steps: 1) Calculate the change between the original and current classifications (as a tensor)</span>
    <span class="c1">#           by indexing into the other_indices we're trying to preserve, like in x[:, features].</span>
    <span class="c1">#        2) Calculate the norm (magnitude) of changes per example.</span>
    <span class="c1">#        3) Multiply the mean of the example norms by the penalty weight. </span>
    <span class="c1">#           This will be our other_class_penalty.</span>
    <span class="c1">#           Make sure to negate the value since it's a penalty!</span>
    <span class="c1">#        4) Take the mean of the current classifications for the target feature over all the examples.</span>
    <span class="c1">#           This mean will be our target_score.</span>
    <span class="c1"># Calculate the norm (magnitude) of changes per example and multiply by penalty weight</span>
    <span class="n">other_class_penalty</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">original_classifications</span><span class="p">[:,</span> <span class="n">other_indices</span><span class="p">]</span>
                          <span class="o">-</span> <span class="n">current_classifications</span><span class="p">[:,</span> <span class="n">other_indices</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
                           <span class="o">*</span> <span class="n">penalty_weight</span><span class="p">)</span>
    <span class="c1"># Take the mean of the current classifications for the target feature</span>
    <span class="n">target_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">current_classifications</span><span class="p">[:,</span> <span class="n">target_indices</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">target_score</span> <span class="o">+</span> <span class="n">other_class_penalty</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org37f5a9d">
<h4 id="org37f5a9d">UNIT TEST</h4>
<div class="outline-text-4" id="text-org37f5a9d">
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span>
    <span class="n">get_score</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">),</span> 
    <span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.2</span>
<span class="p">)</span>
<span class="n">rows</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">current_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">original_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># Must be 3</span>
<span class="k">assert</span> <span class="n">get_score</span><span class="p">(</span><span class="n">current_class</span><span class="p">,</span> <span class="n">original_class</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span>

<span class="n">current_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">original_class</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<span class="c1"># Must be 3 - 0.2 * sqrt(10)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">get_score</span><span class="p">(</span><span class="n">current_class</span><span class="p">,</span> <span class="n">original_class</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mf">0.2</span><span class="p">),</span> 
                     <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.2</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
<p>In the following block of code, we will run the gradient ascent with this new score function. We might notice a few things after running it:</p>
<ol class="org-ol">
<li>It may fail more often at producing the target feature when compared to the original approach. This suggests that the model may not be able to generate an image that has the target feature without changing the other features. This makes sense! For example, it may not be able to generate a face that's smiling but whose mouth is NOT slightly open. This may also expose a limitation of the generator.</li>
</ol>
<p>Alternatively, even if the generator can produce an image with the intended features, it might require many intermediate changes to get there and may get stuck in a local minimum.</p>
<ol class="org-ol">
<li>This process may change features which the classifier was not trained to recognize since there is no way to penalize them with this method. Whether it's possible to train models to avoid changing unsupervised features is an open question.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="n">fake_image_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">### Change me! ###</span>
<span class="n">target_indices</span> <span class="o">=</span> <span class="n">feature_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">"Goatee"</span><span class="p">)</span> <span class="c1"># Feel free to change this value to any string from feature_names from earlier!</span>
<span class="n">other_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">cur_idx</span> <span class="o">!=</span> <span class="n">target_indices</span> <span class="k">for</span> <span class="n">cur_idx</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)]</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">get_noise</span><span class="p">(</span><span class="n">n_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">original_classifications</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">grad_steps</span><span class="p">):</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">fake_image_history</span> <span class="o">+=</span> <span class="p">[</span><span class="n">fake</span><span class="p">]</span>
    <span class="n">fake_score</span> <span class="o">=</span> <span class="n">get_score</span><span class="p">(</span>
        <span class="n">classifier</span><span class="p">(</span><span class="n">fake</span><span class="p">),</span> 
        <span class="n">original_classifications</span><span class="p">,</span>
        <span class="n">target_indices</span><span class="p">,</span>
        <span class="n">other_indices</span><span class="p">,</span>
        <span class="n">penalty_weight</span><span class="o">=</span><span class="mf">0.1</span>
    <span class="p">)</span>
    <span class="n">fake_score</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">noise</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">calculate_updated_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">grad_steps</span><span class="p">)</span>

<span class="n">pyplot</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">'figure.figsize'</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">n_images</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">grad_steps</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">save_tensor_images</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">fake_image_history</span><span class="p">[::</span><span class="n">skip</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">num_images</span><span class="o">=</span><span class="n">n_images</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="n">n_images</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">"goatee.png"</span><span class="p">,</span> <span class="n">folder</span><span class="o">=</span><span class="n">OUTPUT</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Goatee"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="goatee.png" src="posts/gans/controllable-generation/goatee.png"></p>
</div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org4e217ec">
<h2 id="org4e217ec">End</h2>
<div class="outline-text-2" id="text-org4e217ec"></div>
<div class="outline-3" id="outline-container-orgb055d32">
<h3 id="orgb055d32">Sources</h3>
<div class="outline-text-3" id="text-orgb055d32">
<ul class="org-ul">
<li>Liu, Z, Luo, P, Wang, X, Tang, X, Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV) 2015 .</li>
</ul>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/a-conditional-gan/">A Conditional GAN</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/a-conditional-gan/" rel="bookmark"><time class="published dt-published" datetime="2021-04-24T14:34:07-07:00" itemprop="datePublished" title="2021-04-24 14:34">2021-04-24 14:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div class="outline-2" id="outline-container-org5b32c2d">
<h2 id="org5b32c2d">Build You a Conditional GAN For a Great Good</h2>
<div class="outline-text-2" id="text-org5b32c2d"></div>
<div class="outline-3" id="outline-container-orgf3b0466">
<h3 id="orgf3b0466">Imports</h3>
<div class="outline-text-3" id="text-orgf3b0466">
<div class="highlight">
<pre><span></span><span class="c1"># python standard library</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgc1b397e">
<h3 id="orgc1b397e">Set Up</h3>
<div class="outline-text-3" id="text-orgc1b397e"></div>
<div class="outline-4" id="outline-container-org9992622">
<h4 id="org9992622">The Timer</h4>
<div class="outline-text-4" id="text-org9992622">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgcee8f52">
<h4 id="orgcee8f52">The Manual Seed</h4>
<div class="outline-text-4" id="text-orgcee8f52">
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org803ae63">
<h4 id="org803ae63">Plotting</h4>
<div class="outline-text-4" id="text-org803ae63">
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"a-conditional-gan"</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7c7f248">
<h3 id="org7c7f248">Helpers</h3>
<div class="outline-text-3" id="text-org7c7f248">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">save_tensor_images</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                       <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                       <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
                       <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)):</span>
    <span class="sd">"""Plot an Image Tensor</span>

<span class="sd">    Args:</span>
<span class="sd">     image_tensor: tensor with the values for the image to plot</span>
<span class="sd">     filename: name to save the file under</span>
<span class="sd">     folder: path to put the file in</span>
<span class="sd">     title: title for the image</span>
<span class="sd">     num_images: how many images from the tensor to use</span>
<span class="sd">     size: the dimensions for each image</span>
<span class="sd">    """</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">image_unflat</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">image_unflat</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">folder</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[[file:</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">]]"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org54ae9dd">
<h4 id="org54ae9dd">Noise</h4>
<div class="outline-text-4" id="text-org54ae9dd">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_some_noise</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Alias for torch.randn</span>

<span class="sd">    Args:</span>
<span class="sd">      n_samples: the number of samples to generate</span>
<span class="sd">      z_dim: the dimension of the noise vector</span>
<span class="sd">      device: the device type</span>

<span class="sd">    Returns:</span>
<span class="sd">     tensor with random numbers from the normal distribution.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org313aa3b">
<h2 id="org313aa3b">Middle</h2>
<div class="outline-text-2" id="text-org313aa3b"></div>
<div class="outline-3" id="outline-container-org0dbb8da">
<h3 id="org0dbb8da">The Generator</h3>
<div class="outline-text-3" id="text-org0dbb8da">
<p>The Generator and Discriminator are the same ones we used before except the <code>z_dim</code> attribute has been renamed <code>input_dim</code> to reflect the fact that the data is going to be augmented with the classification information.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The DCGAN Generator</span>

<span class="sd">    Args:</span>
<span class="sd">       input_dim: the dimension of the input vector</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is your default)</span>
<span class="sd">       hidden_dim: the inner dimension,</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gen</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_gen_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                       <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Creates a block for the generator (sub sequence)</span>

<span class="sd">       The parts</span>
<span class="sd">        - a transposed convolution</span>
<span class="sd">        - a batchnorm (except for in the last layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>

<span class="sd">       Returns:</span>
<span class="sd">        the sub-sequence of layers</span>
<span class="sd">       """</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""complete a forward pass of the generator: Given a noise tensor, </span>

<span class="sd">       Args:</span>
<span class="sd">        noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        generated images.</span>
<span class="sd">       """</span>
        <span class="c1"># unsqueeze the noise</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">noise</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">noise</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgfba8244">
<h3 id="orgfba8244">Discriminator</h3>
<div class="outline-text-3" id="text-orgfba8244">
<p>This differs a little from the DCGAN Discriminator in that the initial hidden dimension output goes up to 64 nodes from 16 in the original.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The DCGAN Discriminator</span>

<span class="sd">    Args:</span>
<span class="sd">     im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is the default)</span>
<span class="sd">     hidden_dim: the inner dimension,</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">im_chan</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">make_disc_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Make a sub-block of layers for the discriminator</span>

<span class="sd">        - a convolution</span>
<span class="sd">        - a batchnorm (except for in the last layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">         input_channels: how many channels the input feature representation has</span>
<span class="sd">         output_channels: how many channels the output feature representation should have</span>
<span class="sd">         kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">         stride: the stride of the convolution</span>
<span class="sd">         final_layer: if true it is the final layer and otherwise not</span>
<span class="sd">                     (affects activation and batchnorm)</span>
<span class="sd">       """</span>        
        <span class="c1"># Build the neural block</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Final Layer</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Complete a forward pass of the discriminator</span>

<span class="sd">       Args:</span>
<span class="sd">         image: a flattened image tensor with dimension (im_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        a 1-dimension tensor representing fake/real.</span>
<span class="sd">       """</span>
        <span class="n">disc_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">disc_pred</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">disc_pred</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org8ab75f5">
<h3 id="org8ab75f5">The Class Input</h3>
<div class="outline-text-3" id="text-org8ab75f5"></div>
<div class="outline-4" id="outline-container-org14550bf">
<h4 id="org14550bf">One-Hot Encoder</h4>
<div class="outline-text-4" id="text-org14550bf">
<p>In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.</p>
<ol class="org-ol">
<li>This code can be done in one line.</li>
<li>pytorch documentation for <a href="https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot">F.one<sub>hot</sub></a></li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_one_hot_labels</span><span class="p">(</span><span class="n">labels</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Create one-hot vectors for the labels</span>

<span class="sd">    Args:</span>
<span class="sd">       labels: tensor of labels from the dataloader</span>
<span class="sd">       n_classes: the total number of classes in the dataset</span>

<span class="sd">    Returns:</span>
<span class="sd">     a tensor of shape (labels size, num_classes).</span>
<span class="sd">    """</span>
    <span class="c1">#### START CODE HERE ####</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>
    <span class="c1">#### END CODE HERE ####</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="p">(</span>
    <span class="n">get_one_hot_labels</span><span class="p">(</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span>
        <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> 
    <span class="p">[[</span>
      <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
      <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="p">]]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org808fc2c">
<h4 id="org808fc2c">Combine Vectors</h4>
<div class="outline-text-4" id="text-org808fc2c">
<p>Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.</p>
<ol class="org-ol">
<li>This code can also be written in one line.</li>
<li>See the documentation <a href="https://pytorch.org/docs/master/generated/torch.cat.html">torch.cat</a> ( Specifically, look at what the <code>dim</code> argument of <code>torch.cat</code> does)</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">combine_vectors</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Combine two vectors with shapes (n_samples, ?) and (n_samples, ?).</span>

<span class="sd">    Args:</span>
<span class="sd">      x: the first vector. </span>
<span class="sd">      y: the second vector.</span>
<span class="sd">    """</span>
    <span class="c1"># Note: Make sure this function outputs a float no matter what inputs it receives</span>
    <span class="c1">#### START CODE HERE ####</span>
    <span class="n">combined</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1">#### END CODE HERE ####</span>
    <span class="k">return</span> <span class="n">combined</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">combined</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]));</span>
<span class="c1"># Check exact order of elements</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">combined</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]))</span>
<span class="c1"># Tests that items are of float type</span>
<span class="k">assert</span> <span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">combined</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="o">==</span> <span class="nb">float</span><span class="p">)</span>
<span class="c1"># Check shapes</span>
<span class="n">combined</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">));</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">combined</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">combine_vectors</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">())</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org5c69a11">
<h3 id="org5c69a11">Training</h3>
<div class="outline-text-3" id="text-org5c69a11">
<p>First, you will define some new parameters:</p>
<ul class="org-ul">
<li>mnist<sub>shape</sub>: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28</li>
<li>n<sub>classes</sub>: the number of classes in MNIST (10, since there are the digits from 0 to 9)</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">mnist_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">n_classes</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
<p>And you also include the same parameters from before:</p>
<ul class="org-ul">
<li>criterion: the loss function</li>
<li>n<sub>epochs</sub>: the number of times you iterate through the entire dataset when training</li>
<li>z<sub>dim</sub>: the dimension of the noise vector</li>
<li>display<sub>step</sub>: how often to display/visualize the images</li>
<li>batch<sub>size</sub>: the number of images per forward/backward pass</li>
<li>lr: the learning rate</li>
<li>device: the device type</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0002</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">'cuda'</span>

<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,)),</span>
<span class="p">])</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/pytorch-data/MNIST/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">MNIST</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org963a20c">
<h4 id="org963a20c">Input Dimensions</h4>
<div class="outline-text-4" id="text-org963a20c">
<p>Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_input_dimensions</span><span class="p">(</span><span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mnist_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Calculates the size of the conditional input dimensions </span>

<span class="sd">    Args:</span>
<span class="sd">       z_dim: the dimension of the noise vector</span>
<span class="sd">       mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)</span>
<span class="sd">       n_classes: the total number of classes in the dataset, an integer scalar</span>
<span class="sd">               (10 for MNIST)</span>
<span class="sd">    Returns: </span>
<span class="sd">       generator_input_dim: the input dimensionality of the conditional generator, </span>
<span class="sd">                         which takes the noise and class vectors</span>
<span class="sd">       discriminator_im_chan: the number of input channels to the discriminator</span>
<span class="sd">                           (e.g. C x 28 x 28 for MNIST)</span>
<span class="sd">    """</span>
    <span class="c1">#### START CODE HERE ####</span>
    <span class="n">generator_input_dim</span> <span class="o">=</span> <span class="n">z_dim</span> <span class="o">+</span> <span class="n">n_classes</span>
    <span class="n">discriminator_im_chan</span> <span class="o">=</span> <span class="n">n_classes</span> <span class="o">+</span> <span class="n">mnist_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1">#### END CODE HERE ####</span>
    <span class="k">return</span> <span class="n">generator_input_dim</span><span class="p">,</span> <span class="n">discriminator_im_chan</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_input_dims</span><span class="p">():</span>
    <span class="n">gen_dim</span><span class="p">,</span> <span class="n">disc_dim</span> <span class="o">=</span> <span class="n">get_input_dimensions</span><span class="p">(</span><span class="mi">23</span><span class="p">,</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="mi">52</span><span class="p">),</span> <span class="mi">9</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">gen_dim</span> <span class="o">==</span> <span class="mi">32</span>
    <span class="k">assert</span> <span class="n">disc_dim</span> <span class="o">==</span> <span class="mi">21</span>
<span class="n">test_input_dims</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org41bf32f">
<h4 id="org41bf32f">Initialize the Objects</h4>
<div class="outline-text-4" id="text-org41bf32f">
<div class="highlight">
<pre><span></span><span class="n">generator_input_dim</span><span class="p">,</span> <span class="n">discriminator_im_chan</span> <span class="o">=</span> <span class="n">get_input_dimensions</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">mnist_shape</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">)</span>

<span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">generator_input_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">gen_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">(</span><span class="n">im_chan</span><span class="o">=</span><span class="n">discriminator_im_chan</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">disc_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">weights_init</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Initialize the weights from the normal distribution</span>

<span class="sd">    Args:</span>
<span class="sd">     m: object to initialize</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgec2e1a6">
<h4 id="orgec2e1a6">The Training</h4>
<div class="outline-text-4" id="text-orgec2e1a6">
<p>Now to train, you would like both your generator and your discriminator to know what class of image should be generated.</p>
<p>For example, if you're generating a picture of the number "1", you would need to:</p>
<ol class="org-ol">
<li>Tell that to the generator, so that it knows it should be generating a "1"</li>
<li>Tell that to the discriminator, so that it knows it should be looking at a "1". If the discriminator is told it should be looking at a 1 but sees something that's clearly an 8, it can guess that it's probably fake</li>
</ol>
<div class="highlight">
<pre><span></span><span class="n">cur_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">discriminator_losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">noise_and_labels</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">fake</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">fake_image_and_labels</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">real_image_and_labels</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">disc_fake_pred</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">disc_real_pred</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="c1"># Dataloader returns the batches and the labels</span>
        <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="c1"># Flatten the batch of real images from the dataset</span>
            <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">one_hot_labels</span> <span class="o">=</span> <span class="n">get_one_hot_labels</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">n_classes</span><span class="p">)</span>
            <span class="n">image_one_hot_labels</span> <span class="o">=</span> <span class="n">one_hot_labels</span><span class="p">[:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="n">image_one_hot_labels</span> <span class="o">=</span> <span class="n">image_one_hot_labels</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mnist_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">mnist_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

            <span class="c1">### Update discriminator ###</span>
            <span class="c1"># Zero out the discriminator gradients</span>
            <span class="n">disc_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="c1"># Get noise corresponding to the current batch_size </span>
            <span class="n">fake_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Now you can get the images from the generator</span>
            <span class="c1"># Steps: 1) Combine the noise vectors and the one-hot labels for the generator</span>
            <span class="c1">#        2) Generate the conditioned fake images</span>

            <span class="c1">#### START CODE HERE ####</span>
            <span class="n">noise_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">fake_noise</span><span class="p">,</span> <span class="n">one_hot_labels</span><span class="p">)</span>
            <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise_and_labels</span><span class="p">)</span>
            <span class="c1">#### END CODE HERE ####</span>

            <span class="c1"># Make sure that enough images were generated</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">fake</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="c1"># Check that correct tensors were combined</span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">noise_and_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">fake_noise</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">one_hot_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="c1"># It comes from the correct generator</span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>

            <span class="c1"># Now you can get the predictions from the discriminator</span>
            <span class="c1"># Steps: 1) Create the input for the discriminator</span>
            <span class="c1">#           a) Combine the fake images with image_one_hot_labels, </span>
            <span class="c1">#              remember to detach the generator (.detach()) so you do not backpropagate through it</span>
            <span class="c1">#           b) Combine the real images with image_one_hot_labels</span>
            <span class="c1">#        2) Get the discriminator's prediction on the fakes as disc_fake_pred</span>
            <span class="c1">#        3) Get the discriminator's prediction on the reals as disc_real_pred</span>

            <span class="c1">#### START CODE HERE ####</span>
            <span class="n">fake_image_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">image_one_hot_labels</span><span class="p">)</span>
            <span class="n">real_image_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">image_one_hot_labels</span><span class="p">)</span>
            <span class="n">disc_fake_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake_image_and_labels</span><span class="p">)</span>
            <span class="n">disc_real_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">real_image_and_labels</span><span class="p">)</span>
            <span class="c1">#### END CODE HERE ####</span>

            <span class="c1"># Make sure shapes are correct </span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fake_image_and_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">),</span> <span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">image_one_hot_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">28</span> <span class="p">,</span><span class="mi">28</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">real_image_and_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">),</span> <span class="n">real</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">image_one_hot_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">28</span> <span class="p">,</span><span class="mi">28</span><span class="p">)</span>
            <span class="c1"># Make sure that enough predictions were made</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="c1"># Make sure that the inputs are different</span>
            <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">fake_image_and_labels</span> <span class="o">!=</span> <span class="n">real_image_and_labels</span><span class="p">)</span>
            <span class="c1"># Shapes must match</span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">fake_image_and_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">real_image_and_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


            <span class="n">disc_fake_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">))</span>
            <span class="n">disc_real_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="p">))</span>
            <span class="n">disc_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">disc_fake_loss</span> <span class="o">+</span> <span class="n">disc_real_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
            <span class="n">disc_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">disc_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> 

            <span class="c1"># Keep track of the average discriminator loss</span>
            <span class="n">discriminator_losses</span> <span class="o">+=</span> <span class="p">[</span><span class="n">disc_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>

            <span class="c1">### Update generator ###</span>
            <span class="c1"># Zero out the generator gradients</span>
            <span class="n">gen_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

            <span class="n">fake_image_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">fake</span><span class="p">,</span> <span class="n">image_one_hot_labels</span><span class="p">)</span>
            <span class="c1"># This will error if you didn't concatenate your labels to your image correctly</span>
            <span class="n">disc_fake_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake_image_and_labels</span><span class="p">)</span>
            <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">))</span>
            <span class="n">gen_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">gen_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Keep track of the generator losses</span>
            <span class="n">generator_losses</span> <span class="o">+=</span> <span class="p">[</span><span class="n">gen_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
            <span class="c1">#</span>

            <span class="k">if</span> <span class="n">cur_step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">cur_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">gen_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">generator_losses</span><span class="p">[</span><span class="o">-</span><span class="n">display_step</span><span class="p">:])</span> <span class="o">/</span> <span class="n">display_step</span>
                <span class="n">disc_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">discriminator_losses</span><span class="p">[</span><span class="o">-</span><span class="n">display_step</span><span class="p">:])</span> <span class="o">/</span> <span class="n">display_step</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step </span><span class="si">{</span><span class="n">cur_step</span><span class="si">}</span><span class="s2">: Generator loss: </span><span class="si">{</span><span class="n">gen_mean</span><span class="si">}</span><span class="s2">, discriminator loss: </span><span class="si">{</span><span class="n">disc_mean</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                <span class="c1"># show_tensor_images(fake)</span>
                <span class="c1"># show_tensor_images(real)</span>
                <span class="n">step_bins</span> <span class="o">=</span> <span class="mi">20</span>
                <span class="n">x_axis</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">([</span><span class="n">i</span> <span class="o">*</span> <span class="n">step_bins</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">generator_losses</span><span class="p">)</span> <span class="o">//</span> <span class="n">step_bins</span><span class="p">)]</span> <span class="o">*</span> <span class="n">step_bins</span><span class="p">)</span>
                <span class="c1"># num_examples = (len(generator_losses) // step_bins) * step_bins</span>
                <span class="c1"># plt.plot(</span>
                <span class="c1">#     range(num_examples // step_bins), </span>
                <span class="c1">#     torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),</span>
                <span class="c1">#     label="Generator Loss"</span>
                <span class="c1"># )</span>
                <span class="c1"># plt.plot(</span>
                <span class="c1">#     range(num_examples // step_bins), </span>
                <span class="c1">#     torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),</span>
                <span class="c1">#     label="Discriminator Loss"</span>
                <span class="c1"># )</span>
                <span class="c1"># plt.legend()</span>
                <span class="c1"># plt.show()</span>
            <span class="k">elif</span> <span class="n">cur_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!"</span><span class="p">)</span>
            <span class="n">cur_step</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
<pre class="example">
Started: 2021-04-30 17:21:38.697176
Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!
Step 500: Generator loss: 2.2972163581848144, discriminator loss: 0.24098993314430117
Step 1000: Generator loss: 4.111798384666443, discriminator loss: 0.03910111421905458
Step 1500: Generator loss: 5.055200936317444, discriminator loss: 0.017988519712351263
Step 2000: Generator loss: 4.356978572845459, discriminator loss: 0.07956613468006253
Step 2500: Generator loss: 3.1875410358905794, discriminator loss: 0.1825093053430319
Step 3000: Generator loss: 2.7038124163150785, discriminator loss: 0.26903335136175155
Step 3500: Generator loss: 2.3201852326393126, discriminator loss: 0.30360834433138373
Step 4000: Generator loss: 2.3750923416614533, discriminator loss: 0.3380578280091286
Step 4500: Generator loss: 1.9010865423679353, discriminator loss: 0.3774027600288391
Step 5000: Generator loss: 1.9102082657814026, discriminator loss: 0.39759708976745606
Step 5500: Generator loss: 1.6987447377443314, discriminator loss: 0.43035421246290206
Step 6000: Generator loss: 1.6317225174903869, discriminator loss: 0.44889184486865996
Step 6500: Generator loss: 1.4883701887130738, discriminator loss: 0.47211092388629916
Step 7000: Generator loss: 1.4601867563724518, discriminator loss: 0.49546391534805295
Step 7500: Generator loss: 1.3467793072462082, discriminator loss: 0.520910717189312
Step 8000: Generator loss: 1.3130292971134185, discriminator loss: 0.5447795498371124
Step 8500: Generator loss: 1.1705783107280732, discriminator loss: 0.5716251953244209
Step 9000: Generator loss: 1.119933351278305, discriminator loss: 0.594505407333374
Step 9500: Generator loss: 1.0671374444961548, discriminator loss: 0.6109852703809738
Step 10000: Generator loss: 1.042488064646721, discriminator loss: 0.6221774860620498
Step 10500: Generator loss: 1.015932119846344, discriminator loss: 0.6356924445033073
Step 11000: Generator loss: 0.9900961836576462, discriminator loss: 0.6363092860579491
Step 11500: Generator loss: 0.9709519152641296, discriminator loss: 0.642409072637558
Step 12000: Generator loss: 0.9450125323534012, discriminator loss: 0.6527952731847763
Step 12500: Generator loss: 0.9660063650608063, discriminator loss: 0.6581431583166123
Step 13000: Generator loss: 0.8976931695938111, discriminator loss: 0.6624037518501281
Step 13500: Generator loss: 0.8969441390037537, discriminator loss: 0.6631241027116775
Step 14000: Generator loss: 0.902042475938797, discriminator loss: 0.6669842832088471
Step 14500: Generator loss: 0.8777725909948348, discriminator loss: 0.6742114140987396
Step 15000: Generator loss: 0.8425910116434098, discriminator loss: 0.6705276243686676
Step 15500: Generator loss: 0.8403865963220596, discriminator loss: 0.6768578908443451
Step 16000: Generator loss: 0.8556022493839264, discriminator loss: 0.6762306448221207
Step 16500: Generator loss: 0.8764977214336396, discriminator loss: 0.678820540189743
Step 17000: Generator loss: 0.8166215040683746, discriminator loss: 0.6793025200366973
Step 17500: Generator loss: 0.8172620774507523, discriminator loss: 0.6820640956163406
Step 18000: Generator loss: 0.8534817943572998, discriminator loss: 0.679261458158493
Step 18500: Generator loss: 0.814961371421814, discriminator loss: 0.6819399018287658
Step 19000: Generator loss: 0.8046633821725845, discriminator loss: 0.6841713408231735
Step 19500: Generator loss: 0.8040647978782653, discriminator loss: 0.6816601184606552
Step 20000: Generator loss: 0.8188033536672592, discriminator loss: 0.6830086879730225
Step 20500: Generator loss: 0.8088009203672409, discriminator loss: 0.6811848978996277
Step 21000: Generator loss: 0.7853847659826279, discriminator loss: 0.6836997301578521
Step 21500: Generator loss: 0.7798927721977233, discriminator loss: 0.684486163020134
Step 22000: Generator loss: 0.7833593552112579, discriminator loss: 0.685326477766037
Step 22500: Generator loss: 0.8140243920087814, discriminator loss: 0.6838948290348053
Step 23000: Generator loss: 0.7766883851289749, discriminator loss: 0.6896610424518586
Step 23500: Generator loss: 0.7677333387136459, discriminator loss: 0.6847020034790039
Step 24000: Generator loss: 0.7968860776424408, discriminator loss: 0.6859219465255737
Step 24500: Generator loss: 0.7655997285842896, discriminator loss: 0.6866924543380737
Step 25000: Generator loss: 0.7897603868246078, discriminator loss: 0.6862760508060455
Step 25500: Generator loss: 0.7602986326217651, discriminator loss: 0.6839702378511429
Step 26000: Generator loss: 0.7566630525588989, discriminator loss: 0.6884172073602677
Step 26500: Generator loss: 0.7693239089250564, discriminator loss: 0.6848342741727829
Step 27000: Generator loss: 0.7744117819070816, discriminator loss: 0.6884717727899552
Step 27500: Generator loss: 0.7754857275485992, discriminator loss: 0.6871565765142441
Step 28000: Generator loss: 0.7671164673566818, discriminator loss: 0.691150808095932
Step 28500: Generator loss: 0.7767693866491318, discriminator loss: 0.6899988718032837
Step 29000: Generator loss: 0.7584288560152054, discriminator loss: 0.6866833527088165
Step 29500: Generator loss: 0.7469037870168685, discriminator loss: 0.6892017160654068
Step 30000: Generator loss: 0.7409272351264954, discriminator loss: 0.6925988558530808
Step 30500: Generator loss: 0.7461127021312713, discriminator loss: 0.6910134963989257
Step 31000: Generator loss: 0.7623333480358124, discriminator loss: 0.6848250635862351
Step 31500: Generator loss: 0.7320846046209335, discriminator loss: 0.6922971439361573
Step 32000: Generator loss: 0.7360488106012344, discriminator loss: 0.6958958665132523
Step 32500: Generator loss: 0.7436227219104767, discriminator loss: 0.6927914987802506
Step 33000: Generator loss: 0.7528411923646927, discriminator loss: 0.6868532946109772
Step 33500: Generator loss: 0.7555540499687194, discriminator loss: 0.6819704930782318
Step 34000: Generator loss: 0.7339509303569793, discriminator loss: 0.6947230596542359
Step 34500: Generator loss: 0.7203902735710144, discriminator loss: 0.694019063949585
Step 35000: Generator loss: 0.7161798032522202, discriminator loss: 0.694249948143959
Step 35500: Generator loss: 0.7100930047035218, discriminator loss: 0.6956643009185791
Step 36000: Generator loss: 0.7224245357513428, discriminator loss: 0.692036272764206
Step 36500: Generator loss: 0.7294702612161637, discriminator loss: 0.6839023213386536
Step 37000: Generator loss: 0.7326101566553116, discriminator loss: 0.6864855628013611
Step 37500: Generator loss: 0.7289662526845933, discriminator loss: 0.6891102294921875
Step 38000: Generator loss: 0.7277824294567108, discriminator loss: 0.6930312074422836
Step 38500: Generator loss: 0.7523093600273132, discriminator loss: 0.6822635132074356
Step 39000: Generator loss: 0.7260702294111252, discriminator loss: 0.6836128298044205
Step 39500: Generator loss: 0.7210463825464248, discriminator loss: 0.6865772886276245
Step 40000: Generator loss: 0.7197876414060592, discriminator loss: 0.6861994673013687
Step 40500: Generator loss: 0.7156198496818542, discriminator loss: 0.6897815141677857
Step 41000: Generator loss: 0.7411812788248062, discriminator loss: 0.6876297281980515
Step 41500: Generator loss: 0.7482703533172608, discriminator loss: 0.6831764079332352
Step 42000: Generator loss: 0.7353900390863418, discriminator loss: 0.6809069069623948
Step 42500: Generator loss: 0.726880151629448, discriminator loss: 0.6845077587366104
Step 43000: Generator loss: 0.7335763674974441, discriminator loss: 0.6855163406133652
Step 43500: Generator loss: 0.7247586588859558, discriminator loss: 0.684886796593666
Step 44000: Generator loss: 0.7244187197685241, discriminator loss: 0.6869175283908844
Step 44500: Generator loss: 0.7478935513496399, discriminator loss: 0.6783332238197327
Step 45000: Generator loss: 0.7392684471607208, discriminator loss: 0.687694214463234
Step 45500: Generator loss: 0.7384519840478897, discriminator loss: 0.6806207147836685
Step 46000: Generator loss: 0.7173152709007263, discriminator loss: 0.6894198944568634
Step 46500: Generator loss: 0.7135227386951446, discriminator loss: 0.6902039344310761
Step 47000: Generator loss: 0.7121314022541047, discriminator loss: 0.691226885676384
Step 47500: Generator loss: 0.7153779380321502, discriminator loss: 0.6898772416114807
Step 48000: Generator loss: 0.7112214748859406, discriminator loss: 0.6919035356044769
Step 48500: Generator loss: 0.729472970366478, discriminator loss: 0.6832324341535568
Step 49000: Generator loss: 0.7259864670038223, discriminator loss: 0.6850444099903107
Step 49500: Generator loss: 0.7463545156717301, discriminator loss: 0.6876692290306091
Step 50000: Generator loss: 0.72439306807518, discriminator loss: 0.6840117316246033
Step 50500: Generator loss: 0.7304026707410812, discriminator loss: 0.6828315691947937
Step 51000: Generator loss: 0.735065841794014, discriminator loss: 0.6877049984931946
Step 51500: Generator loss: 0.738693750500679, discriminator loss: 0.6786749280691147
Step 52000: Generator loss: 0.7165734323263169, discriminator loss: 0.688656357049942
Step 52500: Generator loss: 0.7124545810222626, discriminator loss: 0.6885210503339767
Step 53000: Generator loss: 0.7169003388881683, discriminator loss: 0.6898472727537155
Step 53500: Generator loss: 0.7116240389347076, discriminator loss: 0.6890990349054337
Step 54000: Generator loss: 0.7254890002012253, discriminator loss: 0.686066904425621
Step 54500: Generator loss: 0.7279696422815323, discriminator loss: 0.6824959990978241
Step 55000: Generator loss: 0.7243433123826981, discriminator loss: 0.686788556933403
Step 55500: Generator loss: 0.72320248234272, discriminator loss: 0.6819899456501007
Step 56000: Generator loss: 0.7283236463069915, discriminator loss: 0.6813042680025101
Step 56500: Generator loss: 0.7257692145109177, discriminator loss: 0.6882435537576675
Step 57000: Generator loss: 0.7204343225955964, discriminator loss: 0.6905163298845292
Step 57500: Generator loss: 0.7234136379957199, discriminator loss: 0.6828762836456299
Step 58000: Generator loss: 0.7213340125083924, discriminator loss: 0.6852367097139358
Step 58500: Generator loss: 0.7139561972618103, discriminator loss: 0.6901394550800324
Step 59000: Generator loss: 0.7128681792020798, discriminator loss: 0.6899428930282593
Step 59500: Generator loss: 0.7178032584190369, discriminator loss: 0.6901476013660431
Step 60000: Generator loss: 0.7218955677747726, discriminator loss: 0.6866856569051742
Step 60500: Generator loss: 0.7173091459274292, discriminator loss: 0.6909447896480561
Step 61000: Generator loss: 0.7196292532682419, discriminator loss: 0.6888659211397171
Step 61500: Generator loss: 0.7136147793531418, discriminator loss: 0.6911007264852523
Step 62000: Generator loss: 0.7167167031764984, discriminator loss: 0.6874131036996841
Step 62500: Generator loss: 0.7095696296691895, discriminator loss: 0.6924118340015412
Step 63000: Generator loss: 0.7100733149051667, discriminator loss: 0.6894952065944672
Step 63500: Generator loss: 0.7075963083505631, discriminator loss: 0.6918715183734894
Step 64000: Generator loss: 0.7087407541275025, discriminator loss: 0.6912821785211564
Step 64500: Generator loss: 0.7044790136814117, discriminator loss: 0.6919414196014404
Step 65000: Generator loss: 0.7120586842298507, discriminator loss: 0.6889722956418991
Step 65500: Generator loss: 0.7059948451519013, discriminator loss: 0.6913756219148636
Step 66000: Generator loss: 0.7103360829353332, discriminator loss: 0.6888430647850037
Step 66500: Generator loss: 0.7106574136018753, discriminator loss: 0.6923392252922058
Step 67000: Generator loss: 0.7205636972188949, discriminator loss: 0.6888139424324036
Step 67500: Generator loss: 0.7325763144493103, discriminator loss: 0.6851953419446946
Step 68000: Generator loss: 0.7144211075305938, discriminator loss: 0.6894719363451004
Step 68500: Generator loss: 0.7039347168207168, discriminator loss: 0.692310958981514
Step 69000: Generator loss: 0.707789731502533, discriminator loss: 0.690034374833107
Step 69500: Generator loss: 0.7080022550821304, discriminator loss: 0.6897176603078842
Step 70000: Generator loss: 0.706935028553009, discriminator loss: 0.6917025876045227
Step 70500: Generator loss: 0.7035844438076019, discriminator loss: 0.6928271135091781
Step 71000: Generator loss: 0.706664494395256, discriminator loss: 0.6913493415117263
Step 71500: Generator loss: 0.7080443944931031, discriminator loss: 0.6943384435176849
Step 72000: Generator loss: 0.7080535914897919, discriminator loss: 0.6904549078941346
Step 72500: Generator loss: 0.7195642621517181, discriminator loss: 0.6883307158946991
Step 73000: Generator loss: 0.7137477462291717, discriminator loss: 0.6895240060091019
Step 73500: Generator loss: 0.7089026942253113, discriminator loss: 0.6893982688188552
Step 74000: Generator loss: 0.71370064163208, discriminator loss: 0.6885940716266632
Step 74500: Generator loss: 0.7126090573072433, discriminator loss: 0.6913927717208862
Step 75000: Generator loss: 0.7061277792453766, discriminator loss: 0.6915859417915344
Step 75500: Generator loss: 0.7079737706184387, discriminator loss: 0.6918540188074112
Step 76000: Generator loss: 0.7094860315322876, discriminator loss: 0.6909938471317292
Step 76500: Generator loss: 0.7089288998842239, discriminator loss: 0.6928894543647766
Step 77000: Generator loss: 0.7099210443496704, discriminator loss: 0.6881472972631455
Step 77500: Generator loss: 0.7087316303253174, discriminator loss: 0.6922812685966492
Step 78000: Generator loss: 0.7124276860952378, discriminator loss: 0.686549712896347
Step 78500: Generator loss: 0.7118150774240494, discriminator loss: 0.6914097841978073
Step 79000: Generator loss: 0.7061567052602767, discriminator loss: 0.6910830926895142
Step 79500: Generator loss: 0.7130619381666183, discriminator loss: 0.6901648813486099
Step 80000: Generator loss: 0.7189263315200806, discriminator loss: 0.6891602661609649
Step 80500: Generator loss: 0.7099695562124252, discriminator loss: 0.6893361113071441
Step 81000: Generator loss: 0.7043007851839066, discriminator loss: 0.6928421225547791
Step 81500: Generator loss: 0.7055111042261124, discriminator loss: 0.6913482803106308
Step 82000: Generator loss: 0.7107034167051315, discriminator loss: 0.6899493371248245
Step 82500: Generator loss: 0.7072652250528335, discriminator loss: 0.691617219209671
Step 83000: Generator loss: 0.7116999027729034, discriminator loss: 0.6887015362977982
Step 83500: Generator loss: 0.7103397004604339, discriminator loss: 0.6889865008592606
Step 84000: Generator loss: 0.702219740986824, discriminator loss: 0.6927198125123978
Step 84500: Generator loss: 0.7042218887805939, discriminator loss: 0.6910840107202529
Step 85000: Generator loss: 0.7036903632879257, discriminator loss: 0.6948130846023559
Step 85500: Generator loss: 0.7157929112911224, discriminator loss: 0.6877820398807526
Step 86000: Generator loss: 0.703074496269226, discriminator loss: 0.6928336225748062
Step 86500: Generator loss: 0.7018165578842163, discriminator loss: 0.6942198793888092
Step 87000: Generator loss: 0.7056693414449692, discriminator loss: 0.6903062870502472
Step 87500: Generator loss: 0.7070764343738556, discriminator loss: 0.690039452791214
Step 88000: Generator loss: 0.7018579070568085, discriminator loss: 0.6929991252422333
Step 88500: Generator loss: 0.7042791714668274, discriminator loss: 0.6906855113506317
Step 89000: Generator loss: 0.7052908551692962, discriminator loss: 0.6917922974824905
Step 89500: Generator loss: 0.7057228873968124, discriminator loss: 0.6917544984817505
Step 90000: Generator loss: 0.7041428442001343, discriminator loss: 0.6921311345100403
Step 90500: Generator loss: 0.7040341221094132, discriminator loss: 0.6916886166334152
Step 91000: Generator loss: 0.7016080802679062, discriminator loss: 0.6918226274251937
Step 91500: Generator loss: 0.7047490992546082, discriminator loss: 0.6919238156080246
Step 92000: Generator loss: 0.7015802135467529, discriminator loss: 0.6937261604070664
Step 92500: Generator loss: 0.7035572265386582, discriminator loss: 0.6899326649904252
Step 93000: Generator loss: 0.7005916707515717, discriminator loss: 0.6933788905143737
Step 93500: Generator loss: 0.7005794968605041, discriminator loss: 0.6932051202058792
Ended: 2021-04-30 18:15:27.636306
Elapsed: 0:53:48.939130
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org5ccbefe">
<h3 id="org5ccbefe">Exploration</h3>
<div class="outline-text-3" id="text-org5ccbefe">
<p>Before you explore, you should put the generator in eval mode, both in general and so that batch norm doesn't cause you issues and is using its eval statistics.</p>
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga274e2c">
<h4 id="orga274e2c">Changing the Class Vector</h4>
<div class="outline-text-4" id="text-orga274e2c">
<p>You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.</p>
<p>So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your're basically morphing one image into another. You can choose what these two images will be using your conditional GAN.</p>
<p>### Change me! ###</p>
<div class="highlight">
<pre><span></span><span class="n">n_interpolation</span> <span class="o">=</span> <span class="mi">9</span> <span class="c1"># Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)</span>
<span class="n">interpolation_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_interpolation</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">interpolate_class</span><span class="p">(</span><span class="n">first_number</span><span class="p">,</span> <span class="n">second_number</span><span class="p">):</span>
    <span class="n">first_label</span> <span class="o">=</span> <span class="n">get_one_hot_labels</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">first_number</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">n_classes</span><span class="p">)</span>
    <span class="n">second_label</span> <span class="o">=</span> <span class="n">get_one_hot_labels</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="n">second_number</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">n_classes</span><span class="p">)</span>

    <span class="c1"># Calculate the interpolation vector between the two labels</span>
    <span class="n">percent_second_label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_interpolation</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">interpolation_labels</span> <span class="o">=</span> <span class="n">first_label</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">percent_second_label</span><span class="p">)</span> <span class="o">+</span> <span class="n">second_label</span> <span class="o">*</span> <span class="n">percent_second_label</span>

    <span class="c1"># Combine the noise and the labels</span>
    <span class="n">noise_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">interpolation_noise</span><span class="p">,</span> <span class="n">interpolation_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise_and_labels</span><span class="p">)</span>
    <span class="n">show_tensor_images</span><span class="p">(</span><span class="n">fake</span><span class="p">,</span> <span class="n">num_images</span><span class="o">=</span><span class="n">n_interpolation</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_interpolation</span><span class="p">)),</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="c1">### Change me! ###</span>
<span class="n">start_plot_number</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Choose the start digit</span>
<span class="c1">### Change me! ###</span>
<span class="n">end_plot_number</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># Choose the end digit</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">interpolate_class</span><span class="p">(</span><span class="n">start_plot_number</span><span class="p">,</span> <span class="n">end_plot_number</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>

<span class="c1">### Uncomment the following lines of code if you would like to visualize a set of pairwise class </span>
<span class="c1">### interpolations for a collection of different numbers, all in a single grid of interpolations.</span>
<span class="c1">### You'll also see another visualization like this in the next code block!</span>
<span class="c1"># plot_numbers = [2, 3, 4, 5, 7]</span>
<span class="c1"># n_numbers = len(plot_numbers)</span>
<span class="c1"># plt.figure(figsize=(8, 8))</span>
<span class="c1"># for i, first_plot_number in enumerate(plot_numbers):</span>
<span class="c1">#     for j, second_plot_number in enumerate(plot_numbers):</span>
<span class="c1">#         plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)</span>
<span class="c1">#         interpolate_class(first_plot_number, second_plot_number)</span>
<span class="c1">#         plt.axis('off')</span>
<span class="c1"># plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)</span>
<span class="c1"># plt.show()</span>
<span class="c1"># plt.close()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge204fb9">
<h4 id="orge204fb9">Changing the Noise Vector</h4>
<div class="outline-text-4" id="text-orge204fb9">
<p>Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step.</p>
<div class="highlight">
<pre><span></span><span class="n">n_interpolation</span> <span class="o">=</span> <span class="mi">9</span> <span class="c1"># How many intermediate images you want + 2 (for the start and end image)</span>
</pre></div>
<p>This time you're interpolating between the noise instead of the labels</p>
<div class="highlight">
<pre><span></span><span class="n">interpolation_label</span> <span class="o">=</span> <span class="n">get_one_hot_labels</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">5</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">n_classes</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_interpolation</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">interpolate_noise</span><span class="p">(</span><span class="n">first_noise</span><span class="p">,</span> <span class="n">second_noise</span><span class="p">):</span>
    <span class="c1"># This time you're interpolating between the noise instead of the labels</span>
    <span class="n">percent_first_noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_interpolation</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">interpolation_noise</span> <span class="o">=</span> <span class="n">first_noise</span> <span class="o">*</span> <span class="n">percent_first_noise</span> <span class="o">+</span> <span class="n">second_noise</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">percent_first_noise</span><span class="p">)</span>

    <span class="c1"># Combine the noise and the labels again</span>
    <span class="n">noise_and_labels</span> <span class="o">=</span> <span class="n">combine_vectors</span><span class="p">(</span><span class="n">interpolation_noise</span><span class="p">,</span> <span class="n">interpolation_label</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">))</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise_and_labels</span><span class="p">)</span>
    <span class="n">show_tensor_images</span><span class="p">(</span><span class="n">fake</span><span class="p">,</span> <span class="n">num_images</span><span class="o">=</span><span class="n">n_interpolation</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n_interpolation</span><span class="p">)),</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
<p>Generate noise vectors to interpolate between.</p>
<div class="highlight">
<pre><span></span><span class="c1">### Change me! ###</span>
<span class="n">n_noise</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># Choose the number of noise examples in the grid</span>
<span class="n">plot_noises</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_noise</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_noise</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">first_plot_noise</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">plot_noises</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">second_plot_noise</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">plot_noises</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">n_noise</span><span class="p">,</span> <span class="n">n_noise</span><span class="p">,</span> <span class="n">i</span> <span class="o">*</span> <span class="n">n_noise</span> <span class="o">+</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">interpolate_noise</span><span class="p">(</span><span class="n">first_plot_noise</span><span class="p">,</span> <span class="n">second_plot_noise</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">top</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org202a7e6">
<h2 id="org202a7e6">End</h2>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/wasserstein-gan-with-gradient-penalty/">Wasserstein GAN With Gradient Penalty</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/wasserstein-gan-with-gradient-penalty/" rel="bookmark"><time class="published dt-published" datetime="2021-04-21T13:24:27-07:00" itemprop="datePublished" title="2021-04-21 13:24">2021-04-21 13:24</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div class="outline-2" id="outline-container-org3ae7c7e">
<h2 id="org3ae7c7e">A Wasserstein GAN with Gradient Penalty (WGAN-GP)</h2>
<div class="outline-text-2" id="text-org3ae7c7e">
<p>We're going to build a Wasserstein GAN with Gradient Penalty (WGAN-GP) that solves some of the stability issues with GANs. Specifically, we'll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse (see <a href="https://www.wikiwand.com/en/Wasserstein_metric">Wasserstein Metric</a>).</p>
<p>Wasserstein is named after a mathematician at Penn State, <a href="https://www.wikiwand.com/en/Leonid_Vaserstein">Leonid VaserÅ¡teÄ­n</a>.</p>
</div>
<div class="outline-3" id="outline-container-orge8d96c0">
<h3 id="orge8d96c0">Imports</h3>
<div class="outline-text-3" id="text-orge8d96c0">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org59377dd">
<h3 id="org59377dd">Set Up</h3>
<div class="outline-text-3" id="text-org59377dd"></div>
<div class="outline-4" id="outline-container-org4a5ee01">
<h4 id="org4a5ee01">The Random Seed</h4>
<div class="outline-text-4" id="text-org4a5ee01">
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd7d1cdb">
<h4 id="orgd7d1cdb">Plotting and the Timer</h4>
<div class="outline-text-4" id="text-orgd7d1cdb">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
<span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"wasserstein-gan-with-gradient-penalty"</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-orgdaa8adb">
<h3 id="orgdaa8adb">Helper Functions</h3>
<div class="outline-text-3" id="text-orgdaa8adb">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">save_tensor_images</span><span class="p">(</span><span class="n">image_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                       <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                       <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                       <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">,</span>
                       <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)):</span>
    <span class="sd">"""Plot an Image Tensor</span>

<span class="sd">    Args:</span>
<span class="sd">     image_tensor: tensor with the values for the image to plot</span>
<span class="sd">     filename: name to save the file under</span>
<span class="sd">     folder: path to put the file in</span>
<span class="sd">     title: title for the image</span>
<span class="sd">     num_images: how many images from the tensor to use</span>
<span class="sd">     size: the dimensions for each image</span>
<span class="sd">    """</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">image_unflat</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">image_unflat</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">folder</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[[file:</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">]]"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">holoviews_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">Image</span><span class="p">:</span>
    <span class="n">image_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">image_tensor</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
    <span class="n">image_unflat</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">image_unflat</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">holoview</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">image_grid</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgaf5cf14">
<h4 id="orgaf5cf14">Gradient Hook</h4>
<div class="outline-text-4" id="text-orgaf5cf14">
<p>This helps to keep track of the gradient for plotting</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_grad_hook</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Function to keep track of gradients for visualization purposes, </span>
<span class="sd">    which fills the grads list when using model.apply(grad_hook).</span>
<span class="sd">    """</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">def</span> <span class="nf">grad_hook</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
            <span class="n">grads</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">grad_hook</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd2ccb1e">
<h4 id="orgd2ccb1e">Noise</h4>
<div class="outline-text-4" id="text-orgd2ccb1e">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_noise</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Alias for torch.randn</span>

<span class="sd">    Args:</span>
<span class="sd">      n_samples: the number of samples to generate</span>
<span class="sd">      z_dim: the dimension of the noise vector</span>
<span class="sd">      device: the device type</span>

<span class="sd">    Returns:</span>
<span class="sd">     tensor with random numbers from the normal distribution.</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcc909b9">
<h2 id="orgcc909b9">Middle</h2>
<div class="outline-text-2" id="text-orgcc909b9"></div>
<div class="outline-3" id="outline-container-org26a5e96">
<h3 id="org26a5e96">The Generator</h3>
<div class="outline-text-3" id="text-org26a5e96">
<p>This is the Deep Convolutional GAN from before.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The DCGAN Generator</span>

<span class="sd">    Args:</span>
<span class="sd">       input_dim: the dimension of the input vector</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is your default)</span>
<span class="sd">       hidden_dim: the inner dimension,</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gen</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_gen_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                       <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Creates a block for the generator (sub sequence)</span>

<span class="sd">       The parts</span>
<span class="sd">        - a transposed convolution</span>
<span class="sd">        - a batchnorm (except for in the last layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>

<span class="sd">       Returns:</span>
<span class="sd">        the sub-sequence of layers</span>
<span class="sd">       """</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">(),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""complete a forward pass of the generator: Given a noise tensor, </span>

<span class="sd">       Args:</span>
<span class="sd">        noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        generated images.</span>
<span class="sd">       """</span>
        <span class="c1"># unsqueeze the noise</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">noise</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">noise</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge0c8fef">
<h3 id="orge0c8fef">The Critic</h3>
<div class="outline-text-3" id="text-orge0c8fef">
<p>This is also essentially the same as our Discriminator class from before.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Critic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Critic Class</span>

<span class="sd">    Args:</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is your default)</span>
<span class="sd">       hidden_dim: the inner dimension</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">crit</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_crit_block</span><span class="p">(</span><span class="n">im_chan</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_crit_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_crit_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_crit_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Creates a sub-block for the network</span>

<span class="sd">        - a convolution</span>
<span class="sd">        - a batchnorm (except in the final layer)</span>
<span class="sd">        - an activation (except in the final layer).</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>
<span class="sd">       """</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                          <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                          <span class="n">stride</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Run a forward pass of the critic</span>

<span class="sd">       Args:</span>
<span class="sd">           image: a flattened image tensor with dimension (im_chan)</span>

<span class="sd">       Returns:</span>
<span class="sd">        a 1-dimension tensor representing fake/real.</span>
<span class="sd">       """</span>
        <span class="n">crit_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">crit</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">crit_pred</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">crit_pred</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orge7e77b8">
<h3 id="orge7e77b8">Training</h3>
<div class="outline-text-3" id="text-orge7e77b8"></div>
<div class="outline-4" id="outline-container-org2a0fa55">
<h4 id="org2a0fa55">Hyperparameters</h4>
<div class="outline-text-4" id="text-org2a0fa55">
<p>As usual, we'll start by setting the parameters:</p>
<ul class="org-ul">
<li>n<sub>epochs</sub>: the number of times you iterate through the entire dataset when training</li>
<li>z<sub>dim</sub>: the dimension of the noise vector</li>
<li>display<sub>step</sub>: how often to display/visualize the images</li>
<li>batch<sub>size</sub>: the number of images per forward/backward pass</li>
<li>lr: the learning rate</li>
<li>beta<sub>1</sub>, beta<sub>2</sub>: the momentum terms</li>
<li>c<sub>lambda</sub>: weight of the gradient penalty</li>
<li>crit<sub>repeats</sub>: number of times to update the critic per generator update - there are more details about this in the <b>Putting It All Together</b> section</li>
<li>device: the device type</li>
</ul>
<div class="highlight">
<pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0002</span>
<span class="n">beta_1</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">beta_2</span> <span class="o">=</span> <span class="mf">0.999</span>
<span class="n">c_lambda</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">crit_repeats</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">'cuda'</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgf415a89">
<h4 id="orgf415a89">The Data</h4>
<div class="outline-text-4" id="text-orgf415a89">
<p>Once again we'll be using the MNIST dataset.</p>
<div class="highlight">
<pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,)),</span>
<span class="p">])</span>

<span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/pytorch-data/MNIST/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">MNIST</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org2399867">
<h4 id="org2399867">Setup For Training</h4>
<div class="outline-text-4" id="text-org2399867">
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">gen_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">))</span>
<span class="n">crit</span> <span class="o">=</span> <span class="n">Critic</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
<span class="n">crit_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">crit</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">weights_init</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>
<span class="n">crit</span> <span class="o">=</span> <span class="n">crit</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">weights_init</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org782ca9e">
<h4 id="org782ca9e">The Gradient</h4>
<div class="outline-text-4" id="text-org782ca9e">
<p>Calculating the gradient penalty can be broken into two functions: (1) compute the gradient with respect to the images and (2) compute the gradient penalty given the gradient.</p>
<p>You can start by getting the gradient. The gradient is computed by first creating a mixed image. This is done by weighing the fake and real image using epsilon and then adding them together. Once you have the intermediate image, you can get the critic's output on the image. Finally, you compute the gradient of the critic score's on the mixed images (output) with respect to the pixels of the mixed images (input).</p>
<ul class="org-ul">
<li>See <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad">pytorch's autograd documentation</a></li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_gradient</span><span class="p">(</span><span class="n">crit</span><span class="p">:</span> <span class="n">Critic</span><span class="p">,</span> <span class="n">real</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">fake</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
    <span class="sd">"""Gradient of the critic's scores with respect to mixes of real and fake images.</span>

<span class="sd">    Args:</span>
<span class="sd">       crit: the critic model</span>
<span class="sd">       real: a batch of real images</span>
<span class="sd">       fake: a batch of fake images</span>
<span class="sd">       epsilon: a vector of the uniformly random proportions of real/fake per mixed image</span>

<span class="sd">    Returns:</span>
<span class="sd">       gradient: the gradient of the critic's scores, with respect to the mixed image</span>
<span class="sd">    """</span>
    <span class="c1"># Mix the images together</span>
    <span class="n">mixed_images</span> <span class="o">=</span> <span class="n">real</span> <span class="o">*</span> <span class="n">epsilon</span> <span class="o">+</span> <span class="n">fake</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">epsilon</span><span class="p">)</span>

    <span class="c1"># Calculate the critic's scores on the mixed images</span>
    <span class="n">mixed_scores</span> <span class="o">=</span> <span class="n">crit</span><span class="p">(</span><span class="n">mixed_images</span><span class="p">)</span>

    <span class="c1"># Take the gradient of the scores with respect to the images</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span>
        <span class="c1"># Note: You need to take the gradient of outputs with respect to inputs.</span>
        <span class="c1">#### START CODE HERE ####</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">mixed_images</span><span class="p">,</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">mixed_scores</span><span class="p">,</span>
        <span class="c1">#### END CODE HERE ####</span>
        <span class="c1"># These other parameters have to do with how the pytorch autograd engine works</span>
        <span class="n">grad_outputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mixed_scores</span><span class="p">),</span> 
        <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">gradient</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="orge064b13"></a>Unit Tests<br>
<div class="outline-text-5" id="text-orge064b13">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_get_gradient</span><span class="p">(</span><span class="n">image_shape</span><span class="p">):</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">image_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">image_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="n">epsilon_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">image_shape</span><span class="p">]</span>
    <span class="n">epsilon_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">epsilon_shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">get_gradient</span><span class="p">(</span><span class="n">crit</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">fake</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">gradient</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="n">image_shape</span>
    <span class="k">assert</span> <span class="n">gradient</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span>
    <span class="k">assert</span> <span class="n">gradient</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">gradient</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">gradient</span> <span class="o">=</span> <span class="n">test_get_gradient</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org7e9a292">
<h4 id="org7e9a292">The Gradient Penalty</h4>
<div class="outline-text-4" id="text-org7e9a292">
<p>The second function you need to complete is to compute the gradient penalty given the gradient. First, you calculate the magnitude of each image's gradient. The magnitude of a gradient is also called the norm. Then, you calculate the penalty by squaring the distance between each magnitude and the ideal norm of 1 and taking the mean of all the squared distances.</p>
<ol class="org-ol">
<li>Make sure you take the mean at the end.</li>
<li>Note that the magnitude of each gradient has already been calculated for you.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradient_penalty</span><span class="p">(</span><span class="n">gradient</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Calculate the size of each image's gradient</span>
<span class="sd">    and penalize the mean quadratic distance of each magnitude to 1.</span>

<span class="sd">    Args:</span>
<span class="sd">       gradient: the gradient of the critic's scores, with respect to the mixed image</span>

<span class="sd">    Returns:</span>
<span class="sd">       penalty: the gradient penalty</span>
<span class="sd">    """</span>
    <span class="c1"># Flatten the gradients so that each row captures one image</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">gradient</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Calculate the magnitude of every row</span>
    <span class="n">gradient_norm</span> <span class="o">=</span> <span class="n">gradient</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Penalize the mean squared distance of the gradient norms from 1</span>
    <span class="n">penalty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">gradient_norm</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">penalty</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org5512424"></a>Unit Testing<br>
<div class="outline-text-5" id="text-org5512424">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_gradient_penalty</span><span class="p">(</span><span class="n">image_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="n">bad_gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="o">*</span><span class="n">image_shape</span><span class="p">)</span>
    <span class="n">bad_gradient_penalty</span> <span class="o">=</span> <span class="n">gradient_penalty</span><span class="p">(</span><span class="n">bad_gradient</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">bad_gradient_penalty</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">))</span>

    <span class="n">image_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
    <span class="n">good_gradient</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="o">*</span><span class="n">image_shape</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">image_size</span><span class="p">)</span>
    <span class="n">good_gradient_penalty</span> <span class="o">=</span> <span class="n">gradient_penalty</span><span class="p">(</span><span class="n">good_gradient</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">good_gradient_penalty</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.</span><span class="p">))</span>

    <span class="n">random_gradient</span> <span class="o">=</span> <span class="n">test_get_gradient</span><span class="p">(</span><span class="n">image_shape</span><span class="p">)</span>
    <span class="n">random_gradient_penalty</span> <span class="o">=</span> <span class="n">gradient_penalty</span><span class="p">(</span><span class="n">random_gradient</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">random_gradient_penalty</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.1</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_gradient_penalty</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">))</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org6fd6950">
<h4 id="org6fd6950">Losses</h4>
<div class="outline-text-4" id="text-org6fd6950">
<p>Next, you need to calculate the loss for the generator and the critic.</p>
</div>
<ul class="org-ul">
<li><a id="orgc7eb797"></a>Generator Loss<br>
<div class="outline-text-5" id="text-orgc7eb797">
<p>For the generator, the loss is calculated by maximizing the critic's prediction on the generator's fake images. The argument has the scores for all fake images in the batch, but you will use the mean of them.</p>
<ol class="org-ol">
<li>This can be written in one line.</li>
<li>This is the negative of the mean of the critic's scores.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_gen_loss</span><span class="p">(</span><span class="n">crit_fake_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""loss of generator given critic's scores of generator's fake images.</span>

<span class="sd">    Args:</span>
<span class="sd">       crit_fake_pred: the critic's scores of the fake images</span>

<span class="sd">    Returns:</span>
<span class="sd">       gen_loss: a scalar loss value for the current batch of the generator</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">crit_fake_pred</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span>
    <span class="n">get_gen_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span>
    <span class="n">get_gen_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10000</span><span class="p">)),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="mf">0.05</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><a id="org650f1e8"></a>The Critic Loss<br>
<div class="outline-text-5" id="text-org650f1e8">
<p>For the critic, the loss is calculated by maximizing the distance between the critic's predictions on the real images and the predictions on the fake images while also adding a gradient penalty. The gradient penalty is weighed according to lambda. The arguments are the scores for all the images in the batch, and you will use the mean of them.</p>
<ol class="org-ol">
<li>The higher the mean fake score, the higher the critic's loss is.</li>
<li>What does this suggest about the mean real score?</li>
<li>The higher the gradient penalty, the higher the critic's loss is, proportional to lambda.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_crit_loss</span><span class="p">(</span><span class="n">crit_fake_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">crit_real_pred</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">gp</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">c_lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""loss of a critic given critic's scores for fake and real images,</span>
<span class="sd">    the gradient penalty, and gradient penalty weight.</span>

<span class="sd">    Args:</span>
<span class="sd">       crit_fake_pred: the critic's scores of the fake images</span>
<span class="sd">       crit_real_pred: the critic's scores of the real images</span>
<span class="sd">       gp: the unweighted gradient penalty</span>
<span class="sd">       c_lambda: the current weight of the gradient penalty </span>

<span class="sd">    Returns:</span>
<span class="sd">       crit_loss: a scalar for the critic's loss, accounting for the relevant factors</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">crit_fake_pred</span> <span class="o">-</span> <span class="n">crit_real_pred</span>  <span class="o">+</span> <span class="n">gp</span> <span class="o">*</span> <span class="n">c_lambda</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span>
    <span class="n">get_crit_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">),</span> <span class="mf">0.1</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">0.7</span><span class="p">)</span>
<span class="p">)</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span>
    <span class="n">get_crit_loss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">20.</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">20.</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">),</span> <span class="mi">10</span><span class="p">),</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">60.</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org5461727">
<h4 id="org5461727">Running the Training</h4>
<div class="outline-text-4" id="text-org5461727">
<p>Before you put everything together, there are a few things to note.</p>
<ol class="org-ol">
<li>Even on GPU, the <b>training will run more slowly</b> than previous labs because the gradient penalty requires you to compute the gradient of a gradient â€“ this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.</li>
<li>One important difference from earlier versions is that you will <b>update the critic multiple times</b> every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you're using).</li>
<li>WGAN-GP isn't necessarily meant to improve overall performance of a GAN, but just <b>increases stability</b> and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.</li>
</ol>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_critic</span><span class="p">(</span><span class="n">critic</span><span class="p">,</span> <span class="n">critic_optimizer</span><span class="p">,</span> <span class="n">generator</span><span class="p">,</span> <span class="n">generator_optimizer</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">real</span><span class="p">):</span>
    <span class="n">critic_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">fake_noise</span> <span class="o">=</span> <span class="n">make_noise</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">fake</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">fake_noise</span><span class="p">)</span>
    <span class="n">crit_fake_pred</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
    <span class="n">crit_real_pred</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>

    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">get_gradient</span><span class="p">(</span><span class="n">critic</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">epsilon</span><span class="p">)</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="n">gradient_penalty</span><span class="p">(</span><span class="n">gradient</span><span class="p">)</span>
    <span class="n">crit_loss</span> <span class="o">=</span> <span class="n">get_crit_loss</span><span class="p">(</span><span class="n">crit_fake_pred</span><span class="p">,</span> <span class="n">crit_real_pred</span><span class="p">,</span> <span class="n">gp</span><span class="p">,</span> <span class="n">c_lambda</span><span class="p">)</span>

    <span class="c1"># Keep track of the average critic loss in this batch</span>
    <span class="n">mean_iteration_critic_loss</span> <span class="o">=</span> <span class="n">crit_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">crit_repeats</span>
    <span class="c1"># Update gradients</span>
    <span class="n">crit_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update optimizer</span>
    <span class="n">crit_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">mean_iteration_critic_loss</span><span class="p">,</span> <span class="n">fake</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">update_generator</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">generator_optimizer</span><span class="p">,</span> <span class="n">critic</span><span class="p">,</span> <span class="n">critic_optimizer</span><span class="p">,</span>
                     <span class="n">batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">):</span>
        <span class="n">generator_optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">fake_noise_2</span> <span class="o">=</span> <span class="n">make_noise</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">fake_2</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">fake_noise_2</span><span class="p">)</span>
        <span class="n">crit_fake_pred</span> <span class="o">=</span> <span class="n">critic</span><span class="p">(</span><span class="n">fake_2</span><span class="p">)</span>

        <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">get_gen_loss</span><span class="p">(</span><span class="n">crit_fake_pred</span><span class="p">)</span>
        <span class="n">gen_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Update the weights</span>
        <span class="n">generator_optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">gen_loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">cur_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">critic_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fakes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="c1"># Dataloader returns the batches</span>
        <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">mean_iteration_critic_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">crit_repeats</span><span class="p">):</span>
                <span class="c1">### Update critic ###</span>
                <span class="n">this_loss</span><span class="p">,</span> <span class="n">fake</span> <span class="o">=</span> <span class="n">update_critic</span><span class="p">(</span><span class="n">crit</span><span class="p">,</span> <span class="n">crit_opt</span><span class="p">,</span> <span class="n">gen</span><span class="p">,</span> <span class="n">gen_opt</span><span class="p">,</span>
                                                <span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">real</span><span class="p">)</span>
                <span class="n">mean_iteration_critic_loss</span> <span class="o">+=</span> <span class="n">this_loss</span>
            <span class="n">critic_losses</span> <span class="o">+=</span> <span class="p">[</span><span class="n">mean_iteration_critic_loss</span><span class="p">]</span>

            <span class="c1">### Update generator ###</span>
            <span class="c1"># Keep track of the average generator loss</span>
            <span class="n">generator_losses</span> <span class="o">+=</span> <span class="n">update_generator</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="n">crit</span><span class="p">,</span> <span class="n">crit_opt</span><span class="p">,</span>
                                                 <span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>

            <span class="c1">### Visualization code ###</span>
            <span class="k">if</span> <span class="n">cur_step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">cur_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">gen_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">generator_losses</span><span class="p">[</span><span class="o">-</span><span class="n">display_step</span><span class="p">:])</span> <span class="o">/</span> <span class="n">display_step</span>
                <span class="n">crit_mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">critic_losses</span><span class="p">[</span><span class="o">-</span><span class="n">display_step</span><span class="p">:])</span> <span class="o">/</span> <span class="n">display_step</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Step </span><span class="si">{</span><span class="n">cur_step</span><span class="si">}</span><span class="s2">: Generator loss: </span><span class="si">{</span><span class="n">gen_mean</span><span class="si">}</span><span class="s2">, critic loss: </span><span class="si">{</span><span class="n">crit_mean</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                <span class="n">fakes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fake</span><span class="p">)</span>
                <span class="c1">#show_tensor_images(fake)</span>
                <span class="c1"># show_tensor_images(real)</span>
                <span class="c1"># step_bins = 20</span>
                <span class="c1">#num_examples = (len(generator_losses) // step_bins) * step_bins</span>
                <span class="c1">#plt.plot(</span>
                <span class="c1">#    range(num_examples // step_bins), </span>
                <span class="c1">#    torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),</span>
                <span class="c1">#    label="Generator Loss"</span>
                <span class="c1">#)</span>
                <span class="c1">#plt.plot(</span>
                <span class="c1">#    range(num_examples // step_bins), </span>
                <span class="c1">#    torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),</span>
                <span class="c1">#    label="Critic Loss"</span>
                <span class="c1">#)</span>
                <span class="c1">#plt.legend()</span>
                <span class="c1">#plt.show()</span>

            <span class="n">cur_step</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
<pre class="example">
Started: 2021-04-23 16:44:37.086571
Step 50: Generator loss: 1.2940945455431938, critic loss: -2.5389487731456755
Step 100: Generator loss: 1.8233803486824036, critic loss: -10.170887191772463
Step 150: Generator loss: -0.8236922709643841, critic loss: -25.889275665283208
Step 200: Generator loss: -1.9489177632331849, critic loss: -57.93669644165039
Step 250: Generator loss: -1.6910316547751427, critic loss: -98.02721130371094
Step 300: Generator loss: -1.057899413406849, critic loss: -148.77607403564457
Step 350: Generator loss: -1.0930944073200226, critic loss: -199.94886077880858
Step 400: Generator loss: 1.900166620016098, critic loss: -245.53067184448247
Step 450: Generator loss: -18.928784263134002, critic loss: -251.46439450645448
Step 500: Generator loss: -7.688082475662231, critic loss: -289.45334830856325
Step 550: Generator loss: 12.447209596633911, critic loss: -395.351733947754
Step 600: Generator loss: 4.604712443947792, critic loss: -442.96986193847647
Step 650: Generator loss: 2.1788939160108565, critic loss: -480.044010131836
Step 700: Generator loss: 2.979072951376438, critic loss: -519.4769331054689
Step 750: Generator loss: -49.77768729448319, critic loss: -406.99980457305907
Step 800: Generator loss: 0.28986886143684387, critic loss: -444.8244698066711
Step 850: Generator loss: 31.1217813873291, critic loss: -608.1500103759765
Step 900: Generator loss: 12.006675623655319, critic loss: -632.0770750808719
Step 950: Generator loss: -0.15041383981704712, critic loss: -659.3277660064699
Step 1000: Generator loss: 15.936325817108154, critic loss: -629.952421447754
Step 1050: Generator loss: -43.25309041261673, critic loss: -504.54743419075004
Step 1100: Generator loss: -127.80617136001587, critic loss: -347.7993973159789
Step 1150: Generator loss: 4.186352119445801, critic loss: -461.6966292152405
Step 1200: Generator loss: 19.471285017728807, critic loss: -417.6742295103073
Step 1250: Generator loss: 34.04052387237549, critic loss: -327.74495936584475
Step 1300: Generator loss: -61.267093954086306, critic loss: -114.96264076042176
Step 1350: Generator loss: -56.96540081501007, critic loss: -257.8397505912781
Step 1400: Generator loss: -58.51407446861267, critic loss: -284.2404485015868
Step 1450: Generator loss: -31.23556293010712, critic loss: -282.15282668590544
Step 1500: Generator loss: 21.97936663866043, critic loss: -201.8184239835738
Step 1550: Generator loss: -35.051265001297, critic loss: -268.2542330398559
Step 1600: Generator loss: -13.768656857013703, critic loss: -201.92625104904172
Step 1650: Generator loss: 22.134875717163087, critic loss: -222.15251140356065
Step 1700: Generator loss: -33.80421092987061, critic loss: -196.00927429389947
Step 1750: Generator loss: -57.25435597419739, critic loss: -182.85244289588928
Step 1800: Generator loss: -41.60410815238953, critic loss: -213.254286611557
Step 1850: Generator loss: -4.978743267059326, critic loss: -101.88668561553959
Step 1900: Generator loss: 43.375376815795896, critic loss: 24.468120357513428
Step 1950: Generator loss: 37.55927352905273, critic loss: 19.142875072479246
Step 2000: Generator loss: 30.793880767822266, critic loss: 27.632160606384268
Step 2050: Generator loss: 28.9916410446167, critic loss: 37.41749234771728
Step 2100: Generator loss: 28.57459102630615, critic loss: 36.46667390441895
Step 2150: Generator loss: 27.179994583129883, critic loss: 37.36057964324953
Step 2200: Generator loss: 26.722407608032228, critic loss: 36.42123816680908
Step 2250: Generator loss: 26.215636711120606, critic loss: 35.10568865203857
Step 2300: Generator loss: 25.28977954864502, critic loss: 38.4949776916504
Step 2350: Generator loss: 25.161714172363283, critic loss: 30.91700393295288
Step 2400: Generator loss: 25.609521713256836, critic loss: 27.127794273376463
Step 2450: Generator loss: 26.457210426330565, critic loss: 23.25596778869629
Step 2500: Generator loss: 27.144473686218262, critic loss: 18.423582084655763
Step 2550: Generator loss: 28.104863624572754, critic loss: 16.720462280273438
Step 2600: Generator loss: 29.460466690063477, critic loss: 13.846090631484987
Step 2650: Generator loss: 31.16196632385254, critic loss: 10.717047594070436
Step 2700: Generator loss: 32.86851013183594, critic loss: 8.973742393493652
Step 2750: Generator loss: 33.90616256713867, critic loss: 9.844469717025756
Step 2800: Generator loss: 34.65669334411621, critic loss: 8.557393852233888
Step 2850: Generator loss: 35.84923110961914, critic loss: 6.227309632301333
Step 2900: Generator loss: 37.1290372467041, critic loss: 4.664727992773057
Step 2950: Generator loss: 39.00773422241211, critic loss: 3.6960949053764343
Step 3000: Generator loss: 41.04932693481445, critic loss: 3.064339481592179
Step 3050: Generator loss: 43.54303398132324, critic loss: 1.5976664029359815
Step 3100: Generator loss: 46.25879165649414, critic loss: 0.43558707976341254
Step 3150: Generator loss: 48.358483200073245, critic loss: -0.8735819962918758
Step 3200: Generator loss: 49.9193138885498, critic loss: -1.9399951877593993
Step 3250: Generator loss: 50.604149169921875, critic loss: -2.96596682035923
Step 3300: Generator loss: 51.37260269165039, critic loss: -4.266795755624772
Step 3350: Generator loss: 50.53414665222168, critic loss: -6.2572406907081595
Step 3400: Generator loss: 49.34995780944824, critic loss: -8.031075536847114
Step 3450: Generator loss: 46.14337966918945, critic loss: -8.019683789610863
Step 3500: Generator loss: 42.769298782348635, critic loss: -9.498445952415468
Step 3550: Generator loss: 37.38293798446655, critic loss: -9.02791331624985
Step 3600: Generator loss: 32.84453460693359, critic loss: -9.934100509524345
Step 3650: Generator loss: 29.88087886810303, critic loss: -9.069164658904075
Step 3700: Generator loss: 27.295934791564942, critic loss: -12.109804625511167
Step 3750: Generator loss: 23.694135398864745, critic loss: -14.327697192192076
Step 3800: Generator loss: 22.836445541381835, critic loss: -15.450897558450697
Step 3850: Generator loss: 21.66964967727661, critic loss: -18.371595690727236
Step 3900: Generator loss: 22.644691734313966, critic loss: -18.472765784740442
Step 3950: Generator loss: 23.275020160675048, critic loss: -14.622903740763663
Step 4000: Generator loss: 20.404177145957945, critic loss: -20.265531128883364
Step 4050: Generator loss: 20.57322360277176, critic loss: -22.811122689247135
Step 4100: Generator loss: 20.653975734710695, critic loss: -21.081045699119564
Step 4150: Generator loss: 22.07396845817566, critic loss: -25.1140656299591
Step 4200: Generator loss: 23.147041385173797, critic loss: -25.637423175573346
Step 4250: Generator loss: 24.7466512966156, critic loss: -27.446938713431358
Step 4300: Generator loss: 23.155011949539183, critic loss: -29.866371290445326
Step 4350: Generator loss: 28.670740413665772, critic loss: -27.526438851594932
Step 4400: Generator loss: 28.197952184677124, critic loss: -32.777981144189845
Step 4450: Generator loss: 30.352355518341064, critic loss: -27.1594803442955
Step 4500: Generator loss: 28.54464930534363, critic loss: -33.94081681919097
Step 4550: Generator loss: 30.315768175125122, critic loss: -32.86432695555688
Step 4600: Generator loss: 31.542511186599732, critic loss: -30.20407930350304
Step 4650: Generator loss: 32.1046596121788, critic loss: -25.409390352845193
Step 4700: Generator loss: 32.14258025169372, critic loss: -31.69375462341309
Step 4750: Generator loss: 34.99601099014282, critic loss: -17.207461384415634
Step 4800: Generator loss: 34.72456073760986, critic loss: -28.68983098757266
Step 4850: Generator loss: 43.15867195129395, critic loss: -3.741025509417056
Step 4900: Generator loss: 39.205870933532715, critic loss: -10.995340047717095
Step 4950: Generator loss: 33.214964599609374, critic loss: -22.35341439080238
Step 5000: Generator loss: 36.83505029678345, critic loss: -22.059852074146274
Step 5050: Generator loss: 44.310142288208006, critic loss: -9.833503689646719
Step 5100: Generator loss: 46.455570983886716, critic loss: -6.97827914196253
Step 5150: Generator loss: 50.3965446472168, critic loss: 2.86564082187414
Step 5200: Generator loss: 49.87795219421387, critic loss: -1.3452879690229889
Step 5250: Generator loss: 47.53674819946289, critic loss: -2.096805039405823
Step 5300: Generator loss: 46.8746314239502, critic loss: -3.2593628435134883
Step 5350: Generator loss: 45.44812057495117, critic loss: -8.15779336643219
Step 5400: Generator loss: 44.419895820617675, critic loss: -14.570247013330457
Step 5450: Generator loss: 46.02410781860352, critic loss: -15.177982830524446
Step 5500: Generator loss: 49.54875686645508, critic loss: -9.89209368979931
Step 5550: Generator loss: 48.06167510986328, critic loss: -14.110691767692567
Step 5600: Generator loss: 49.201857833862306, critic loss: -14.137419148623945
Step 5650: Generator loss: 50.152088012695316, critic loss: -12.306397112727165
Step 5700: Generator loss: 48.29638786315918, critic loss: -16.661144974470133
Step 5750: Generator loss: 48.57353067398071, critic loss: -14.890159791767603
Step 5800: Generator loss: 49.75064552307129, critic loss: -18.844482659339906
Step 5850: Generator loss: 60.04904914855957, critic loss: -6.717597324132919
Step 5900: Generator loss: 51.537723999023434, critic loss: -16.97626993632317
Step 5950: Generator loss: 53.64197952270508, critic loss: -17.934735801696778
Step 6000: Generator loss: 58.61811660766602, critic loss: -12.544874910593034
Step 6050: Generator loss: 57.9530167388916, critic loss: -12.869983579158779
Step 6100: Generator loss: 58.112417755126955, critic loss: -14.860800614833833
Step 6150: Generator loss: 59.45550857543945, critic loss: -16.21854728984833
Step 6200: Generator loss: 61.55990020751953, critic loss: -13.752459713578226
Step 6250: Generator loss: 63.91949012756348, critic loss: -15.32866345870495
Step 6300: Generator loss: 61.11529357910156, critic loss: -19.138810309529305
Step 6350: Generator loss: 68.78476165771484, critic loss: -3.858711770117282
Step 6400: Generator loss: 72.07508163452148, critic loss: -3.3317795319557204
Step 6450: Generator loss: 62.11038558959961, critic loss: -12.74781008577347
Step 6500: Generator loss: 66.10368064880372, critic loss: -13.66576182627678
Step 6550: Generator loss: 62.73857864379883, critic loss: -19.79733684468269
Step 6600: Generator loss: 64.86283889770507, critic loss: -15.91535943055153
Step 6650: Generator loss: 65.02771781921386, critic loss: -16.515603628635407
Step 6700: Generator loss: 73.10651649475098, critic loss: -7.974747009277344
Step 6750: Generator loss: 69.39200439453126, critic loss: -12.647881946563723
Step 6800: Generator loss: 70.61859390258789, critic loss: -14.981548887073998
Step 6850: Generator loss: 71.39209846496583, critic loss: -12.02037605035305
Step 6900: Generator loss: 68.91642692565918, critic loss: -17.377452049493794
Step 6950: Generator loss: 73.83714424133301, critic loss: -14.842290714025498
Step 7000: Generator loss: 76.0492682647705, critic loss: -4.022153543114662
Step 7050: Generator loss: 73.60314575195312, critic loss: -11.167652189731598
Step 7100: Generator loss: 73.69744178771973, critic loss: -16.215790304422377
Step 7150: Generator loss: 73.02161018371582, critic loss: -11.844917020320892
Step 7200: Generator loss: 84.43860961914062, critic loss: -4.338678442955016
Step 7250: Generator loss: 72.4216611480713, critic loss: -16.95018665671349
Step 7300: Generator loss: 75.08161041259766, critic loss: -13.94019952297211
Step 7350: Generator loss: 76.7044221496582, critic loss: -14.254385577440262
Step 7400: Generator loss: 81.03584564208984, critic loss: -3.171723330259324
Step 7450: Generator loss: 80.19454528808593, critic loss: -6.323260527610778
Step 7500: Generator loss: 74.55620361328126, critic loss: -8.62027923491597
Step 7550: Generator loss: 84.05591217041015, critic loss: -3.5706960783004775
Step 7600: Generator loss: 81.2258724975586, critic loss: -8.142396178722382
Step 7650: Generator loss: 73.19812255859375, critic loss: -16.196065732836722
Step 7700: Generator loss: 74.52944702148437, critic loss: -15.7419521817565
Step 7750: Generator loss: 80.32163719177247, critic loss: -7.413010147571564
Step 7800: Generator loss: 76.99493499755859, critic loss: -12.079633572757244
Step 7850: Generator loss: 81.32430145263672, critic loss: -2.8193510160446174
Step 7900: Generator loss: 80.63022003173828, critic loss: -3.1151746976375576
Step 7950: Generator loss: 75.89005561828613, critic loss: -8.688790566921234
Step 8000: Generator loss: 72.94720428466798, critic loss: -14.186805599212649
Step 8050: Generator loss: 80.84135955810547, critic loss: -11.586392744839191
Step 8100: Generator loss: 79.48079322814941, critic loss: -1.3788062819838527
Step 8150: Generator loss: 72.63796539306641, critic loss: -14.767250993669036
Step 8200: Generator loss: 76.29679145812989, critic loss: -16.04671211397648
Step 8250: Generator loss: 72.60974617004395, critic loss: -17.008654308915133
Step 8300: Generator loss: 75.25621772766114, critic loss: -12.109682399034496
Step 8350: Generator loss: 81.09654647827148, critic loss: -10.706179085254668
Step 8400: Generator loss: 77.28005485534668, critic loss: -4.09239830350876
Step 8450: Generator loss: 83.45014526367187, critic loss: -3.1862959499359125
Step 8500: Generator loss: 80.24715942382812, critic loss: -4.144565615177154
Step 8550: Generator loss: 76.43464157104492, critic loss: -9.53649512773752
Step 8600: Generator loss: 73.67140350341796, critic loss: -15.18680296653509
Step 8650: Generator loss: 75.6114599609375, critic loss: -10.128391755342484
Step 8700: Generator loss: 73.68272163391113, critic loss: -16.97586714470387
Step 8750: Generator loss: 83.1702619934082, critic loss: -0.6609140309095384
Step 8800: Generator loss: 80.41752578735351, critic loss: -4.212692310333251
Step 8850: Generator loss: 71.03237358093261, critic loss: -14.983835175275805
Step 8900: Generator loss: 75.80495880126954, critic loss: -12.667168443322183
Step 8950: Generator loss: 81.14228034973145, critic loss: 2.7472501730918872
Step 9000: Generator loss: 81.20193344116211, critic loss: -3.052738008499146
Step 9050: Generator loss: 73.43904174804688, critic loss: -7.423715700268742
Step 9100: Generator loss: 73.12181861877441, critic loss: -14.306883191585541
Step 9150: Generator loss: 76.89906158447266, critic loss: -13.396733086347583
Step 9200: Generator loss: 75.99712623596191, critic loss: -12.318668732821939
Step 9250: Generator loss: 77.78204513549805, critic loss: -6.621456883490087
Step 9300: Generator loss: 77.82661689758301, critic loss: -11.999425900220869
Step 9350: Generator loss: 81.48483535766601, critic loss: -11.480147421479224
Step 9400: Generator loss: 75.37383903503418, critic loss: -11.605070021390913
Step 9450: Generator loss: 83.24758972167969, critic loss: -1.770111013114451
Step 9500: Generator loss: 75.71745803833008, critic loss: -14.370290687352417
Step 9550: Generator loss: 80.75228134155273, critic loss: -12.244659341961144
Step 9600: Generator loss: 80.36522689819336, critic loss: -9.994889120757579
Step 9650: Generator loss: 79.76879989624024, critic loss: -12.11628355455398
Step 9700: Generator loss: 75.03965270996093, critic loss: -15.582087687492374
Step 9750: Generator loss: 78.26055725097656, critic loss: -9.227732668161394
Step 9800: Generator loss: 86.73946716308593, critic loss: -3.9114915781021113
Step 9850: Generator loss: 77.57634506225585, critic loss: -16.903033419966697
Step 9900: Generator loss: 79.62038360595703, critic loss: -13.387711975812913
Step 9950: Generator loss: 83.48049461364747, critic loss: 0.4212318459749224
Step 10000: Generator loss: 86.0385548400879, critic loss: -3.0202082567214954
Step 10050: Generator loss: 84.96556030273437, critic loss: -3.2984186277389527
Step 10100: Generator loss: 82.55163467407226, critic loss: -5.651416356563568
Step 10150: Generator loss: 72.47459297180175, critic loss: -16.2935069770813
Step 10200: Generator loss: 77.47050117492675, critic loss: -14.219993201971054
Step 10250: Generator loss: 82.40048095703125, critic loss: -9.951535837292676
Step 10300: Generator loss: 78.51686393737793, critic loss: -5.037457182884218
Step 10350: Generator loss: 79.00918548583985, critic loss: -10.983480290770531
Step 10400: Generator loss: 79.10479446411132, critic loss: -11.458023426651957
Step 10450: Generator loss: 79.01952590942383, critic loss: -13.550984252214432
Step 10500: Generator loss: 79.7324333190918, critic loss: -15.04755926167965
Step 10550: Generator loss: 83.25529792785645, critic loss: -10.678096773743627
Step 10600: Generator loss: 78.7729409790039, critic loss: -14.363517974853519
Step 10650: Generator loss: 83.80620101928712, critic loss: -12.4009742795825
Step 10700: Generator loss: 83.44554489135743, critic loss: -5.4632708239853365
Step 10750: Generator loss: 84.38950912475586, critic loss: -4.946207571595907
Step 10800: Generator loss: 84.90599151611327, critic loss: -10.688541789770127
Step 10850: Generator loss: 80.39469886779786, critic loss: -13.391746405303474
Step 10900: Generator loss: 79.68403381347656, critic loss: -14.792330410242082
Step 10950: Generator loss: 84.55435623168945, critic loss: -12.792006389081477
Step 11000: Generator loss: 85.3377848815918, critic loss: -1.002582928955554
Step 11050: Generator loss: 76.42176498413086, critic loss: -16.618346381425855
Step 11100: Generator loss: 82.8500619506836, critic loss: -10.213502784013746
Step 11150: Generator loss: 80.111083984375, critic loss: -16.506468793153765
Step 11200: Generator loss: 81.84511749267578, critic loss: -14.588824108004571
Step 11250: Generator loss: 82.36108421325683, critic loss: -14.826971750736238
Step 11300: Generator loss: 82.89525245666503, critic loss: -14.743118989944467
Step 11350: Generator loss: 78.9609211730957, critic loss: -6.072368972778322
Step 11400: Generator loss: 79.75704879760742, critic loss: -11.66915795624256
Step 11450: Generator loss: 92.73718231201173, critic loss: -8.626956017732619
Step 11500: Generator loss: 76.74110557556152, critic loss: -13.485125755786896
Step 11550: Generator loss: 86.92150177001953, critic loss: -11.96049699956179
Step 11600: Generator loss: 87.94025703430175, critic loss: -7.829241111636162
Step 11650: Generator loss: 78.58638778686523, critic loss: -13.818019400000573
Step 11700: Generator loss: 82.94163925170898, critic loss: -16.088717435359957
Step 11750: Generator loss: 82.20194442749023, critic loss: -13.443735618114472
Step 11800: Generator loss: 77.3590771484375, critic loss: -0.26538432469963885
Step 11850: Generator loss: 87.65712219238281, critic loss: -2.2925723257064816
Step 11900: Generator loss: 86.44266906738281, critic loss: -2.755362086296081
Step 11950: Generator loss: 85.7614064025879, critic loss: -2.9416364326477047
Step 12000: Generator loss: 84.22476821899414, critic loss: -3.100327790260315
Step 12050: Generator loss: 81.84705871582031, critic loss: -3.3889783926010137
Step 12100: Generator loss: 74.62463600158691, critic loss: -9.155223772525787
Step 12150: Generator loss: 83.41003746032715, critic loss: -7.312069640517238
Step 12200: Generator loss: 77.82574188232422, critic loss: -10.063361536026001
Step 12250: Generator loss: 77.09058532714843, critic loss: -15.389594004154203
Step 12300: Generator loss: 85.65135437011719, critic loss: -11.597671725511553
Step 12350: Generator loss: 79.91491325378418, critic loss: -0.8456090040206905
Step 12400: Generator loss: 83.31446044921876, critic loss: -3.5672192862033842
Step 12450: Generator loss: 80.4154541015625, critic loss: -9.493659735798834
Step 12500: Generator loss: 77.22660888671875, critic loss: -11.343838263094426
Step 12550: Generator loss: 76.51863540649414, critic loss: -15.957162732720372
Step 12600: Generator loss: 71.82434341430664, critic loss: -15.232202378749843
Step 12650: Generator loss: 81.55846801757812, critic loss: -12.02893185913563
Step 12700: Generator loss: 77.01351791381836, critic loss: -14.394531373143197
Step 12750: Generator loss: 82.79933059692382, critic loss: -10.995534277558324
Step 12800: Generator loss: 80.33022705078125, critic loss: -7.422801446437835
Step 12850: Generator loss: 77.88019416809082, critic loss: -10.48680070441961
Step 12900: Generator loss: 77.28355583190918, critic loss: -15.062006795048712
Step 12950: Generator loss: 72.02762420654297, critic loss: -18.125201426446434
Step 13000: Generator loss: 78.97825164794922, critic loss: -11.02606911355257
Step 13050: Generator loss: 76.02745002746582, critic loss: -13.242777463912965
Step 13100: Generator loss: 82.44893028259277, critic loss: -10.203380972802634
Step 13150: Generator loss: 80.63447105407715, critic loss: -11.436619911789894
Step 13200: Generator loss: 69.52673934936523, critic loss: -12.998723325610163
Step 13250: Generator loss: 75.26367416381837, critic loss: -12.58380482053757
Step 13300: Generator loss: 78.29216751098633, critic loss: 0.21028297042846839
Step 13350: Generator loss: 70.94842475891113, critic loss: -8.405993442416191
Step 13400: Generator loss: 77.60350791931153, critic loss: -12.201066960632803
Step 13450: Generator loss: 78.38650337219238, critic loss: -13.255251537919046
Step 13500: Generator loss: 72.39071220397949, critic loss: -13.91472595399618
Step 13550: Generator loss: 78.81595336914063, critic loss: -12.717635474145416
Step 13600: Generator loss: 69.23250061035156, critic loss: -15.01334501111508
Step 13650: Generator loss: 77.3666291809082, critic loss: -16.321711009979246
Step 13700: Generator loss: 73.45859939575195, critic loss: -17.17580293393135
Step 13750: Generator loss: 74.07134948730469, critic loss: -14.143001305580142
Step 13800: Generator loss: 68.98319381713867, critic loss: -18.013431072473526
Step 13850: Generator loss: 73.18379371643067, critic loss: -13.245033169150352
Step 13900: Generator loss: 73.70108238220215, critic loss: -15.747089947700497
Step 13950: Generator loss: 71.67143341064452, critic loss: -6.442092946648602
Step 14000: Generator loss: 74.99322380065918, critic loss: -5.310949310302733
Step 14050: Generator loss: 69.55456466674805, critic loss: -7.584069814443586
Step 14100: Generator loss: 68.11343818664551, critic loss: -15.932588892817499
Step 14150: Generator loss: 73.32868095397949, critic loss: -14.538219540774824
Step 14200: Generator loss: 71.54050506591797, critic loss: -6.507004916965961
Step 14250: Generator loss: 73.50055587768554, critic loss: -12.074983437180519
Step 14300: Generator loss: 75.37609176635742, critic loss: -12.215355042934414
Step 14350: Generator loss: 78.41978523254394, critic loss: -13.282461894750588
Step 14400: Generator loss: 69.06725090026856, critic loss: -8.44315874606371
Step 14450: Generator loss: 77.47375007629394, critic loss: -10.59642046368122
Step 14500: Generator loss: 72.112548828125, critic loss: -9.080148652315138
Step 14550: Generator loss: 71.41747200012207, critic loss: -12.610691975355143
Step 14600: Generator loss: 68.53853507995605, critic loss: -14.517420025825501
Step 14650: Generator loss: 71.00217765808105, critic loss: -16.055311642885208
Step 14700: Generator loss: 75.56183944702148, critic loss: -4.261986103117466
Step 14750: Generator loss: 68.21860916137695, critic loss: -14.03696541213989
Step 14800: Generator loss: 71.7959959411621, critic loss: -13.989702057063587
Step 14850: Generator loss: 76.38227409362793, critic loss: -10.939811514139176
Step 14900: Generator loss: 67.81556015014648, critic loss: -15.070325279712678
Step 14950: Generator loss: 71.62906150817871, critic loss: -12.239016912937165
Step 15000: Generator loss: 73.60893783569335, critic loss: -5.476252611890436
Step 15050: Generator loss: 64.43828086853027, critic loss: -11.680644391536712
Step 15100: Generator loss: 66.76135398864746, critic loss: -18.935012437820443
Step 15150: Generator loss: 64.45999412536621, critic loss: -16.77594568133354
Step 15200: Generator loss: 68.81907485961914, critic loss: -16.819265387773513
Step 15250: Generator loss: 71.44663459777831, critic loss: -14.780536164999004
Step 15300: Generator loss: 69.05639785766601, critic loss: -15.773872276782981
Step 15350: Generator loss: 72.00610313415527, critic loss: -12.428475862145426
Step 15400: Generator loss: 66.33817840576172, critic loss: -11.460507846534249
Step 15450: Generator loss: 73.98657371520996, critic loss: -12.046799251675607
Step 15500: Generator loss: 71.14604766845703, critic loss: -14.48868891143799
Step 15550: Generator loss: 72.55676879882813, critic loss: -9.285633412837981
Step 15600: Generator loss: 72.27706947326661, critic loss: -11.433179477930068
Step 15650: Generator loss: 70.14899436950684, critic loss: -14.64919223260879
Step 15700: Generator loss: 70.52759353637695, critic loss: -13.822800672113893
Step 15750: Generator loss: 66.5163092803955, critic loss: -13.497988208055496
Step 15800: Generator loss: 65.68713722229003, critic loss: -13.090139507174491
Step 15850: Generator loss: 68.86076667785645, critic loss: -12.112882311582563
Step 15900: Generator loss: 72.71573020935058, critic loss: -11.739855915784835
Step 15950: Generator loss: 69.23649925231933, critic loss: 1.5610642746686931
Step 16000: Generator loss: 63.27606719970703, critic loss: -6.625546929836272
Step 16050: Generator loss: 65.96758232116699, critic loss: -17.750343059539794
Step 16100: Generator loss: 62.09547576904297, critic loss: -17.4672027888298
Step 16150: Generator loss: 75.76868995666504, critic loss: -12.86666469740868
Step 16200: Generator loss: 64.08884880065918, critic loss: -7.587684287369252
Step 16250: Generator loss: 64.09755882263184, critic loss: -10.57423495966196
Step 16300: Generator loss: 66.86840660095214, critic loss: -3.5877239196300508
Step 16350: Generator loss: 70.60188285827637, critic loss: -7.692209842979907
Step 16400: Generator loss: 62.29129165649414, critic loss: -12.677523095130923
Step 16450: Generator loss: 62.98073165893555, critic loss: -13.866112356960771
Step 16500: Generator loss: 61.780632400512694, critic loss: -6.281874860048294
Step 16550: Generator loss: 62.74724609375, critic loss: -13.956338333368299
Step 16600: Generator loss: 61.48925178527832, critic loss: -16.810678883075717
Step 16650: Generator loss: 52.7329150390625, critic loss: -18.111987345457074
Step 16700: Generator loss: 60.26760322570801, critic loss: -17.837719259858133
Step 16750: Generator loss: 60.27441291809082, critic loss: -14.668455944180492
Step 16800: Generator loss: 64.81710945129394, critic loss: -8.937785160303115
Step 16850: Generator loss: 61.87463485717773, critic loss: -13.174851733446122
Step 16900: Generator loss: 66.52726516723632, critic loss: -17.641908020138743
Step 16950: Generator loss: 63.35795883178711, critic loss: -17.725372922539712
Step 17000: Generator loss: 67.46929817199707, critic loss: -13.343407141447067
Step 17050: Generator loss: 59.79177055358887, critic loss: -16.512492282271385
Step 17100: Generator loss: 66.42052528381348, critic loss: -9.183917128443717
Step 17150: Generator loss: 59.208996353149416, critic loss: -13.243339603602893
Step 17200: Generator loss: 63.88817520141602, critic loss: -13.442776112914084
Step 17250: Generator loss: 69.03452033996582, critic loss: -11.0614826682806
Step 17300: Generator loss: 57.58331108093262, critic loss: -13.529039879202841
Step 17350: Generator loss: 67.6368569946289, critic loss: -11.620229701399802
Step 17400: Generator loss: 60.044710845947264, critic loss: -9.055887681692841
Step 17450: Generator loss: 64.43620628356933, critic loss: -11.920627628207207
Step 17500: Generator loss: 56.046851272583005, critic loss: -22.301562000870714
Step 17550: Generator loss: 62.282958908081056, critic loss: -14.955312865734099
Step 17600: Generator loss: 65.897964553833, critic loss: -6.340100202620029
Step 17650: Generator loss: 58.14865257263184, critic loss: -10.649906709671022
Step 17700: Generator loss: 66.7437523651123, critic loss: -12.996105446338657
Step 17750: Generator loss: 63.59517837524414, critic loss: -12.67168800020218
Step 17800: Generator loss: 65.87414787292481, critic loss: -13.124171116769311
Step 17850: Generator loss: 66.03790901184082, critic loss: -13.294757736086847
Step 17900: Generator loss: 56.273787307739255, critic loss: -18.649981175422667
Step 17950: Generator loss: 68.3505224609375, critic loss: -13.77443748676777
Step 18000: Generator loss: 59.758854675292966, critic loss: -13.528435281991955
Step 18050: Generator loss: 70.61318840026856, critic loss: -12.014795050919052
Step 18100: Generator loss: 62.63155372619629, critic loss: -9.601117482304572
Step 18150: Generator loss: 58.44773849487305, critic loss: -9.71425095164776
Step 18200: Generator loss: 59.341090240478515, critic loss: -16.878086137115954
Step 18250: Generator loss: 57.848808708190916, critic loss: -19.507797758817674
Step 18300: Generator loss: 63.10433967590332, critic loss: -4.3734778246283526
Step 18350: Generator loss: 57.56446601867676, critic loss: -10.789146659135817
Step 18400: Generator loss: 51.76399398803711, critic loss: -15.076736944794657
Step 18450: Generator loss: 57.02366355895996, critic loss: -12.479052137970923
Step 18500: Generator loss: 62.833531875610355, critic loss: -12.99720428943634
Step 18550: Generator loss: 56.588841400146485, critic loss: -14.211519970417026
Step 18600: Generator loss: 61.620222854614255, critic loss: -14.894168957710265
Step 18650: Generator loss: 59.04514297485352, critic loss: -3.9987226614952096
Step 18700: Generator loss: 54.68501613616943, critic loss: -13.798751793980603
Step 18750: Generator loss: 60.477030181884764, critic loss: -13.97087904036045
Step 18800: Generator loss: 59.51054759979248, critic loss: -18.694762709856033
Step 18850: Generator loss: 53.82080192565918, critic loss: -14.210277070969342
Step 18900: Generator loss: 64.43251205444336, critic loss: -13.768319549560543
Step 18950: Generator loss: 56.9788289642334, critic loss: -10.571144456863403
Step 19000: Generator loss: 59.03595703125, critic loss: -12.603199533462528
Step 19050: Generator loss: 60.19775802612305, critic loss: -14.499388661146167
Step 19100: Generator loss: 59.592409973144534, critic loss: -8.202755635917187
Step 19150: Generator loss: 55.50546585083008, critic loss: -16.347688998579976
Step 19200: Generator loss: 61.19869083404541, critic loss: -18.950819284915923
Step 19250: Generator loss: 66.31558391571045, critic loss: -12.890463754177098
Step 19300: Generator loss: 57.29240139007568, critic loss: -18.10998232960701
Step 19350: Generator loss: 59.32999900817871, critic loss: -13.577078444600104
Step 19400: Generator loss: 65.96876052856446, critic loss: -11.8188825455904
Step 19450: Generator loss: 56.72755683898926, critic loss: -14.319641982913016
Step 19500: Generator loss: 57.38858169555664, critic loss: -17.450813733339313
Step 19550: Generator loss: 66.02516723632813, critic loss: -10.693548452854154
Step 19600: Generator loss: 54.7833975982666, critic loss: -13.142704640865325
Step 19650: Generator loss: 57.28132354736328, critic loss: -14.967523851156233
Step 19700: Generator loss: 59.98361915588379, critic loss: -16.183865994155408
Step 19750: Generator loss: 58.18478466033935, critic loss: -15.35918751955032
Step 19800: Generator loss: 63.54511661529541, critic loss: -10.502776491999626
Step 19850: Generator loss: 56.24938293457031, critic loss: -6.75664558506012
Step 19900: Generator loss: 60.40652961730957, critic loss: -13.489446130156516
Ended: 2021-04-23 17:33:03.659309
Elapsed: 0:48:26.572738
</pre>
<pre class="example">
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-35-08d8bddbdcc4&gt; in &lt;module&gt;
     14             for _ in range(crit_repeats):
     15                 ### Update critic ###
---&gt; 16                 this_loss, fake = update_critic(crit, crit_opt, gen, gen_opt,
     17                                                 cur_batch_size, z_dim, real)
     18                 mean_iteration_critic_loss += this_loss

&lt;ipython-input-33-696fefd91963&gt; in update_critic(critic, critic_optimizer, generator, generator_optimizer, batch_size, z_dim, real)
      7 
      8     epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)
----&gt; 9     gradient = get_gradient(critic, real, fake.detach(), epsilon)
     10     gp = gradient_penalty(gradient)
     11     crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)

&lt;ipython-input-16-06dea6615386&gt; in get_gradient(crit, real, fake, epsilon)
     19 
     20     # Take the gradient of the scores with respect to the images
---&gt; 21     gradient = torch.autograd.grad(
     22         # Note: You need to take the gradient of outputs with respect to inputs.
     23         #### START CODE HERE ####

~/.conda/envs/neurotic-pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)
    221         retain_graph = create_graph
    222 
--&gt; 223     return Variable._execution_engine.run_backward(
    224         outputs, grad_outputs_, retain_graph, create_graph,
    225         inputs, allow_unused, accumulate_grad=False)

RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 7.92 GiB total capacity; 7.13 GiB already allocated; 24.62 MiB free; 7.13 GiB reserved in total by PyTorch)
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org53ff810">
<h2 id="org53ff810">End</h2>
<div class="outline-text-2" id="text-org53ff810">
<ul class="org-ul">
<li>Arjovsky M, Chintala S, Bottou L. Wasserstein generative adversarial networks. International conference on machine learning 2017 Jul 17 (pp. 214-223). PMLR. (<a href="https://arxiv.org/abs/1701.07875?source=post_page-----aee68ed8a38c----------------------">archiv.org</a>)</li>
<li>Gulrajani I, Ahmed F, Arjovsky M, Dumoulin V, Courville A. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028. 2017 Mar 31. (<a href="https://arxiv.org/abs/1704.00028">archiv.org</a>)</li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/cnn-gan/">CNN GAN</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/cnn-gan/" rel="bookmark"><time class="published dt-published" datetime="2021-04-14T19:52:11-07:00" itemprop="datePublished" title="2021-04-14 19:52">2021-04-14 19:52</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/gans/cnn-gan/#org35411e1">Deep Convolutional GAN (DCGAN)</a>
<ul>
<li><a href="posts/gans/cnn-gan/#org75d0c87">Imports</a></li>
<li><a href="posts/gans/cnn-gan/#org1c0670e">Set Up</a>
<ul>
<li><a href="posts/gans/cnn-gan/#orgdd692be">The Random Seed</a></li>
<li><a href="posts/gans/cnn-gan/#orgbf1f331">Plotting and Timing</a></li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#org1335bb2">Helper Functions</a>
<ul>
<li><a href="posts/gans/cnn-gan/#org0067200">A Plotter</a></li>
<li><a href="posts/gans/cnn-gan/#org1c996b6">A Noise Maker</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#orgf513324">Middle</a>
<ul>
<li><a href="posts/gans/cnn-gan/#org66f4dd6">The Generator</a>
<ul>
<li><a href="posts/gans/cnn-gan/#orga4ba662">The Generator Class</a></li>
<li><a href="posts/gans/cnn-gan/#orgc34e570">Setup Testing</a></li>
<li><a href="posts/gans/cnn-gan/#org3a668c6">Unit Tests</a></li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#org176d00a">The Discriminator</a>
<ul>
<li><a href="posts/gans/cnn-gan/#org8de9880">The Discriminator Class</a></li>
<li><a href="posts/gans/cnn-gan/#org7f7dfa4">Set Up Testing</a></li>
<li><a href="posts/gans/cnn-gan/#org1638034">Unit Testing</a></li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#orgd808d27">Training The Model</a>
<ul>
<li><a href="posts/gans/cnn-gan/#org6da4cfb">Set Up The Data</a></li>
<li><a href="posts/gans/cnn-gan/#org281fd86">Set Up the GAN</a></li>
<li><a href="posts/gans/cnn-gan/#orgd61636f">A Weight Initializer</a></li>
<li><a href="posts/gans/cnn-gan/#orga91f155">Train it</a></li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#org882f085">Looking at the Final model.</a></li>
</ul>
</li>
<li><a href="posts/gans/cnn-gan/#org0de3cd6">End</a>
<ul>
<li><a href="posts/gans/cnn-gan/#orgc148eb2">Sources</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org35411e1">
<h2 id="org35411e1">Deep Convolutional GAN (DCGAN)</h2>
<div class="outline-text-2" id="text-org35411e1">
<p>We're going to build a Generative Adversarial Network to generate handwritten digits. Instead of using fully-connected layers we'll use Convolutional layers.</p>
<p>Here are the main features of a DCGAN.</p>
<ul class="org-ul">
<li>Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).</li>
<li>Use BatchNorm in both the generator and the discriminator.</li>
<li>Remove fully connected hidden layers for deeper architectures.</li>
<li>ReLU activation in generator for all layers except for the output, which uses Tanh.</li>
<li>Use LeakyReLU activation in the discriminator for all layers.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org75d0c87">
<h3 id="org75d0c87">Imports</h3>
<div class="outline-text-3" id="text-org75d0c87">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># conda</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1c0670e">
<h3 id="org1c0670e">Set Up</h3>
<div class="outline-text-3" id="text-org1c0670e"></div>
<div class="outline-4" id="outline-container-orgdd692be">
<h4 id="orgdd692be">The Random Seed</h4>
<div class="outline-text-4" id="text-orgdd692be">
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgbf1f331">
<h4 id="orgbf1f331">Plotting and Timing</h4>
<div class="outline-text-4" id="text-orgbf1f331">
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
<span class="n">slug</span> <span class="o">=</span> <span class="s2">"cnn-gan"</span>

<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org1335bb2">
<h3 id="org1335bb2">Helper Functions</h3>
<div class="outline-text-3" id="text-org1335bb2"></div>
<div class="outline-4" id="outline-container-org0067200">
<h4 id="org0067200">A Plotter</h4>
<div class="outline-text-4" id="text-org0067200">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">plot_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span>
                <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">/"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Plot the image and save it</span>

<span class="sd">    Args:</span>
<span class="sd">     image: the tensor with the image to plot</span>
<span class="sd">     filename: name for the final image file</span>
<span class="sd">     title: title to put on top of the image</span>
<span class="sd">     num_images: how many images to put in the composite image</span>
<span class="sd">     size: the size for the image</span>
<span class="sd">     folder: sub-folder to save the file in</span>
<span class="sd">    """</span>
    <span class="n">unflattened_image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">size</span><span class="p">)</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">unflattened_image</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

    <span class="n">pyplot</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">folder</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[[file:</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">]]"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1c996b6">
<h4 id="org1c996b6">A Noise Maker</h4>
<div class="outline-text-4" id="text-org1c996b6">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">make_some_noise</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"cpu"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""create noise vectors</span>

<span class="sd">    creates </span>
<span class="sd">    Args:</span>
<span class="sd">       n_samples: the number of samples to generate, a scalar</span>
<span class="sd">       z_dim: the dimension of the noise vector, a scalar</span>
<span class="sd">       device: the device type (cpu or cuda)</span>

<span class="sd">    Returns:</span>
<span class="sd">     tensor with random numbers from the normal distribution.</span>
<span class="sd">    """</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgf513324">
<h2 id="orgf513324">Middle</h2>
<div class="outline-text-2" id="text-orgf513324"></div>
<div class="outline-3" id="outline-container-org66f4dd6">
<h3 id="org66f4dd6">The Generator</h3>
<div class="outline-text-3" id="text-org66f4dd6">
<p>The first component you will make is the generator. You may notice that instead of passing in the image dimension, you will pass the number of image channels to the generator. This is because with DCGAN, you use convolutions which donâ€™t depend on the number of pixels on an image. However, the number of channels is important to determine the size of the filters.</p>
<p>You will build a generator using 4 layers (3 hidden layers + 1 output layer). As before, you will need to write a function to create a single block for the generator's neural network. From the paper:</p>
<ul class="org-ul">
<li>[u]se batchnorm in both the generator and the discriminator"</li>
<li>[u]se ReLU activation in generator for all layers except for the output, which uses Tanh.</li>
</ul>
<p>Since in DCGAN the activation function will be different for the output layer, you will need to check what layer is being created.</p>
<p>At the end of the generator class, you are given a forward pass function that takes in a noise vector and generates an image of the output dimension using your neural network. You are also given a function to create a noise vector. These functions are the same as the ones from the last assignment.</p>
<p>See also:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.ConvTranspose2d.html">nn.ConvTranspose2d</a></li>
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html">nn.BatchNorm2d</a></li>
</ul>
</div>
<div class="outline-4" id="outline-container-orga4ba662">
<h4 id="orga4ba662">The Generator Class</h4>
<div class="outline-text-4" id="text-orga4ba662">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The DCGAN Generator</span>

<span class="sd">    Args:</span>
<span class="sd">       z_dim: the dimension of the noise vector</span>
<span class="sd">       im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is your default)</span>
<span class="sd">       hidden_dim: the inner dimension,</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span> <span class="o">=</span> <span class="n">z_dim</span>
        <span class="c1"># Build the neural network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gen</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_gen_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                       <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Creates a block for the generator (sub sequence)</span>

<span class="sd">       The parts</span>
<span class="sd">        - a transposed convolution</span>
<span class="sd">        - a batchnorm (except for in the last layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">           input_channels: how many channels the input feature representation has</span>
<span class="sd">           output_channels: how many channels the output feature representation should have</span>
<span class="sd">           kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">           stride: the stride of the convolution</span>
<span class="sd">           final_layer: a boolean, true if it is the final layer and false otherwise </span>
<span class="sd">                     (affects activation and batchnorm)</span>

<span class="sd">       Returns:</span>
<span class="sd">        the sub-sequence of layers</span>
<span class="sd">       """</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Final Layer</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">unsqueeze_noise</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""transforms the noise tensor</span>

<span class="sd">       Args:</span>
<span class="sd">           noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        copy of noise with width and height = 1 and channels = z_dim.</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="n">noise</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">noise</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">z_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""complete a forward pass of the generator: Given a noise tensor, </span>

<span class="sd">       Args:</span>
<span class="sd">        noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        generated images.</span>
<span class="sd">       """</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unsqueeze_noise</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gen</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc34e570">
<h4 id="orgc34e570">Setup Testing</h4>
<div class="outline-text-4" id="text-orgc34e570">
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
<span class="n">num_test</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Test the hidden block</span>
<span class="n">test_hidden_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">gen</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)</span>
<span class="n">test_hidden_block</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_uns_noise</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">unsqueeze_noise</span><span class="p">(</span><span class="n">test_hidden_noise</span><span class="p">)</span>
<span class="n">hidden_output</span> <span class="o">=</span> <span class="n">test_hidden_block</span><span class="p">(</span><span class="n">test_uns_noise</span><span class="p">)</span>

<span class="c1"># Check that it works with other strides</span>
<span class="n">test_hidden_block_stride</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">test_final_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">gen</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)</span> <span class="o">*</span> <span class="mi">20</span>
<span class="n">test_final_block</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">make_gen_block</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_final_uns_noise</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">unsqueeze_noise</span><span class="p">(</span><span class="n">test_final_noise</span><span class="p">)</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="n">test_final_block</span><span class="p">(</span><span class="n">test_final_uns_noise</span><span class="p">)</span>

<span class="c1"># Test the whole thing:</span>
<span class="n">test_gen_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">gen</span><span class="o">.</span><span class="n">z_dim</span><span class="p">)</span>
<span class="n">test_uns_gen_noise</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">unsqueeze_noise</span><span class="p">(</span><span class="n">test_gen_noise</span><span class="p">)</span>
<span class="n">gen_output</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">test_uns_gen_noise</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3a668c6">
<h4 id="org3a668c6">Unit Tests</h4>
<div class="outline-text-4" id="text-org3a668c6">
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.2</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>

<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_hidden_block_stride</span><span class="p">(</span><span class="n">hidden_output</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">gen_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">gen_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="k">assert</span> <span class="n">gen_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.8</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Success!"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org176d00a">
<h3 id="org176d00a">The Discriminator</h3>
<div class="outline-text-3" id="text-org176d00a">
<p>The second component you need to create is the discriminator.</p>
<p>You will use 3 layers in your discriminator's neural network. Like with the generator, you will need to create the method to create a single neural network block for the discriminator.</p>
<p>From the paper:</p>
<ul class="org-ul">
<li>[u]se LeakyReLU activation in the discriminator for all layers.</li>
<li>For the LeakyReLUs, "the slope of the leak was set to 0.2" in DCGAN.</li>
</ul>
<p>See Also:</p>
<ul class="org-ul">
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html">nn.Conv2d</a></li>
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html">nn.BatchNorm2d</a></li>
<li><a href="https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html">nn.LeakyReLU</a></li>
</ul>
</div>
<div class="outline-4" id="outline-container-org8de9880">
<h4 id="org8de9880">The Discriminator Class</h4>
<div class="outline-text-4" id="text-org8de9880">
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The DCGAN Discriminator</span>

<span class="sd">    Args:</span>
<span class="sd">     im_chan: the number of channels in the images, fitted for the dataset used</span>
<span class="sd">             (MNIST is black-and-white, so 1 channel is the default)</span>
<span class="sd">     hidden_dim: the inner dimension,</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im_chan</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Discriminator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">im_chan</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">make_disc_block</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">stride</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">final_layer</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
        <span class="sd">"""Make a sub-block of layers for the discriminator</span>

<span class="sd">        - a convolution</span>
<span class="sd">        - a batchnorm (except for in the last layer)</span>
<span class="sd">        - an activation.</span>

<span class="sd">       Args:</span>
<span class="sd">         input_channels: how many channels the input feature representation has</span>
<span class="sd">         output_channels: how many channels the output feature representation should have</span>
<span class="sd">         kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)</span>
<span class="sd">         stride: the stride of the convolution</span>
<span class="sd">         final_layer: if true it is the final layer and otherwise not</span>
<span class="sd">                     (affects activation and batchnorm)</span>
<span class="sd">       """</span>        
        <span class="c1"># Build the neural block</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">final_layer</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">output_channels</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Final Layer</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">input_channels</span><span class="p">,</span> <span class="n">output_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">),</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""Complete a forward pass of the discriminator</span>

<span class="sd">       Args:</span>
<span class="sd">         image: a flattened image tensor with dimension (im_dim)</span>

<span class="sd">       Returns:</span>
<span class="sd">        a 1-dimension tensor representing fake/real.</span>
<span class="sd">       """</span>
        <span class="n">disc_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">disc_pred</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">disc_pred</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org7f7dfa4">
<h4 id="org7f7dfa4">Set Up Testing</h4>
<div class="outline-text-4" id="text-org7f7dfa4">
<div class="highlight">
<pre><span></span><span class="n">num_test</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>
<span class="n">test_images</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">make_some_noise</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">gen</span><span class="o">.</span><span class="n">z_dim</span><span class="p">))</span>

<span class="c1"># Test the hidden block</span>
<span class="n">test_hidden_block</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">hidden_output</span> <span class="o">=</span> <span class="n">test_hidden_block</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>

<span class="c1"># Test the final block</span>
<span class="n">test_final_block</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">make_disc_block</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">final_layer</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">final_output</span> <span class="o">=</span> <span class="n">test_final_block</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>

<span class="c1"># Test the whole thing:</span>
<span class="n">disc_output</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">test_images</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1638034">
<h4 id="org1638034">Unit Testing</h4>
<div class="outline-text-4" id="text-org1638034"></div>
<ul class="org-ul">
<li><a id="org2e145f8"></a>The Hidden Block<br>
<div class="outline-text-5" id="text-org2e145f8">
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="c1"># Because of the LeakyReLU slope</span>
<span class="k">assert</span> <span class="o">-</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">/</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.15</span>
<span class="k">assert</span> <span class="o">-</span><span class="n">hidden_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">/</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.25</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
<span class="k">assert</span> <span class="n">hidden_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1</span>
</pre></div>
</div>
</li>
<li><a id="org119f9e9"></a>The Final Block<br>
<div class="outline-text-5" id="text-org119f9e9">
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">final_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">1.0</span>
<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">1.0</span>
<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.3</span>
<span class="k">assert</span> <span class="n">final_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.6</span>
</pre></div>
</div>
</li>
<li><a id="org454b636"></a>The Whole Thing<br>
<div class="outline-text-5" id="text-org454b636">
<div class="highlight">
<pre><span></span><span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">disc_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">disc_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.25</span>
<span class="k">assert</span> <span class="n">disc_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Success!"</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orgd808d27">
<h3 id="orgd808d27">Training The Model</h3>
<div class="outline-text-3" id="text-orgd808d27">
<p>Remember that these are your parameters:</p>
<ul class="org-ul">
<li>criterion: the loss function</li>
<li>n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>z_dim: the dimension of the noise vector</li>
<li>display_step: how often to display/visualize the images</li>
<li>batch_size: the number of images per forward/backward pass</li>
<li>lr: the learning rate</li>
<li>beta_1, beta_2: the momentum term</li>
<li>device: the device type</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org6da4cfb">
<h4 id="org6da4cfb">Set Up The Data</h4>
<div class="outline-text-4" id="text-org6da4cfb">
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="c1"># A learning rate of 0.0002 works well on DCGAN</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0002</span>

<span class="c1"># These parameters control the optimizer's momentum, which you can read more about here:</span>
<span class="c1"># https://distill.pub/2017/momentum/ but you donâ€™t need to worry about it for this course!</span>
<span class="n">beta_1</span> <span class="o">=</span> <span class="mf">0.5</span> 
<span class="n">beta_2</span> <span class="o">=</span> <span class="mf">0.999</span>
<span class="n">device</span> <span class="o">=</span> <span class="s1">'cuda'</span>

<span class="c1"># You can tranform the image values to be between -1 and 1 (the range of the tanh activation)</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,)),</span>
<span class="p">])</span>

<span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/pytorch-data/MNIST"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">MNIST</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org281fd86">
<h4 id="org281fd86">Set Up the GAN</h4>
<div class="outline-text-4" id="text-org281fd86">
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">gen_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">))</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
<span class="n">disc_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="n">beta_1</span><span class="p">,</span> <span class="n">beta_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgd61636f">
<h4 id="orgd61636f">A Weight Initializer</h4>
<div class="outline-text-4" id="text-orgd61636f">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">initial_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="sd">"""Initialize the weights to the normal distribution</span>

<span class="sd">     - mean 0</span>
<span class="sd">     - standard deviation 0.02</span>

<span class="sd">    Args:</span>
<span class="sd">     m: layer whose weights to initialize</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">gen</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
<span class="n">disc</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">initial_weights</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orga91f155">
<h4 id="orga91f155">Train it</h4>
<div class="outline-text-4" id="text-orga91f155">
<p>For each epoch, you will process the entire dataset in batches. For every batch, you will update the discriminator and generator. Then, you can see DCGAN's results!</p>
<p>Here's roughly the progression you should be expecting. On GPU this takes about 30 seconds per thousand steps. On CPU, this can take about 8 hours per thousand steps. You might notice that in the image of Step 5000, the generator is disproprotionately producing things that look like ones. If the discriminator didn't learn to detect this imbalance quickly enough, then the generator could just produce more ones. As a result, it may have ended up tricking the discriminator so well that there would be no more improvement, known as mode collapse.</p>
<div class="highlight">
<pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">cur_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">display_step</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">mean_generator_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mean_discriminator_loss</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">discriminator_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">best_loss</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
<span class="n">best_step</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">best_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/gans/mnist-dcgan/best_model.pth"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>

<span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="c1"># Dataloader returns the batches</span>
        <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="c1">## Update discriminator ##</span>
            <span class="n">disc_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">fake_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">fake</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">fake_noise</span><span class="p">)</span>
            <span class="n">disc_fake_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
            <span class="n">disc_fake_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">))</span>
            <span class="n">disc_real_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
            <span class="n">disc_real_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">disc_real_pred</span><span class="p">))</span>
            <span class="n">disc_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">disc_fake_loss</span> <span class="o">+</span> <span class="n">disc_real_loss</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

            <span class="c1"># Keep track of the average discriminator loss</span>
            <span class="n">mean_discriminator_loss</span> <span class="o">+=</span> <span class="n">disc_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">display_step</span>
            <span class="c1"># Update gradients</span>
            <span class="n">disc_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="c1"># Update optimizer</span>
            <span class="n">disc_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1">## Update generator ##</span>
            <span class="n">gen_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">fake_noise_2</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
            <span class="n">fake_2</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">fake_noise_2</span><span class="p">)</span>
            <span class="n">disc_fake_pred</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fake_2</span><span class="p">)</span>
            <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">disc_fake_pred</span><span class="p">))</span>
            <span class="n">gen_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">gen_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Keep track of the average generator loss</span>
            <span class="n">mean_generator_loss</span> <span class="o">+=</span> <span class="n">gen_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">display_step</span>
            <span class="k">if</span> <span class="n">mean_generator_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                <span class="n">best_loss</span><span class="p">,</span> <span class="n">best_step</span> <span class="o">=</span> <span class="n">mean_generator_loss</span><span class="p">,</span> <span class="n">cur_step</span>
                <span class="k">with</span> <span class="n">best_path</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">writer</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">writer</span><span class="p">)</span>
            <span class="c1">## Visualization code ##</span>
            <span class="k">if</span> <span class="n">cur_step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">cur_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, step </span><span class="si">{</span><span class="n">cur_step</span><span class="si">}</span><span class="s2">: Generator loss:"</span>
                        <span class="sa">f</span><span class="s2">" </span><span class="si">{</span><span class="n">mean_generator_loss</span><span class="si">}</span><span class="s2">, discriminator loss:"</span>
                        <span class="sa">f</span><span class="s2">" </span><span class="si">{</span><span class="n">mean_discriminator_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

                <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_step</span><span class="p">)</span>
                <span class="n">generator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_generator_loss</span><span class="p">)</span>
                <span class="n">discriminator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_discriminator_loss</span><span class="p">)</span>

                <span class="n">mean_generator_loss</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">mean_discriminator_loss</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">cur_step</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
<pre class="example">
Started: 2021-04-21 12:45:12.452739
Epoch 2, step 1000: Generator loss: 1.2671969079673289, discriminator loss: 0.43014343224465823
Epoch 4, step 2000: Generator loss: 1.1353899730443968, discriminator loss: 0.5306872705817226
Epoch 6, step 3000: Generator loss: 0.8764803466945883, discriminator loss: 0.611450107574464
Epoch 8, step 4000: Generator loss: 0.7747784045338618, discriminator loss: 0.6631499938964849
Epoch 10, step 5000: Generator loss: 0.7640163034200661, discriminator loss: 0.6734729865789411
Epoch 12, step 6000: Generator loss: 0.7452541967928404, discriminator loss: 0.6805261079072958
Epoch 14, step 7000: Generator loss: 0.7337032879889016, discriminator loss: 0.6874966211915009
Epoch 17, step 8000: Generator loss: 0.7245009585618979, discriminator loss: 0.6908933531045917
Epoch 19, step 9000: Generator loss: 0.7180560626983646, discriminator loss: 0.6936621717810626
Epoch 21, step 10000: Generator loss: 0.7115822317004211, discriminator loss: 0.695760274052621
Epoch 23, step 11000: Generator loss: 0.7090291924774644, discriminator loss: 0.6962701203227039
Epoch 25, step 12000: Generator loss: 0.7059894913136957, discriminator loss: 0.6973492541313167
Epoch 27, step 13000: Generator loss: 0.7030480077862743, discriminator loss: 0.6978999735713001
Epoch 29, step 14000: Generator loss: 0.7028095332086096, discriminator loss: 0.6974007876515396
Epoch 31, step 15000: Generator loss: 0.7027116653919212, discriminator loss: 0.6965595571994787
Epoch 34, step 16000: Generator loss: 0.7005282629728309, discriminator loss: 0.6962912415862079
Epoch 36, step 17000: Generator loss: 0.7007142878770828, discriminator loss: 0.6961965024471283
Epoch 38, step 18000: Generator loss: 0.699474583208561, discriminator loss: 0.6952810400128371
Epoch 40, step 19000: Generator loss: 0.6989677719473828, discriminator loss: 0.6954642050266268
Epoch 42, step 20000: Generator loss: 0.6977452509403238, discriminator loss: 0.695180906951427
Epoch 44, step 21000: Generator loss: 0.6973587237596515, discriminator loss: 0.6950308464765543
Epoch 46, step 22000: Generator loss: 0.6960379970669743, discriminator loss: 0.6949119175076485
Epoch 49, step 23000: Generator loss: 0.6957966268062581, discriminator loss: 0.6948324624896048
Epoch 51, step 24000: Generator loss: 0.6958502059578898, discriminator loss: 0.6945331234931943
Epoch 53, step 25000: Generator loss: 0.6954856168627734, discriminator loss: 0.6943869084119801
Epoch 55, step 26000: Generator loss: 0.6957543395757682, discriminator loss: 0.694317172288894
Epoch 57, step 27000: Generator loss: 0.6947923063635825, discriminator loss: 0.694082073867321
Epoch 59, step 28000: Generator loss: 0.6945026598572728, discriminator loss: 0.6939926172494871
Epoch 61, step 29000: Generator loss: 0.6947789136767392, discriminator loss: 0.6938506522774704
Epoch 63, step 30000: Generator loss: 0.6946699734926227, discriminator loss: 0.6937169924378406
Epoch 66, step 31000: Generator loss: 0.6944284628629694, discriminator loss: 0.6936815274357805
Epoch 68, step 32000: Generator loss: 0.6940396347641948, discriminator loss: 0.6935891906023032
Epoch 70, step 33000: Generator loss: 0.6946771386265761, discriminator loss: 0.6937210547327995
Epoch 72, step 34000: Generator loss: 0.693429798424244, discriminator loss: 0.6937174627780922
Epoch 74, step 35000: Generator loss: 0.6937471128702157, discriminator loss: 0.6935204346776015
Epoch 76, step 36000: Generator loss: 0.6938841561675072, discriminator loss: 0.6934832554459566
Epoch 78, step 37000: Generator loss: 0.6934520475268362, discriminator loss: 0.6934578058719627
Epoch 81, step 38000: Generator loss: 0.6936635475754732, discriminator loss: 0.6934186050295835
Epoch 83, step 39000: Generator loss: 0.6936795052289972, discriminator loss: 0.6935187472105031
Epoch 85, step 40000: Generator loss: 0.6933113215565679, discriminator loss: 0.6933534587025645
Epoch 87, step 41000: Generator loss: 0.6934976277351385, discriminator loss: 0.6933284662365923
Epoch 89, step 42000: Generator loss: 0.6933313971757892, discriminator loss: 0.693348657488824
Epoch 91, step 43000: Generator loss: 0.6937436528205883, discriminator loss: 0.6933502901792529
Epoch 93, step 44000: Generator loss: 0.6943431540131578, discriminator loss: 0.6933887023925772
Epoch 95, step 45000: Generator loss: 0.6938722513914105, discriminator loss: 0.6932663491368296
Epoch 98, step 46000: Generator loss: 0.6933276618123067, discriminator loss: 0.6934270900487906
Ended: 2021-04-21 13:06:00.256725
Elapsed: 0:20:47.803986
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org882f085">
<h3 id="org882f085">Looking at the Final model.</h3>
<div class="outline-text-3" id="text-org882f085">
<div class="highlight">
<pre><span></span><span class="n">fake_noise</span> <span class="o">=</span> <span class="n">make_some_noise</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">best_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">best_path</span><span class="p">)</span>
<span class="n">fake</span> <span class="o">=</span> <span class="n">best_model</span><span class="p">(</span><span class="n">fake_noise</span><span class="p">)</span>
<span class="n">plot_image</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">fake</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">"fake_digits.png"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Fake Digits"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="fake_digits.png" src="posts/gans/cnn-gan/fake_digits.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">plot_image</span><span class="p">(</span><span class="n">real</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">"real_digits.png"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Real Digits"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="real_digits.png" src="posts/gans/cnn-gan/real_digits.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="n">plotting</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
    <span class="s2">"Step"</span><span class="p">:</span> <span class="n">steps</span><span class="p">,</span>
    <span class="s2">"Generator Loss"</span><span class="p">:</span> <span class="n">generator_losses</span><span class="p">,</span>
    <span class="s2">"Discriminator Loss"</span><span class="p">:</span> <span class="n">discriminator_losses</span>
<span class="p">})</span>

<span class="n">best</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">plotting</span><span class="p">[</span><span class="s2">"Generator Loss"</span><span class="p">]</span><span class="o">.</span><span class="n">argmin</span><span class="p">()]</span>
<span class="n">best_line</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">best</span><span class="o">.</span><span class="n">Step</span><span class="p">)</span>
<span class="n">gen_plot</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Step"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Generator Loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">disc_plot</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Step"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Discriminator Loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">gen_plot</span> <span class="o">*</span> <span class="n">disc_plot</span> <span class="o">*</span> <span class="n">best_line</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Training Losses"</span><span class="p">,</span>
                                               <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
                                               <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
                                               <span class="n">ylabel</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">,</span>
                                               <span class="n">fontscale</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">fontscale</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"losses"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/cnn-gan/losses.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
</div>
<div class="outline-2" id="outline-container-org0de3cd6">
<h2 id="org0de3cd6">End</h2>
<div class="outline-text-2" id="text-org0de3cd6"></div>
<div class="outline-3" id="outline-container-orgc148eb2">
<h3 id="orgc148eb2">Sources</h3>
<div class="outline-text-3" id="text-orgc148eb2">
<ul class="org-ul">
<li>Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434. 2015 Nov 19. (<a href="https://arxiv.org/pdf/1511.06434v1.pdf">PDF</a>)</li>
</ul>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/pytorch/pytorch-linear-regression/">PyTorch Linear Regression</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/pytorch/pytorch-linear-regression/" rel="bookmark"><time class="published dt-published" datetime="2021-04-10T16:05:44-07:00" itemprop="datePublished" title="2021-04-10 16:05">2021-04-10 16:05</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/pytorch/pytorch-linear-regression/#orgcce1fa1">Beginning</a>
<ul>
<li><a href="posts/pytorch/pytorch-linear-regression/#orgc1fc083">Imports</a></li>
<li><a href="posts/pytorch/pytorch-linear-regression/#org2ddd378">Set Up</a></li>
</ul>
</li>
<li><a href="posts/pytorch/pytorch-linear-regression/#org6e0c049">Middle</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgcce1fa1">
<h2 id="orgcce1fa1">Beginning</h2>
<div class="outline-text-2" id="text-orgcce1fa1"></div>
<div class="outline-3" id="outline-container-orgc1fc083">
<h3 id="orgc1fc083">Imports</h3>
<div class="outline-text-3" id="text-orgc1fc083">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># local stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org2ddd378">
<h3 id="org2ddd378">Set Up</h3>
<div class="outline-text-3" id="text-org2ddd378">
<div class="highlight">
<pre><span></span><span class="n">random_generator</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2021</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">slug</span> <span class="o">=</span> <span class="s2">"pytorch-linear-regression"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/pytorch/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">start</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">stop</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">,</span> <span class="n">uniform</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Create a random sample</span>

<span class="sd">    Args:</span>
<span class="sd">     start: lowest allowed value</span>
<span class="sd">     stop: highest allowed value</span>
<span class="sd">     shape: shape for the final array (just an int for single values)</span>
<span class="sd">     uniform: use the uniform distribution instead of the standard normal</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">uniform</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_generator</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">start</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">stop</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span> <span class="o">*</span> <span class="n">random_generator</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="n">start</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org6e0c049">
<h2 id="org6e0c049">Middle</h2>
<div class="outline-text-2" id="text-org6e0c049">
<div class="highlight">
<pre><span></span><span class="n">SAMPLES</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">X_RANGE</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">x_values</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="o">-</span><span class="n">X_RANGE</span><span class="p">,</span> <span class="n">X_RANGE</span><span class="p">,</span> <span class="n">SAMPLES</span><span class="p">)</span>
<span class="n">SLOPE</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">INTERCEPT</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">SAMPLES</span><span class="p">,</span> <span class="n">uniform</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">y_values</span> <span class="o">=</span> <span class="n">SLOPE</span> <span class="o">*</span> <span class="n">x_values</span> <span class="o">+</span> <span class="n">INTERCEPT</span> <span class="o">+</span> <span class="n">noise</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">data_frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x_values</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">y_values</span><span class="p">))</span>
<span class="n">first</span><span class="p">,</span> <span class="n">last</span> <span class="o">=</span> <span class="n">x_values</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_values</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">line_frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="n">first</span><span class="p">,</span> <span class="n">last</span><span class="p">],</span>
         <span class="n">Y</span><span class="o">=</span><span class="p">[</span><span class="n">SLOPE</span> <span class="o">*</span> <span class="n">first</span> <span class="o">+</span> <span class="n">INTERCEPT</span><span class="p">,</span>
            <span class="n">SLOPE</span> <span class="o">*</span> <span class="n">last</span> <span class="o">+</span> <span class="n">INTERCEPT</span><span class="p">]))</span>
<span class="n">line_plot</span> <span class="o">=</span> <span class="n">line_frame</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
<span class="n">data_plot</span> <span class="o">=</span> <span class="n">data_frame</span><span class="o">.</span><span class="n">hvplot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Y"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Sample Data"</span><span class="p">,</span>
                                      <span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">tan</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_plot</span> <span class="o">*</span> <span class="n">line_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">fontscale</span>
<span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"sample_data"</span><span class="p">)()</span>
</pre></div>
<object data="posts/pytorch/pytorch-linear-regression/sample_data.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">XY</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">is_tensor</span><span class="p">(</span><span class="n">index</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">"x"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="s2">"y"</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]}</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">XY</span><span class="p">(</span><span class="n">x_values</span><span class="p">,</span> <span class="n">y_values</span><span class="p">)</span>
<span class="n">loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/gans/mnist-gan/">MNIST GAN</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/gans/mnist-gan/" rel="bookmark"><time class="published dt-published" datetime="2021-04-06T17:48:17-07:00" itemprop="datePublished" title="2021-04-06 17:48">2021-04-06 17:48</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/gans/mnist-gan/#org3c0e743">Beginning</a>
<ul>
<li><a href="posts/gans/mnist-gan/#org43f55f8">Imports</a></li>
<li><a href="posts/gans/mnist-gan/#org15d69f9">Some Setup</a></li>
</ul>
</li>
<li><a href="posts/gans/mnist-gan/#orgdf2583d">Middle</a>
<ul>
<li><a href="posts/gans/mnist-gan/#orgdc5ac8c">The MNIST Dataset</a></li>
<li><a href="posts/gans/mnist-gan/#org9182168">The Generator</a>
<ul>
<li><a href="posts/gans/mnist-gan/#org6ea95f3">Verify the generator block function</a></li>
<li><a href="posts/gans/mnist-gan/#orgb992f37">Building the Generator Class</a></li>
<li><a href="posts/gans/mnist-gan/#org8f9d1d7">Verify the Generator Class</a></li>
</ul>
</li>
<li><a href="posts/gans/mnist-gan/#org30f3386">Noise</a>
<ul>
<li><a href="posts/gans/mnist-gan/#org2c07608">Verify the noise vector function</a></li>
</ul>
</li>
<li><a href="posts/gans/mnist-gan/#org6dd9fc1">The Discriminator</a>
<ul>
<li><a href="posts/gans/mnist-gan/#orga27f2bd">Verify the discriminator block function</a></li>
<li><a href="posts/gans/mnist-gan/#orge5d47b4">The Discriminator Class</a></li>
</ul>
</li>
<li><a href="posts/gans/mnist-gan/#org68476b3">Training</a>
<ul>
<li><a href="posts/gans/mnist-gan/#orgcd1b715">Set your parameters</a></li>
<li><a href="posts/gans/mnist-gan/#org93096d5">Load MNIST dataset as tensors</a></li>
<li><a href="posts/gans/mnist-gan/#org0482b62">Generator Loss</a></li>
<li><a href="posts/gans/mnist-gan/#org5e49a93">All Together</a></li>
</ul>
</li>
<li><a href="posts/gans/mnist-gan/#org24d4dcf">Looking at the Final model.</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3c0e743">
<h2 id="org3c0e743">Beginning</h2>
<div class="outline-text-2" id="text-org3c0e743"></div>
<div class="outline-3" id="outline-container-org43f55f8">
<h3 id="org43f55f8">Imports</h3>
<div class="outline-text-3" id="text-org43f55f8">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torchvision.datasets</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">make_grid</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">pyplot</span>
<span class="kn">import</span> <span class="nn">pandas</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># local code</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org15d69f9">
<h3 id="org15d69f9">Some Setup</h3>
<div class="outline-text-3" id="text-org15d69f9">
<p>First we'll set the manual seed to make this reproducible.</p>
<div class="highlight">
<pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
<p>This is a convenience object to time the training.</p>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
<p>This is for plotting.</p>
<div class="highlight">
<pre><span></span><span class="n">slug</span> <span class="o">=</span> <span class="s2">"mnist-gan"</span>

<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/gans/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span>
                           <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">,</span> <span class="s2">"sizing_mode"</span><span class="p">],</span>
                  <span class="n">defaults</span><span class="o">=</span><span class="p">[</span>
                      <span class="mi">900</span><span class="p">,</span>
                      <span class="mi">556</span><span class="p">,</span>
                      <span class="mi">2</span><span class="p">,</span>
                      <span class="s2">"#ddb377"</span><span class="p">,</span>
                      <span class="s2">"#4687b7"</span><span class="p">,</span>
                      <span class="s2">"#ce7b6d"</span><span class="p">,</span>
                      <span class="s2">"scale_both"</span><span class="p">,</span>
                  <span class="p">])()</span>

<span class="n">GANParts</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"GANParts"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"generator"</span><span class="p">,</span> <span class="s2">"generator_optimizer"</span><span class="p">,</span>
                                   <span class="s2">"discriminator"</span><span class="p">,</span> <span class="s2">"discriminator_optimizer"</span><span class="p">])</span>

<span class="n">PlotData</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"PlotData"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"steps"</span><span class="p">,</span> <span class="s2">"generator_losses"</span><span class="p">,</span>
                                   <span class="s2">"discriminator_losses"</span><span class="p">,</span> <span class="s2">"best_loss"</span><span class="p">])</span>

<span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/gans/mnist.pth"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">INFINITY</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">"inf"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgdf2583d">
<h2 id="orgdf2583d">Middle</h2>
<div class="outline-text-2" id="text-orgdf2583d"></div>
<div class="outline-3" id="outline-container-orgdc5ac8c">
<h3 id="orgdc5ac8c">The MNIST Dataset</h3>
<div class="outline-text-3" id="text-orgdc5ac8c">
<p>The training images we will be using are from a dataset called <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>. The dataset contains 60,000 images of handwritten digits, from 0 to 9.</p>
<p>The images are 28 pixels x 28 pixels in size. The small size of its images makes MNIST ideal for simple training. Additionally, these images are also in black-and-white so only one dimension, or "color channel", is needed to represent them. Pytorch has a <a href="https://pytorch.org/vision/0.8/datasets.html#mnist">version of it</a> ready-made for their system so we'll use theirs.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org9182168">
<h3 id="org9182168">The Generator</h3>
<div class="outline-text-3" id="text-org9182168">
<p>The first step is to build the generator component.</p>
<p>We'll start by creating a function to make a single layer/block for the generator's neural network. Each block should include a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear transformation</a> (\(y=xA^T + b\)) to the input to another shape, <a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html">batch normalization</a> for stabilization, and finally a non-linear activation function (<a href="https://pytorch.org/docs/master/generated/torch.nn.ReLU.html">ReLU</a> in this case).</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">generator_block</span><span class="p">(</span><span class="n">input_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Creates a block of the generator's neural network</span>

<span class="sd">    Args:</span>
<span class="sd">      input_features: the dimension of the input vector</span>
<span class="sd">      output_features: the dimension of the output vector</span>

<span class="sd">    Returns:</span>
<span class="sd">       a generator neural network layer, with a linear transformation </span>
<span class="sd">         followed by a batch normalization and then a relu activation</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">output_features</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org6ea95f3">
<h4 id="org6ea95f3">Verify the generator block function</h4>
<div class="outline-text-4" id="text-org6ea95f3">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_gen_block</span><span class="p">(</span><span class="n">in_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">out_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                   <span class="n">test_rows</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Test the generator block creator</span>

<span class="sd">    Args:</span>
<span class="sd">     in_features: number of features for the block input</span>
<span class="sd">     out_features: the final number of features for it to output</span>
<span class="sd">     test_rows: how many rows to put in the test Tensor</span>

<span class="sd">    Raises:</span>
<span class="sd">     AssertionError: something isn't right</span>
<span class="sd">    """</span>
    <span class="n">block</span> <span class="o">=</span> <span class="n">generator_block</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="c1"># Check the three parts</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">block</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">block</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span>
    <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">block</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span>

    <span class="c1"># Check the output shape</span>
    <span class="n">test_output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">test_rows</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">test_rows</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="c1"># check the normalization</span>
    <span class="k">assert</span> <span class="mf">0.65</span> <span class="o">&gt;</span> <span class="n">test_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.55</span>
    <span class="k">return</span>

<span class="n">test_gen_block</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">test_gen_block</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb992f37">
<h4 id="orgb992f37">Building the Generator Class</h4>
<div class="outline-text-4" id="text-orgb992f37">
<p>Now that we have the block-builder we can define our Generator network. It's going to contain a sequence of blocks output by our block-building function and a final two layers that use the linear transformation again, but don't apply normalization and use a <a href="https://pytorch.org/docs/master/generated/torch.nn.Sigmoid.html">Sigmoid Function</a> instead of the ReLU. Each block will have an output double that of the previous one.</p>
<div class="figure">
<p><img alt="generator.png" src="posts/gans/mnist-gan/generator.png"></p>
</div>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""Generator Class</span>

<span class="sd">    Args:</span>
<span class="sd">      input_dimension: the dimension of the noise vector</span>
<span class="sd">      image_dimension: the dimension of the images, fitted for the dataset used</span>
<span class="sd">        (MNIST images are 28 x 28 = 784 so that is the default)</span>
<span class="sd">      hidden_dimension: the initial hidden-layer dimension</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">image_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span>
                 <span class="n">hidden_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">generator_block</span><span class="p">(</span><span class="n">input_dimension</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="p">),</span>
            <span class="n">generator_block</span><span class="p">(</span><span class="n">hidden_dimension</span><span class="p">,</span> <span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">generator_block</span><span class="p">(</span><span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">generator_block</span><span class="p">(</span><span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">8</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">image_dimension</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">noise</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""</span>
<span class="sd">       Method for a forward pass of the generator</span>

<span class="sd">       Args:</span>
<span class="sd">        noise: a noise tensor with dimensions (n_samples, z_dim)</span>

<span class="sd">       Returns: </span>
<span class="sd">        generated images.</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org8f9d1d7">
<h4 id="org8f9d1d7">Verify the Generator Class</h4>
<div class="outline-text-4" id="text-org8f9d1d7">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">im_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                   <span class="n">num_test</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Test the Generator Class</span>

<span class="sd">    Args:</span>
<span class="sd">     z_dim: the size of the input</span>
<span class="sd">     im_dim: the size of the image</span>
<span class="sd">     hidden_dim: the size of the initial hidden layer</span>

<span class="sd">    Raises:</span>
<span class="sd">     AssertionError: something is wrong</span>
<span class="sd">    """</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">im_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">generator</span>

    <span class="c1"># Check there are six modules in the sequential part</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span>
    <span class="n">test_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="n">test_output</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

    <span class="c1"># Check that the output shape is correct</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">im_dim</span><span class="p">)</span>

    <span class="c1"># Chechk the output</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">test_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"Make sure to use a sigmoid"</span>
    <span class="k">assert</span> <span class="n">test_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">"Don't use a block in your solution"</span>
    <span class="k">assert</span> <span class="mf">0.15</span> <span class="o">&gt;</span> <span class="n">test_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">"Don't use batchnorm here"</span>
    <span class="k">return</span>

<span class="n">test_generator</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">test_generator</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">24</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org30f3386">
<h3 id="org30f3386">Noise</h3>
<div class="outline-text-3" id="text-org30f3386">
<p>To be able to use the generator, we will need to be able to create noise vectors. The noise vector <code>z</code> has the important role of making sure the images generated from the same class don't all look the same â€“ think of it as a random seed. You will generate it randomly using PyTorch by sampling random numbers from the normal distribution. Since multiple images will be processed per pass, you will generate all the noise vectors at once.</p>
<p>Note that whenever you create a new tensor using torch.ones, torch.zeros, or <a href="https://pytorch.org/docs/master/generated/torch.randn.html">torch.randn</a>, you either need to create it on the target device, e.g. <code>torch.ones(3, 3, device=device)</code>, or move it onto the target device using <code>torch.ones(3, 3).to(device)</code>. You do not need to do this if you're creating a tensor by manipulating another tensor or by using a variation that defaults the device to the input, such as <code>torch.ones_like</code>. In general, use <code>torch.ones_like</code> and <code>torch.zeros_like</code> instead of <code>torch.ones</code> or <code>torch.zeros</code> where possible.</p>
<div class="highlight">
<pre><span></span><span class="n">get_noise</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-org2c07608">
<h4 id="org2c07608">Verify the noise vector function</h4>
<div class="outline-text-4" id="text-org2c07608">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_get_noise</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">'cpu'</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">get_noise</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Make sure a normal distribution was used</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">0.01</span>
    <span class="k">assert</span> <span class="nb">str</span><span class="p">(</span><span class="n">noise</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">test_get_noise</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org6dd9fc1">
<h3 id="org6dd9fc1">The Discriminator</h3>
<div class="outline-text-3" id="text-org6dd9fc1">
<p>The second component that you need to construct is the discriminator. As with the generator component, you will start by creating a function that builds a neural network block for the discriminator.</p>
<p><b>Note: You use <a href="https://pytorch.org/docs/master/generated/torch.nn.LeakyReLU.html">leaky ReLUs</a> to prevent the "dying ReLU" problem, which refers to the phenomenon where the parameters stop changing due to consistently negative values passed to a ReLU, which result in a zero gradient.</b></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_discriminator_block</span><span class="p">(</span><span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                            <span class="n">negative_slope</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">:</span>
    <span class="sd">"""Create the Discriminator block</span>

<span class="sd">    Args:</span>
<span class="sd">      input_dim: the dimension of the input vector, a scalar</span>
<span class="sd">      output_dim: the dimension of the output vector, a scalar</span>
<span class="sd">      negative_slope: angle for the negative slope</span>

<span class="sd">    Returns:</span>
<span class="sd">       a discriminator neural network layer, with a linear transformation </span>
<span class="sd">         followed by an nn.LeakyReLU activation with negative slope of 0.2 </span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">negative_slope</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga27f2bd">
<h4 id="orga27f2bd">Verify the discriminator block function</h4>
<div class="outline-text-4" id="text-orga27f2bd">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_disc_block</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">num_test</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="n">block</span> <span class="o">=</span> <span class="n">get_discriminator_block</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="c1"># Check there are two parts</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">block</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="n">test_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>
    <span class="n">test_output</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>

    <span class="c1"># Check that the shape is right</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>

    <span class="c1"># Check that the LeakyReLU slope is about 0.2</span>
    <span class="k">assert</span> <span class="o">-</span><span class="n">test_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">/</span> <span class="n">test_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.1</span>
    <span class="k">assert</span> <span class="o">-</span><span class="n">test_output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">/</span> <span class="n">test_output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.3</span>
    <span class="k">assert</span> <span class="n">test_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.3</span>
    <span class="k">assert</span> <span class="n">test_output</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.5</span>

<span class="n">test_disc_block</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">test_disc_block</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orge5d47b4">
<h4 id="orge5d47b4">The Discriminator Class</h4>
<div class="outline-text-4" id="text-orge5d47b4">
<p>The discriminator class holds 2 values:</p>
<ul class="org-ul">
<li>The image dimension</li>
<li>The hidden dimension</li>
</ul>
<p>The discriminator will build a neural network with 4 layers. It will start with the image tensor and transform it until it returns a single number (1-dimension tensor) output. This output classifies whether an image is fake or real. Note that you do not need a sigmoid after the output layer since it is included in the loss function. Finally, to use your discrimator's neural network you are given a forward pass function that takes in an image tensor to be classified.</p>
<div class="highlight">
<pre><span></span><span class="k">class</span> <span class="nc">Discriminator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">"""The Discriminator Class</span>

<span class="sd">    Args:</span>
<span class="sd">       im_dim: the dimension of the images, fitted for the dataset used, a scalar</span>
<span class="sd">           (MNIST images are 28x28 = 784 so that is your default)</span>
<span class="sd">       hidden_dim: the inner dimension, a scalar</span>
<span class="sd">    """</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">im_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">get_discriminator_block</span><span class="p">(</span><span class="n">im_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">),</span>
            <span class="n">get_discriminator_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span>
            <span class="n">get_discriminator_block</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">"""forward pass of the discriminator</span>

<span class="sd">       Args:</span>
<span class="sd">           image: a flattened image tensor with dimension (im_dim)</span>

<span class="sd">       Returns a 1-dimension tensor representing fake/real.</span>
<span class="sd">       """</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">disc</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
<ul class="org-ul">
<li><a id="org9406fc8"></a>Verify the discriminator class<br>
<div class="outline-text-5" id="text-org9406fc8">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_discriminator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">num_test</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>

    <span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span><span class="o">.</span><span class="n">disc</span>

    <span class="c1"># Check there are three parts</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">disc</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>

    <span class="c1"># Check the linear layer is correct</span>
    <span class="n">test_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="n">test_output</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_test</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Don't use a block</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">disc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">)</span>

<span class="n">test_discriminator</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">test_discriminator</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org68476b3">
<h3 id="org68476b3">Training</h3>
<div class="outline-text-3" id="text-org68476b3">
<p>First, you will set your parameters:</p>
<ul class="org-ul">
<li>criterion: the loss function (<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=bcewithlogitsloss">BCEWithLogitsLoss</a>)</li>
<li>n_epochs: the number of times you iterate through the entire dataset when training</li>
<li>z_dim: the dimension of the noise vector</li>
<li>display_step: how often to display/visualize the images</li>
<li>batch_size: the number of images per forward/backward pass</li>
<li>lr: the learning rate</li>
<li>device: the device type, here using a GPU (which runs CUDA), not CPU</li>
</ul>
<p>Next, you will load the MNIST dataset as tensors using a dataloader.</p>
</div>
<div class="outline-4" id="outline-container-orgcd1b715">
<h4 id="orgcd1b715">Set your parameters</h4>
<div class="outline-text-4" id="text-orgcd1b715">
<div class="highlight">
<pre><span></span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">()</span>
<span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.00001</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org93096d5">
<h4 id="org93096d5">Load MNIST dataset as tensors</h4>
<div class="outline-text-4" id="text-org93096d5">
<div class="highlight">
<pre><span></span><span class="n">data_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/data/datasets/pytorch/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="n">data_path</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
<p>Now, you can initialize your generator, discriminator, and optimizers. Note that each optimizer only takes the parameters of one particular model, since we want each optimizer to optimize only one of the models.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">build_parts</span><span class="p">(</span><span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="n">z_dim</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">GANParts</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">gen_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
    <span class="n">disc_optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">GANParts</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">gen_optimizer</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">,</span> <span class="n">disc_optimizer</span><span class="p">)</span>

<span class="n">gen</span><span class="p">,</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">disc_opt</span> <span class="o">=</span> <span class="n">build_parts</span><span class="p">()</span>
</pre></div>
<p>This next bit is from <a href="https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu">https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu</a>.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">check_gpu</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">current_device</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">'Memory Usage:'</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Allocated: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span><span class="o">/</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">"</span>
          <span class="s2">" MB"</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Cached:   </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">/</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> MB"</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">check_gpu</span><span class="p">()</span>
</pre></div>
<pre class="example">
0
1
NVIDIA GeForce GTX 1070 Ti
Memory Usage:
Allocated: 7.92138671875 MB
Cached:   22.0 MB
</pre>
<p>Before you train your GAN, you will need to create functions to calculate the discriminator's loss and the generator's loss. This is how the discriminator and generator will know how they are doing and improve themselves. Since the generator is needed when calculating the discriminator's loss, you will need to call .detach() on the generator result to ensure that only the discriminator is updated!</p>
<p>Remember that you have already defined a loss function earlier (<code>criterion</code>) and you are encouraged to use <a href="https://pytorch.org/docs/master/generated/torch.ones_like.html?highlight=ones_like#torch.ones_like"><code>torch.ones_like</code></a> and <a href="https://pytorch.org/docs/master/generated/torch.zeros_like.html?highlight=zeros_like#torch.zeros_like"><code>torch.zeros_like</code></a> instead of <code>torch.ones</code> or <code>torch.zeros</code>. If you use <code>torch.ones</code> or <code>torch.zeros</code>, you'll need to pass <code>device=device</code> to them.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span> <span class="n">disc</span><span class="p">:</span> <span class="n">Discriminator</span><span class="p">,</span>
                  <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">,</span>
                  <span class="n">real</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                  <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Get the loss of the discriminator given inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">       gen: the generator model, which returns an image given z-dimensional noise</span>
<span class="sd">       disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span>
<span class="sd">       criterion: the loss function, which should be used to compare </span>
<span class="sd">              the discriminator's predictions to the ground truth reality of the images </span>
<span class="sd">              (e.g. fake = 0, real = 1)</span>
<span class="sd">       real: a batch of real images</span>
<span class="sd">       num_images: the number of images the generator should produce, </span>
<span class="sd">               which is also the length of the real images</span>
<span class="sd">       z_dim: the dimension of the noise vector, a scalar</span>
<span class="sd">       device: the device type</span>

<span class="sd">    Returns:</span>
<span class="sd">       disc_loss: a torch scalar loss value for the current batch</span>
<span class="sd">    """</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">fakes</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

    <span class="n">fake_prediction</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fakes</span><span class="p">)</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">fake_prediction</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">fake_prediction</span><span class="p">))</span>

    <span class="n">real_prediction</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
    <span class="n">real_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">real_prediction</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">real_prediction</span><span class="p">))</span>
    <span class="n">disc_loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">fake_loss</span> <span class="o">+</span> <span class="n">real_loss</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">disc_loss</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_disc_reasonable</span><span class="p">(</span><span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Don't use explicit casts to cuda - use the device argument</span>
    <span class="kn">import</span> <span class="nn">inspect</span><span class="o">,</span> <span class="nn">re</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getsource</span><span class="p">(</span><span class="n">get_disc_loss</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">"to\(.cuda.\)"</span><span class="p">,</span> <span class="n">lines</span><span class="p">))</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">"\.cuda\(\)"</span><span class="p">,</span> <span class="n">lines</span><span class="p">))</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="c1"># Multiply</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="n">disc_loss</span> <span class="o">=</span> <span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">disc_loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">gen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="c1"># Multiply</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">gen</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="mi">10</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="c1"># Multiply</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="mi">5</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>

    <span class="n">gen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
    <span class="n">disc</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
    <span class="n">disc_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">disc_loss</span> <span class="o">=</span> <span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">disc_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="mf">11.25</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.75</span><span class="p">))</span>
    <span class="k">return</span>

<span class="n">test_disc_reasonable</span><span class="p">()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span>
<span class="k">def</span> <span class="nf">test_disc_loss</span><span class="p">(</span><span class="n">max_tests</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
    <span class="n">disc_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>
        <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1">### Update discriminator ###</span>
        <span class="c1"># Zero out the gradient before backpropagation</span>
        <span class="n">disc_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Calculate discriminator loss</span>
        <span class="n">disc_loss</span> <span class="o">=</span> <span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span> <span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">disc_loss</span> <span class="o">-</span> <span class="mf">0.68</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">disc_loss</span>

        <span class="c1"># Update gradients</span>
        <span class="n">disc_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Check that they detached correctly</span>
        <span class="k">assert</span> <span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span>

        <span class="c1"># Update optimizer</span>
        <span class="n">old_weight</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">disc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">disc_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">new_weight</span> <span class="o">=</span> <span class="n">disc</span><span class="o">.</span><span class="n">disc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>

        <span class="c1"># Check that some discriminator weights changed</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">old_weight</span><span class="p">,</span> <span class="n">new_weight</span><span class="p">))</span>
        <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;=</span> <span class="n">max_tests</span><span class="p">:</span>
            <span class="k">break</span>

<span class="n">test_disc_loss</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org0482b62">
<h4 id="org0482b62">Generator Loss</h4>
<div class="outline-text-4" id="text-org0482b62">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">get_generator_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">:</span> <span class="n">Generator</span><span class="p">,</span>
                       <span class="n">disc</span><span class="p">:</span> <span class="n">Discriminator</span><span class="p">,</span>
                       <span class="n">criterion</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">,</span>
                       <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                       <span class="n">z_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"cuda"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">"""Calculates the loss for the generator</span>

<span class="sd">    Args:</span>
<span class="sd">       gen: the generator model, which returns an image given z-dimensional noise</span>
<span class="sd">       disc: the discriminator model, which returns a single-dimensional prediction of real/fake</span>
<span class="sd">       criterion: the loss function, which should be used to compare </span>
<span class="sd">              the discriminator's predictions to the ground truth reality of the images </span>
<span class="sd">              (e.g. fake = 0, real = 1)</span>
<span class="sd">       num_images: the number of images the generator should produce, </span>
<span class="sd">               which is also the length of the real images</span>
<span class="sd">       z_dim: the dimension of the noise vector, a scalar</span>
<span class="sd">       device: the device type</span>
<span class="sd">    Returns:</span>
<span class="sd">       gen_loss: a torch scalar loss value for the current batch</span>
<span class="sd">    """</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">fakes</span> <span class="o">=</span> <span class="n">gen</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">fake_prediction</span> <span class="o">=</span> <span class="n">disc</span><span class="p">(</span><span class="n">fakes</span><span class="p">)</span>
    <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">fake_prediction</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">fake_prediction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">gen_loss</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_gen_reasonable</span><span class="p">(</span><span class="n">num_images</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Don't use explicit casts to cuda - use the device argument</span>
    <span class="kn">import</span> <span class="nn">inspect</span><span class="o">,</span> <span class="nn">re</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="n">inspect</span><span class="o">.</span><span class="n">getsource</span><span class="p">(</span><span class="n">get_gen_loss</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">"to\(.cuda.\)"</span><span class="p">,</span> <span class="n">lines</span><span class="p">))</span> <span class="ow">is</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="sa">r</span><span class="s2">"\.cuda\(\)"</span><span class="p">,</span> <span class="n">lines</span><span class="p">))</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="c1"># Multiply</span>
    <span class="n">gen_loss_tensor</span> <span class="o">=</span> <span class="n">get_gen_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">gen_loss_tensor</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="c1">#Verify shape. Related to gen_noise parametrization</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">gen_loss_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>

    <span class="n">gen</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mul</span> <span class="c1"># Multiply</span>
    <span class="n">real</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">gen_loss_tensor</span> <span class="o">=</span> <span class="n">get_gen_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="s1">'cpu'</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">gen_loss_tensor</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="c1">#Verify shape. Related to gen_noise parametrization</span>
    <span class="k">assert</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">gen_loss_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">)</span>
    <span class="k">return</span>
<span class="n">test_gen_reasonable</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_gen_loss</span><span class="p">(</span><span class="n">num_images</span><span class="p">):</span>
    <span class="n">z_dim</span> <span class="o">=</span> <span class="mi">64</span>
    <span class="n">gen</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">(</span><span class="n">z_dim</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">disc</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> 
    <span class="n">disc_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

    <span class="n">gen_loss</span> <span class="o">=</span> <span class="n">get_generator_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">num_images</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

    <span class="c1"># Check that the loss is reasonable</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">gen_loss</span> <span class="o">-</span> <span class="mf">0.7</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.1</span>
    <span class="n">gen_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">old_weight</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">gen_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">new_weight</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span>
    <span class="k">assert</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">old_weight</span><span class="p">,</span> <span class="n">new_weight</span><span class="p">))</span>
<span class="n">test_gen_loss</span><span class="p">(</span><span class="mi">18</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5e49a93">
<h4 id="org5e49a93">All Together</h4>
<div class="outline-text-4" id="text-org5e49a93">
<p>For each epoch, you will process the entire dataset in batches. For every batch, you will need to update the discriminator and generator using their loss. Batches are sets of images that will be predicted on before the loss functions are calculated (instead of calculating the loss function after each image). Note that you may see a loss to be greater than 1, this is okay since binary cross entropy loss can be any positive number for a sufficiently confident wrong guess.</p>
<p>Itâ€™s also often the case that the discriminator will outperform the generator, especially at the start, because its job is easier. It's important that neither one gets too good (that is, near-perfect accuracy), which would cause the entire model to stop learning. Balancing the two models is actually remarkably hard to do in a standard GAN and something you will see more of in later lectures and assignments.</p>
<p>After you've submitted a working version with the original architecture, feel free to play around with the architecture if you want to see how different architectural choices can lead to better or worse GANs. For example, consider changing the size of the hidden dimension, or making the networks shallower or deeper by changing the number of layers.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">gen</span><span class="p">:</span> <span class="n">Generator</span><span class="o">=</span><span class="n">gen</span><span class="p">,</span>
          <span class="n">gen_opt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="o">=</span><span class="n">gen_opt</span><span class="p">,</span>
          <span class="n">disc</span><span class="p">:</span> <span class="n">Discriminator</span><span class="o">=</span><span class="n">disc</span><span class="p">,</span>
          <span class="n">disc_opt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="o">=</span><span class="n">disc_opt</span><span class="p">,</span>
          <span class="n">start_step</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
          <span class="n">best_loss</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">INFINITY</span><span class="p">,</span>
          <span class="n">test_the_generator</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">display_step</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4500</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PlotData</span><span class="p">:</span>
    <span class="sd">"""Train a batch</span>

<span class="sd">    Args:</span>
<span class="sd">     epochs: number of epochs to train</span>
<span class="sd">     gen: the Generator object</span>
<span class="sd">     gen_opt: optimizer for the generator</span>
<span class="sd">     disc: the Discriminator object</span>
<span class="sd">     disc_opt: the optimizer for the discriminator</span>
<span class="sd">     start_step: where we are in the training if this is called more than once</span>
<span class="sd">     best_loss: what the best loss for the generator has been so far</span>
<span class="sd">     test_the_generator: whether to double-check the generator is changing</span>
<span class="sd">     display_step: how often to emit messages</span>
<span class="sd">    """</span>
    <span class="n">current_step</span> <span class="o">=</span> <span class="n">start_step</span>
    <span class="n">mean_generator_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">mean_discriminator_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">generator_loss</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">error</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">generator_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">discriminator_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

            <span class="c1"># Dataloader returns the batches</span>
            <span class="k">for</span> <span class="n">real</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
                <span class="n">cur_batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">real</span><span class="p">)</span>

                <span class="c1"># Flatten the batch of real images from the dataset</span>
                <span class="n">real</span> <span class="o">=</span> <span class="n">real</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">cur_batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="c1">### Update discriminator ###</span>
                <span class="c1"># Zero out the gradients before backpropagation</span>
                <span class="n">disc_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

                <span class="c1"># Calculate discriminator loss</span>
                <span class="n">disc_loss</span> <span class="o">=</span> <span class="n">get_disc_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">real</span><span class="p">,</span>
                                          <span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

                <span class="c1"># Update gradients</span>
                <span class="n">disc_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="c1"># Update optimizer</span>
                <span class="n">disc_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># For testing purposes, to keep track of the generator weights</span>
                <span class="k">if</span> <span class="n">test_generator</span><span class="p">:</span>
                    <span class="n">old_generator_weights</span> <span class="o">=</span> <span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

                <span class="c1">### Update generator ###</span>
                <span class="n">gen_opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">generator_loss</span> <span class="o">=</span> <span class="n">get_generator_loss</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">disc</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">cur_batch_size</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
                <span class="n">generator_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">gen_opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

                <span class="c1"># check if it's the best so far</span>
                <span class="k">if</span> <span class="n">generator_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
                    <span class="n">best_loss</span> <span class="o">=</span> <span class="n">generator_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Saving model with best loss so far (</span><span class="si">{</span><span class="n">best_loss</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)"</span><span class="p">)</span>

                    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">MODEL_PATH</span><span class="p">)</span>
                <span class="c1"># For testing purposes, to check that your code changes the generator weights</span>
                <span class="k">if</span> <span class="n">test_generator</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mf">0.0000002</span> <span class="ow">or</span> <span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.0005</span> <span class="ow">and</span> <span class="n">epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
                        <span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">gen</span><span class="o">.</span><span class="n">generator</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="o">!=</span> <span class="n">old_generator_weights</span><span class="p">)</span>
                    <span class="k">except</span><span class="p">:</span>
                        <span class="n">error</span> <span class="o">=</span> <span class="kc">True</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s2">"Runtime tests have failed"</span><span class="p">)</span>

                <span class="c1"># Keep track of the average discriminator loss</span>
                <span class="n">mean_discriminator_loss</span> <span class="o">+=</span> <span class="n">disc_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">display_step</span>

                <span class="c1"># Keep track of the average generator loss</span>
                <span class="n">mean_generator_loss</span> <span class="o">+=</span> <span class="n">generator_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">display_step</span>

                <span class="k">if</span> <span class="n">current_step</span> <span class="o">%</span> <span class="n">display_step</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">current_step</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, step </span><span class="si">{</span><span class="n">current_step</span><span class="si">}</span><span class="s2">: Generator loss:"</span>
                            <span class="sa">f</span><span class="s2">" </span><span class="si">{</span><span class="n">mean_generator_loss</span><span class="si">}</span><span class="s2">, discriminator loss:"</span>
                            <span class="sa">f</span><span class="s2">" </span><span class="si">{</span><span class="n">mean_discriminator_loss</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
                    <span class="n">steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">current_step</span><span class="p">)</span>
                    <span class="n">generator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_generator_loss</span><span class="p">)</span>
                    <span class="n">discriminator_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_discriminator_loss</span><span class="p">)</span>

                    <span class="n">mean_generator_loss</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">mean_discriminator_loss</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">current_step</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">PlotData</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">generator_losses</span><span class="o">=</span><span class="n">generator_losses</span><span class="p">,</span>
                    <span class="n">discriminator_losses</span><span class="o">=</span><span class="n">discriminator_losses</span><span class="p">,</span> <span class="n">best_loss</span><span class="o">=</span><span class="n">best_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
<pre class="example">
Started: 2021-12-12 21:18:06.595695
Saving model with best loss so far (3.82)
Saving model with best loss so far (3.77)
Saving model with best loss so far (3.75)
Saving model with best loss so far (3.58)
Saving model with best loss so far (3.54)
Saving model with best loss so far (3.50)
Saving model with best loss so far (3.25)
Epoch 9, step 4500: Generator loss: 4.100310390790306, discriminator loss: 0.053616619475599675
Ended: 2021-12-12 21:19:19.963121
Elapsed: 0:01:13.367426
PlotData(steps=[4500], generator_losses=[4.100310390790306], discriminator_losses=[0.053616619475599675], best_loss=3.2485642433166504)
</pre>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">run_batch</span><span class="p">(</span><span class="n">parts</span><span class="p">:</span> <span class="n">GANParts</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">:</span> <span class="n">PlotData</span><span class="p">,</span>
              <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PlotData</span><span class="p">:</span>
    <span class="sd">"""Run a smaller batch of epochs</span>

<span class="sd">    Args:</span>
<span class="sd">     parts: the GAN parts</span>
<span class="sd">     plot_data: the accumulated output of the training</span>

<span class="sd">    Returns:</span>
<span class="sd">     updated plot_data</span>
<span class="sd">    """</span>
    <span class="n">next_step</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">plot_data</span><span class="o">.</span><span class="n">steps</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">steps</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">best_loss</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">gen</span><span class="o">=</span><span class="n">parts</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span>
                   <span class="n">gen_opt</span><span class="o">=</span><span class="n">parts</span><span class="o">.</span><span class="n">generator_optimizer</span><span class="p">,</span>
                   <span class="n">disc</span><span class="o">=</span><span class="n">parts</span><span class="o">.</span><span class="n">discriminator</span><span class="p">,</span>
                   <span class="n">disc_opt</span><span class="o">=</span><span class="n">parts</span><span class="o">.</span><span class="n">discriminator_optimizer</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                   <span class="n">start_step</span><span class="o">=</span><span class="n">next_step</span><span class="p">,</span> <span class="n">best_loss</span><span class="o">=</span><span class="n">best_loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">PlotData</span><span class="p">(</span>
        <span class="n">steps</span><span class="o">=</span><span class="n">plot_data</span><span class="o">.</span><span class="n">steps</span> <span class="o">+</span> <span class="n">output</span><span class="o">.</span><span class="n">steps</span><span class="p">,</span>
        <span class="n">generator_losses</span><span class="o">=</span><span class="n">plot_data</span><span class="o">.</span><span class="n">generator_losses</span> <span class="o">+</span> <span class="n">output</span><span class="o">.</span><span class="n">generator_losses</span><span class="p">,</span>
        <span class="n">discriminator_losses</span><span class="o">=</span><span class="p">(</span><span class="n">plot_data</span><span class="o">.</span><span class="n">discriminator_losses</span> <span class="o">+</span>
                              <span class="n">output</span><span class="o">.</span><span class="n">discriminator_losses</span><span class="p">),</span> <span class="n">best_loss</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">best_loss</span><span class="p">)</span>
</pre></div>
<p>At about one epoch a minute, this should take about an hour and forty minutes or so.</p>
<div class="highlight">
<pre><span></span><span class="n">parts</span> <span class="o">=</span> <span class="n">build_parts</span><span class="p">()</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">run_batch</span><span class="p">(</span><span class="n">parts</span><span class="p">,</span> <span class="n">PlotData</span><span class="p">([],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="n">best_loss</span><span class="o">=</span><span class="n">INFINITY</span><span class="p">))</span>
</pre></div>
<pre class="example">
Started: 2021-12-12 22:41:21.823164
Saving model with best loss so far (0.68)
Epoch 9, step 4500: Generator loss: 2.0671158762905333, discriminator loss: 0.2002916043450438
Epoch 19, step 9000: Generator loss: 3.894586259894906, discriminator loss: 0.07030286101831354
Epoch 28, step 13500: Generator loss: 4.128244651317588, discriminator loss: 0.07312827965741342
Epoch 38, step 18000: Generator loss: 3.679966155105171, discriminator loss: 0.11400978071076953
Epoch 47, step 22500: Generator loss: 3.396804347621069, discriminator loss: 0.1500528004517162
Epoch 57, step 27000: Generator loss: 3.1040636014938245, discriminator loss: 0.17772292008996066
Epoch 67, step 31500: Generator loss: 2.8721981742117144, discriminator loss: 0.20018046746320184
Epoch 76, step 36000: Generator loss: 2.6418062130610167, discriminator loss: 0.22749259825216356
Epoch 86, step 40500: Generator loss: 2.5520630269580473, discriminator loss: 0.2384359383367829
Epoch 95, step 45000: Generator loss: 2.2842552210754827, discriminator loss: 0.2856089066366352
Ended: 2021-12-12 22:53:26.813378
Elapsed: 0:12:04.990214
</pre>
<p>Twelve minutesâ€¦ something's wrong with my math (or my code).</p>
<div class="highlight">
<pre><span></span><span class="n">plot_data</span> <span class="o">=</span> <span class="n">run_batch</span><span class="p">(</span><span class="n">parts</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
<pre class="example">
Started: 2021-12-12 22:56:24.985796
Epoch 9, step 49500: Generator loss: 1.9961646406915445, discriminator loss: 0.3290849660535635
Epoch 19, step 54000: Generator loss: 1.8578781396812813, discriminator loss: 0.352255903386407
Epoch 28, step 58500: Generator loss: 1.7970310585498779, discriminator loss: 0.35997216095195933
Epoch 38, step 63000: Generator loss: 1.790595402267244, discriminator loss: 0.35704792838626415
Epoch 47, step 67500: Generator loss: 1.6337452937761876, discriminator loss: 0.399976011382207
Epoch 57, step 72000: Generator loss: 1.553067971309023, discriminator loss: 0.4149618665973359
Epoch 67, step 76500: Generator loss: 1.530716036346228, discriminator loss: 0.41230284125937433
Epoch 76, step 81000: Generator loss: 1.4763377503554027, discriminator loss: 0.42583107221126515
Epoch 86, step 85500: Generator loss: 1.4468076727655206, discriminator loss: 0.43238778846131376
Epoch 95, step 90000: Generator loss: 1.341741836388908, discriminator loss: 0.4629372137056456
Epoch 105, step 94500: Generator loss: 1.3321984058486116, discriminator loss: 0.4620412641101409
Epoch 115, step 99000: Generator loss: 1.2581868343618232, discriminator loss: 0.48384387395116984
Epoch 124, step 103500: Generator loss: 1.2193222048944967, discriminator loss: 0.4943520678679146
Epoch 134, step 108000: Generator loss: 1.210901861535182, discriminator loss: 0.4949321229921447
Epoch 143, step 112500: Generator loss: 1.1491395987007338, discriminator loss: 0.5172655161817865
Epoch 153, step 117000: Generator loss: 1.1057299662033748, discriminator loss: 0.5293599960472835
Epoch 163, step 121500: Generator loss: 1.0677826208141108, discriminator loss: 0.5450594869587152
Epoch 172, step 126000: Generator loss: 1.0314588887823974, discriminator loss: 0.5558468225730787
Epoch 182, step 130500: Generator loss: 0.988256526258255, discriminator loss: 0.5727653868463289
Epoch 191, step 135000: Generator loss: 0.9866233132415357, discriminator loss: 0.5702601793143482
Epoch 201, step 139500: Generator loss: 0.9552508686383555, discriminator loss: 0.5773511084649297
Epoch 211, step 144000: Generator loss: 0.9940739690330269, discriminator loss: 0.5602315128578074
Epoch 220, step 148500: Generator loss: 0.957331194546487, discriminator loss: 0.5762877901130254
Epoch 230, step 153000: Generator loss: 0.940105734388034, discriminator loss: 0.5758193738725437
Epoch 239, step 157500: Generator loss: 0.9115204359822786, discriminator loss: 0.5869797533286937
Epoch 249, step 162000: Generator loss: 0.9108314050965834, discriminator loss: 0.5894923475186025
Epoch 259, step 166500: Generator loss: 0.8571861512131169, discriminator loss: 0.6157774040169194
Epoch 268, step 171000: Generator loss: 0.9255197024875222, discriminator loss: 0.5831143510672784
Epoch 278, step 175500: Generator loss: 0.9227028174797725, discriminator loss: 0.5861976487901457
Epoch 287, step 180000: Generator loss: 0.9116632200082172, discriminator loss: 0.5921973378062267
Epoch 297, step 184500: Generator loss: 0.896604921857515, discriminator loss: 0.5983437673317067
Epoch 307, step 189000: Generator loss: 0.8876715467108619, discriminator loss: 0.5988295680549406
Epoch 316, step 193500: Generator loss: 0.8981850113338913, discriminator loss: 0.596900998810927
Epoch 326, step 198000: Generator loss: 0.9132995079225976, discriminator loss: 0.5932699956099183
Saving model with best loss so far (0.67)
Saving model with best loss so far (0.66)
Saving model with best loss so far (0.66)
Saving model with best loss so far (0.65)
Saving model with best loss so far (0.64)
Saving model with best loss so far (0.63)
Saving model with best loss so far (0.62)
Epoch 335, step 202500: Generator loss: 0.8218274900118518, discriminator loss: 0.6400571531719623
Epoch 345, step 207000: Generator loss: 0.9687661434412017, discriminator loss: 0.5792819880843164
Saving model with best loss so far (0.62)
Saving model with best loss so far (0.61)
Saving model with best loss so far (0.59)
Saving model with best loss so far (0.59)
Saving model with best loss so far (0.58)
Saving model with best loss so far (0.58)
Saving model with best loss so far (0.57)
Saving model with best loss so far (0.56)
Saving model with best loss so far (0.56)
Saving model with best loss so far (0.53)
Saving model with best loss so far (0.51)
Saving model with best loss so far (0.50)
Epoch 355, step 211500: Generator loss: 0.7063771485355128, discriminator loss: 0.7654304582807753
Epoch 364, step 216000: Generator loss: 0.8059421989785318, discriminator loss: 0.6302714381350409
Epoch 374, step 220500: Generator loss: 0.9056880848937555, discriminator loss: 0.5996440589427954
Epoch 383, step 225000: Generator loss: 0.9476164460844481, discriminator loss: 0.5872065601017749
Epoch 393, step 229500: Generator loss: 0.8700628989405085, discriminator loss: 0.614429159349864
Epoch 402, step 234000: Generator loss: 0.9316757869985379, discriminator loss: 0.5935855323341168
Epoch 412, step 238500: Generator loss: 0.9884794838296045, discriminator loss: 0.5769219878051017
Epoch 422, step 243000: Generator loss: 0.9787677707009845, discriminator loss: 0.5881667502986057
Epoch 431, step 247500: Generator loss: 0.9812438432375566, discriminator loss: 0.5934927140739243
Epoch 441, step 252000: Generator loss: 0.9866997564368781, discriminator loss: 0.5696533908512852
Epoch 450, step 256500: Generator loss: 1.0139115802447014, discriminator loss: 0.5784266778760502
Epoch 460, step 261000: Generator loss: 0.9905964203940505, discriminator loss: 0.5876510691973909
Epoch 470, step 265500: Generator loss: 0.9866478079954776, discriminator loss: 0.5789911089009697
Epoch 479, step 270000: Generator loss: 0.9699252954191648, discriminator loss: 0.593749921997389
Epoch 489, step 274500: Generator loss: 0.9484661724964777, discriminator loss: 0.5820076752834854
Epoch 498, step 279000: Generator loss: 0.9874062088992859, discriminator loss: 0.5781058085229654
Ended: 2021-12-12 23:56:45.487848
Elapsed: 1:00:20.502052
</pre>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">hook</span><span class="p">(</span><span class="n">plot</span><span class="p">,</span> <span class="n">element</span><span class="p">):</span>
    <span class="n">figure</span> <span class="o">=</span> <span class="n">plot</span><span class="o">.</span><span class="n">state</span>
    <span class="n">figure</span><span class="p">[</span><span class="s2">"layout"</span><span class="p">][</span><span class="s2">"sizing_mode"</span><span class="p">]</span> <span class="o">=</span> <span class="n">Plot</span><span class="o">.</span><span class="n">sizing_mode</span>
    <span class="k">return</span>

<span class="k">def</span> <span class="nf">plot_losses</span><span class="p">(</span><span class="n">plot_data</span><span class="p">:</span> <span class="n">PlotData</span><span class="p">,</span> <span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"losses"</span><span class="p">,</span>
                <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"Training Loss"</span><span class="p">):</span>
    <span class="sd">"""Plot the losses in Holoviews</span>

<span class="sd">    Args:</span>
<span class="sd">     plot_data: namedtuple with the losses over time</span>
<span class="sd">     file_name: name to save the plot (without extension)</span>
<span class="sd">     title: title for the plot</span>
<span class="sd">    """</span>
    <span class="n">plotting</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">({</span>
        <span class="s2">"Step"</span><span class="p">:</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">steps</span><span class="p">,</span>
        <span class="s2">"Generator Loss"</span><span class="p">:</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">generator_losses</span><span class="p">,</span>
        <span class="s2">"Discriminator Loss"</span><span class="p">:</span> <span class="n">plot_data</span><span class="o">.</span><span class="n">discriminator_losses</span>
    <span class="p">})</span>

    <span class="n">gen_plot</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Step"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Generator Loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">blue</span><span class="p">)</span>
    <span class="n">disc_plot</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Step"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Discriminator Loss"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">red</span><span class="p">)</span>

    <span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">gen_plot</span> <span class="o">*</span> <span class="n">disc_plot</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span>
                                       <span class="n">height</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
                                       <span class="n">width</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
                                       <span class="n">ylabel</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">,</span>
                                       <span class="n">hooks</span><span class="o">=</span><span class="p">[</span><span class="n">hook</span><span class="p">],</span>
                                       <span class="n">fontscale</span><span class="o">=</span><span class="n">Plot</span><span class="o">.</span><span class="n">fontscale</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="n">file_name</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">plot_losses</span><span class="p">(</span><span class="n">plot_data</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/mnist-gan/losses.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span><span class="n">plot_data</span> <span class="o">=</span> <span class="n">run_batch</span><span class="p">(</span><span class="n">parts</span><span class="p">,</span> <span class="n">plot_data</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">plot_losses</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"losses_2"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Training Loss 2"</span><span class="p">)</span>
</pre></div>
<object data="posts/gans/mnist-gan/losses_2.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
</div>
<div class="outline-3" id="outline-container-org24d4dcf">
<h3 id="org24d4dcf">Looking at the Final model.</h3>
<div class="outline-text-3" id="text-org24d4dcf">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">plot_image</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">title</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                <span class="n">num_images</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                <span class="n">size</span><span class="p">:</span> <span class="nb">tuple</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span>
                <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s2">"files/posts/gans/mnist-gan/"</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">"""Plot the image and save it</span>

<span class="sd">    Args:</span>
<span class="sd">     image: the tensor with the image to plot</span>
<span class="sd">     filename: name for the final image file</span>
<span class="sd">     title: title to put on top of the image</span>
<span class="sd">     num_images: how many images to put in the composite image</span>
<span class="sd">     size: the size for the image</span>
<span class="sd">     folder: sub-folder to save the file in</span>
<span class="sd">    """</span>
    <span class="n">unflattened_image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">*</span><span class="n">size</span><span class="p">)</span>
    <span class="n">image_grid</span> <span class="o">=</span> <span class="n">make_grid</span><span class="p">(</span><span class="n">unflattened_image</span><span class="p">[:</span><span class="n">num_images</span><span class="p">],</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

    <span class="n">pyplot</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image_grid</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

    <span class="n">pyplot</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelbottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">labelleft</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">pyplot</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">folder</span> <span class="o">+</span> <span class="n">filename</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"[[file:</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">]]"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">fake_noise</span> <span class="o">=</span> <span class="n">get_noise</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">z_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
<span class="n">parts</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">MODEL_PATH</span><span class="p">))</span>
<span class="n">fake</span> <span class="o">=</span> <span class="n">parts</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">fake_noise</span><span class="p">)</span>
<span class="n">plot_image</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">fake</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">"fake_digits.png"</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">"Fake Digits"</span><span class="p">)</span>
</pre></div>
<div class="figure">
<p><img alt="fake_digits.png" src="posts/gans/mnist-gan/fake_digits.png"></p>
</div>
<p>It's interesting that there's that crossover point where the generator's loss dips below the discriminator's and then they diverge again and the generator seems to stop improving. The digits for the best model do look passable as human-written digits, in most cases (depending on the human it might be in all cases).</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/neural-machine-translation-helper-functions/">Neural Machine Translation: Helper Functions</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/neural-machine-translation-helper-functions/" rel="bookmark"><time class="published dt-published" datetime="2021-02-27T14:41:04-08:00" itemprop="datePublished" title="2021-02-27 14:41">2021-02-27 14:41</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#org190e138">Helper Functions</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#org9b69148">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#org7c158bb">Helper functions</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#org7b3572b">Input encoder</a></li>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#org534697c">Pre-attention decoder</a></li>
<li><a href="posts/nlp/neural-machine-translation-helper-functions/#orgcd144f3">Preparing the attention input</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org190e138">
<h2 id="org190e138">Helper Functions</h2>
<div class="outline-text-2" id="text-org190e138">
<p>We will first implement a few functions that we will use later on. These will be for:</p>
<ul class="org-ul">
<li>the input encoder</li>
<li>the pre-attention decoder</li>
<li>preparation of the queries, keys, values, and mask.</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org9b69148">
<h3 id="org9b69148">Imports</h3>
<div class="outline-text-3" id="text-org9b69148">
<div class="highlight">
<pre><span></span><span class="c1"># from pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">trax.fastmath</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">fastmath_numpy</span>

<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org7c158bb">
<h2 id="org7c158bb">Helper functions</h2>
<div class="outline-text-2" id="text-org7c158bb"></div>
<div class="outline-3" id="outline-container-org7b3572b">
<h3 id="org7b3572b">Input encoder</h3>
<div class="outline-text-3" id="text-org7b3572b">
<p>The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">Serial</a> network which uses:</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>. We want to be able to configure how many encoder layers we have so remember to create LSTM layers equal to the number of the <code>n_encoder_layers</code> parameter.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">input_encoder</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">n_encoder_layers</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">""" Input encoder runs on the input sentence and creates</span>
<span class="sd">    activations that will be the keys and values for attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       input_vocab_size: vocab size of the input</span>
<span class="sd">       d_model:  depth of embedding (n_units in the LSTM cell)</span>
<span class="sd">       n_encoder_layers: number of LSTM layers in the encoder</span>

<span class="sd">    Returns:</span>
<span class="sd">       tl.Serial: The input encoder</span>
<span class="sd">    """</span>
    <span class="n">input_encoder</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span> 
        <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_encoder_layers</span><span class="p">)]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">input_encoder</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_input_encoder_fn</span><span class="p">(</span><span class="n">input_encoder_fn</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">input_encoder_fn</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">input_vocab_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">n_encoder_layers</span> <span class="o">=</span> <span class="mi">6</span>

    <span class="n">encoder</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_encoder_layers</span><span class="p">)</span>

    <span class="n">lstms</span> <span class="o">=</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s1">'  LSTM_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="s1">'</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_encoder_layers</span><span class="p">)</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Serial[</span><span class="se">\n</span><span class="s2">  Embedding_</span><span class="si">{</span><span class="n">input_vocab_size</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="si">{</span><span class="n">lstms</span><span class="si">}</span><span class="se">\n</span><span class="s2">]"</span>

    <span class="n">proposed</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">encoder</span><span class="p">)</span>

    <span class="c1"># Test all layers are in the expected sequence</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">proposed</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong model. </span><span class="se">\n</span><span class="s2">Proposed:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">proposed</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">expected</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Test the number of layers</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Test </span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span> <span class="o">==</span> <span class="p">(</span><span class="n">n_encoder_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'The number of sublayers does not match </span><span class="si">%s</span><span class="s1"> &lt;&gt;'</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">),</span> <span class="s2">" </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="p">(</span><span class="n">n_encoder_layers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"The enconder is not an object of "</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">)</span>


    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_input_encoder_fn</span><span class="p">(</span><span class="n">input_encoder</span><span class="p">)</span>
</pre></div>
<pre class="example">
[92m All tests passed
</pre></div>
</div>
<div class="outline-3" id="outline-container-org534697c">
<h3 id="org534697c">Pre-attention decoder</h3>
<div class="outline-text-3" id="text-org534697c">
<p>The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:</p>
<ul class="org-ul">
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight">tl.ShiftRight</a>: This pads a token to the beginning of your target tokens (e.g. <code>[8, 34, 12]</code> shifted right is <code>[0, 8, 34, 12]</code>). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding">tl.Embedding</a>: Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: <code>tl.Embedding(vocab_size, d_model)</code>. <code>vocab_size</code> is the number of entries in the given vocabulary. <code>d_model</code> is the number of elements in the word embedding.</li>
<li><a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a>: LSTM layer of size <code>d_model</code>.</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">pre_attention_decoder</span><span class="p">(</span><span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">""" Pre-attention decoder runs on the targets and creates</span>
<span class="sd">    activations that are used as queries in attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       mode: 'train' or 'eval'</span>
<span class="sd">       target_vocab_size: vocab size of the target</span>
<span class="sd">       d_model:  depth of embedding (n_units in the LSTM cell)</span>
<span class="sd">    Returns:</span>
<span class="sd">       tl.Serial: The pre-attention decoder</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">ShiftRight</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_pre_attention_decoder_fn</span><span class="p">(</span><span class="n">pre_attention_decoder_fn</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">pre_attention_decoder_fn</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="n">mode</span> <span class="o">=</span> <span class="s1">'train'</span>
    <span class="n">target_vocab_size</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="n">decoder</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="n">expected</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Serial[</span><span class="se">\n</span><span class="s2">  ShiftRight(1)</span><span class="se">\n</span><span class="s2">  Embedding_</span><span class="si">{</span><span class="n">target_vocab_size</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="s2">  LSTM_</span><span class="si">{</span><span class="n">d_model</span><span class="si">}</span><span class="se">\n</span><span class="s2">]"</span>

    <span class="n">proposed</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">decoder</span><span class="p">)</span>

    <span class="c1"># Test all layers are in the expected sequence</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">proposed</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="o">==</span> <span class="n">expected</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">" "</span><span class="p">,</span> <span class="s2">""</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong model. </span><span class="se">\n</span><span class="s2">Proposed:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">proposed</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">expected</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">decoder</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># Test the number of layers</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Test </span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
            <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">'The number of sublayers does not match </span><span class="si">%s</span><span class="s1"> &lt;&gt;'</span> <span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">decoder</span><span class="o">.</span><span class="n">sublayers</span><span class="p">),</span> <span class="s2">" </span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"The enconder is not an object of "</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">combinators</span><span class="o">.</span><span class="n">Serial</span><span class="p">)</span>


    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<p>They changed the behavior of the <code>Fn</code> (or something in there) so that it always wraps the ShiftRight in a Serial layer, so it doesn't match the test anymore. Testing strings is kind of gimpy anywayâ€¦</p>
<p>It looks like they're using a decorator to check the shape which then wraps it in a Serial layer. See trax.layers.assert_shape.AssertFunction</p>
<div class="highlight">
<pre><span></span><span class="n">test_pre_attention_decoder_fn</span><span class="p">(</span><span class="n">pre_attention_decoder</span><span class="p">)</span>
</pre></div>
<pre class="example">
Wrong model. 
Proposed:
Serial[
  Serial[
    ShiftRight(1)
  ]
  Embedding_10_2
  LSTM_2
] 
Expected:
Serial[
  ShiftRight(1)
  Embedding_10_2
  LSTM_2
]
[92m 2  Tests passed
[91m 1  Tests failed
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgcd144f3">
<h3 id="orgcd144f3">Preparing the attention input</h3>
<div class="outline-text-3" id="text-orgcd144f3">
<p>This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities. From the data preparation steps in Section 1 of this assignment, you should know which tokens in the input correspond to padding.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">prepare_attention_input</span><span class="p">(</span><span class="n">encoder_activations</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
                            <span class="n">decoder_activations</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">,</span>
                            <span class="n">inputs</span><span class="p">:</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
    <span class="sd">"""Prepare queries, keys, values and mask for attention.</span>

<span class="sd">    Args:</span>
<span class="sd">       encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder</span>
<span class="sd">       decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder</span>
<span class="sd">       inputs fastnp.array(batch_size, padded_input_length): padded input tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">       queries, keys, values and mask for attention.</span>
<span class="sd">    """</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">encoder_activations</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">encoder_activations</span>
    <span class="n">queries</span> <span class="o">=</span> <span class="n">decoder_activations</span>    
    <span class="n">mask</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">!=</span> <span class="mi">0</span>

    <span class="n">mask</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="n">mask</span> <span class="o">+=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">decoder_activations</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_prepare_attention_input</span><span class="p">(</span><span class="n">prepare_attention_input</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">prepare_attention_input</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1">#This unit test consider a batch size = 2, number_of_tokens = 3 and embedding_size = 4</span>

    <span class="n">enc_act</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>
               <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
    <span class="n">dec_act</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span> 
               <span class="p">[[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]])</span>
    <span class="n">inputs</span> <span class="o">=</span>  <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

    <span class="n">exp_mask</span> <span class="o">=</span> <span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]]],</span> 
                             <span class="p">[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]]]])</span>

    <span class="n">exp_type</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">enc_act</span><span class="p">)</span>

    <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">enc_act</span><span class="p">,</span> <span class="n">dec_act</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">dec_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Queries does not match the decoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">enc_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Keys does not match the encoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">enc_act</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Values does not match the encoder activations"</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">fastmath_numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">exp_mask</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Mask does not match expected tensor. </span><span class="se">\n</span><span class="s2">Expected:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">exp_mask</span><span class="p">,</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">Output:</span><span class="se">\n</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span><span class="n">mask</span><span class="p">)</span>

    <span class="c1"># Test the output type</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">exp_type</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"One of the output object are not of type "</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">interpreters</span><span class="o">.</span><span class="n">xla</span><span class="o">.</span><span class="n">DeviceArray</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_prepare_attention_input</span><span class="p">(</span><span class="n">prepare_attention_input</span><span class="p">)</span>
</pre></div>
<pre class="example">
[92m All tests passed
</pre></div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/first-course/logistic-regression-with-neural-networks/">Logistic Regression With Neural Networks</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/first-course/logistic-regression-with-neural-networks/" rel="bookmark"><time class="published dt-published" datetime="2021-02-23T18:22:52-08:00" itemprop="datePublished" title="2021-02-23 18:22">2021-02-23 18:22</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#orgba7e40c">Beginning</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org77be299">Instructions</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org0d090f0">1 - Packages</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org3c1d605">2 - Overview of the Problem set</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org683b13a">3 - General Architecture of the learning algorithm</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#orga6c9ead">4 - Building the parts of our algorithm</a>
<ul>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org931f6a5">4.1 - Helper functions</a></li>
</ul>
</li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org51b6c04">4.2 - Initializing parameters</a>
<ul>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org082aaee">4.3 - Forward and Backward propagation</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#orgb7c6b45">d) Optimization</a></li>
</ul>
</li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org76d469c">5 - Merge all functions into a model</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#orge42fbeb">6 - Further analysis (optional/ungraded exercise)</a></li>
<li><a href="posts/first-course/logistic-regression-with-neural-networks/#org3ba4db6">7 - Test with your own image (optional/ungraded exercise)</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgba7e40c">
<h2 id="orgba7e40c">Beginning</h2>
<div class="outline-text-2" id="text-orgba7e40c">
<p>In this post we will build a logistic regression classifier to recognize cats.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org77be299">
<h2 id="org77be299">Instructions</h2>
<div class="outline-text-2" id="text-org77be299">
<ul class="org-ul">
<li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li>
</ul>
<p><b>You will learn to:</b></p>
<ul class="org-ul">
<li>Build the general architecture of a learning algorithm, including:
<ul class="org-ul">
<li>Initializing parameters</li>
<li>Calculating the cost function and its gradient</li>
<li>Using an optimization algorithm (gradient descent)</li>
</ul>
</li>
<li>Gather all three functions above into a main model function, in the right order.</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org0d090f0">
<h2 id="org0d090f0">1 - Packages</h2>
<div class="outline-text-2" id="text-org0d090f0">
<p>First, let's run the cell below to import all the packages that you will need during this assignment.</p>
<ul class="org-ul">
<li><a href="http://www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li>
<li><a href="http://www.h5py.org">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</li>
<li><a href="http://matplotlib.org">matplotlib</a> is a famous library to plot graphs in Python.</li>
<li><a href="http://www.pythonware.com/products/pil/">PIL</a> and <a href="https://www.scipy.org/">scipy</a> are used here to test your model with your own picture at the end.</li>
</ul>
<div class="highlight">
<pre><span></span>import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset
</pre></div>
<div class="highlight">
<pre><span></span>get_ipython().run_line_magic('matplotlib', 'inline')
</pre></div>
<div class="highlight">
<pre><span></span>TOLERANCE = (0.1)**5
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-org3c1d605">
<h2 id="org3c1d605">2 - Overview of the Problem set</h2>
<div class="outline-text-2" id="text-org3c1d605">
<p><b>Problem Statement</b>: You are given a dataset ("data.h5") containing:</p>
<ul class="org-ul">
<li>a training set of m_train images labeled as cat (y=1) or non-cat (y=0)</li>
<li>a test set of m_test images labeled as cat or non-cat</li>
<li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</li>
</ul>
<p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p>
<p>Let's get more familiar with the dataset. Load the data by running the following code.</p>
<div class="highlight">
<pre><span></span># Loading the data (cat/non-cat)
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()
</pre></div>
<p>We added "_orig" at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don't need any preprocessing).</p>
<p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images.</p>
<div class="highlight">
<pre><span></span># Example of a picture
index = 25
image = plt.imshow(train_set_x_orig[index])
</pre></div>
<div class="figure">
<p><img alt="picture_example.png" src="file:///tmp/picture_example.png"></p>
</div>
<div class="highlight">
<pre><span></span>print ("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") +  "' picture.")
</pre></div>
<p>Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p>
<p><b>Exercise:</b> Find the values for:</p>
<ul class="org-ul">
<li>m_train (number of training examples)</li>
<li>m_test (number of test examples)</li>
<li>num_px (= height = width of a training image)</li>
</ul>
<p>Remember that `train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access `m_train` by writing `train_set_x_orig.shape[0]`.</p>
<div class="highlight">
<pre><span></span>### START CODE HERE ### (â‰ˆ 3 lines of code)
m_train, num_px, num_px, dimensions = train_set_x_orig.shape
m_test, num_px, num_px, dimensions = test_set_x_orig.shape
m_train = m_train
m_test = m_test
num_px = num_px
### END CODE HERE ###
</pre></div>
<div class="highlight">
<pre><span></span>assert m_train == 209
assert m_test == 50
assert num_px == 64
</pre></div>
<div class="highlight">
<pre><span></span>print ("Number of training examples: m_train = " + str(m_train))
print ("Number of testing examples: m_test = " + str(m_test))
print ("Height/Width of each image: num_px = " + str(num_px))
print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
print ("train_set_x shape: " + str(train_set_x_orig.shape))
print ("train_set_y shape: " + str(train_set_y.shape))
print ("test_set_x shape: " + str(test_set_x_orig.shape))
print ("test_set_y shape: " + str(test_set_y.shape))
</pre></div>
<p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px \(*\) num_px \(*\) 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p>
<p><b>Exercise:</b> Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\_px \(*\) num\_px \(*\) 3, 1).</p>
<p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use:</p>
<pre class="example">
X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
</pre>
<p>Reshape the training and test examples</p>
<div class="highlight">
<pre><span></span>### START CODE HERE ### (â‰ˆ 2 lines of code)
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T
### END CODE HERE ###
</pre></div>
<div class="highlight">
<pre><span></span>assert train_set_x_flatten.shape == (12288, 209)
assert train_set_y.shape == (1, 209)
assert test_set_x_flatten.shape == (12288, 50)
assert test_set_y.shape == (1, 50)
assert (train_set_x_flatten[0:5, 0] == np.array([17, 31, 56, 22, 33])).all()
</pre></div>
<div class="highlight">
<pre><span></span>print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))
print ("train_set_y shape: " + str(train_set_y.shape))
print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))
print ("test_set_y shape: " + str(test_set_y.shape))
print ("sanity check after reshaping: " + str(train_set_x_flatten[0:5,0]))
</pre></div>
<p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p>
<p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p>
<p>&lt;!â€“ During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !â€“&gt;</p>
<p>Let's standardize our dataset.</p>
<div class="highlight">
<pre><span></span>train_set_x = train_set_x_flatten/255.
test_set_x = test_set_x_flatten/255.
</pre></div>
<p><b>What you need to remember:</b></p>
<p>Common steps for pre-processing a new dataset are:</p>
<ul class="org-ul">
<li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, â€¦)</li>
<li>Reshape the datasets such that each example is now a vector of size (num_px \* num_px \* 3, 1)</li>
<li>"Standardize" the data</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org683b13a">
<h2 id="org683b13a">3 - General Architecture of the learning algorithm</h2>
<div class="outline-text-2" id="text-org683b13a">
<p>It's time to design a simple algorithm to distinguish cat images from non-cat images.</p>
<p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <b>Logistic Regression is actually a very simple Neural Network!</b></p>
<p>&lt;img src="images/LogReg_kiank.png" style="width:650px;height:400px;"&gt;</p>
<p><b>Mathematical expression of the algorithm</b>:</p>
<p><b>Key steps</b>: In this exercise, you will carry out the following steps:</p>
<ul class="org-ul">
<li>Initialize the parameters of the model</li>
<li>Learn the parameters for the model by minimizing the cost</li>
<li>Use the learned parameters to make predictions (on the test set)</li>
<li>Analyse the results and conclude</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orga6c9ead">
<h2 id="orga6c9ead">4 - Building the parts of our algorithm</h2>
<div class="outline-text-2" id="text-orga6c9ead">
<p>The main steps for building a Neural Network are:</p>
<ol class="org-ol">
<li>Define the model structure (such as number of input features)</li>
<li>Initialize the model's parameters</li>
<li>Loop:
<ul class="org-ul">
<li>Calculate current loss (forward propagation)</li>
<li>Calculate current gradient (backward propagation)</li>
<li>Update parameters (gradient descent)</li>
</ul>
</li>
</ol>
<p>You often build 1-3 separately and integrate them into one function we call `model()`.</p>
</div>
<div class="outline-3" id="outline-container-org931f6a5">
<h3 id="org931f6a5">4.1 - Helper functions</h3>
<div class="outline-text-3" id="text-org931f6a5">
<p><b>Exercise</b>: Using your code from "Python Basics", implement `sigmoid()`. As you've seen in the figure above, you need to compute \(sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}\) to make predictions. Use np.exp().</p>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: sigmoid

def sigmoid(z):
    """
    Compute the sigmoid of z

    Arguments:
    z -- A scalar or numpy array of any size.

    Return:
    s -- sigmoid(z)
    """

    ### START CODE HERE ### (â‰ˆ 1 line of code)
    s = 1/(1 + np.exp(-z))
    ### END CODE HERE ###

    return s
</pre></div>
<div class="highlight">
<pre><span></span>expected = np.array([0.5, 0.88079708])
actual = sigmoid(np.array([0,2]))
print ("sigmoid([0, 2]) = " + str(actual))
assert (expected - actual &lt; TOLERANCE).all()
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org51b6c04">
<h2 id="org51b6c04">4.2 - Initializing parameters</h2>
<div class="outline-text-2" id="text-org51b6c04">
<p><b>Exercise:</b> Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don't know what numpy function to use, look up np.zeros() in the Numpy library's documentation.</p>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: initialize_with_zeros

def initialize_with_zeros(dim):
    """
    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.

    Argument:
    dim -- size of the w vector we want (or number of parameters in this case)

    Returns:
    w -- initialized vector of shape (dim, 1)
    b -- initialized scalar (corresponds to the bias)
    """

    ### START CODE HERE ### (â‰ˆ 1 line of code)
    w = np.zeros((dim, 1))
    b = 0
    ### END CODE HERE ###

    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))

    return w, b
</pre></div>
<div class="highlight">
<pre><span></span>dim = 2
w, b = initialize_with_zeros(dim)
print ("w = " + str(w))
print ("b = " + str(b))
</pre></div>
<p>For image inputs, w will be of shape (num_px \(\times\) num_px \(\times\) 3, 1).</p>
</div>
<div class="outline-3" id="outline-container-org082aaee">
<h3 id="org082aaee">4.3 - Forward and Backward propagation</h3>
<div class="outline-text-3" id="text-org082aaee">
<p>Now that your parameters are initialized, you can do the "forward" and "backward" propagation steps for learning the parameters.</p>
<p><b>Exercise:</b> Implement a function `propagate()` that computes the cost function and its gradient.</p>
<p><b>Hints</b>:</p>
<p>Forward Propagation:</p>
<ul class="org-ul">
<li>You get X</li>
<li>You compute \(A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})\)</li>
<li>You calculate the cost function: \(J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})\)</li>
</ul>
<p>Here are the two formulas you will be using:</p>
<p>\[ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}\] \[ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}\]</p>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: propagate

def propagate(w, b, X, Y):
    """
    Implement the cost function and its gradient for the propagation explained above

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)

    Return:
    cost -- negative log-likelihood cost for logistic regression
    dw -- gradient of the loss with respect to w, thus same shape as w
    db -- gradient of the loss with respect to b, thus same shape as b

    Tips:
    - Write your code step by step for the propagation. np.log(), np.dot()
    """

    m = X.shape[1]

    # FORWARD PROPAGATION (FROM X TO COST)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A = sigmoid(np.dot(w.T, X) + b)                                    # compute activation
    cost = -(Y * np.log(A) + (1 - Y) * np.log(1 - A)).mean()
    ### END CODE HERE ###

    # BACKWARD PROPAGATION (TO FIND GRAD)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dz = A - Y
    dw = np.dot(X, dz.T)/m
    db = dz.mean()
    ### END CODE HERE ###

    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost)
    assert(cost.shape == ())

    grads = {"dw": dw,
             "db": db}

    return grads, cost
</pre></div>
<div class="highlight">
<pre><span></span>w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])
grads, cost = propagate(w, b, X, Y)
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
print ("cost = " + str(cost))
</pre></div>
<div class="highlight">
<pre><span></span>assert (grads["dw"] - np.abs(np.array([[0.99845601], [2.39507239]])) &lt; TOLERANCE).all()
assert np.abs(grads["db"] - 0.00145557813678) &lt; TOLERANCE
assert abs(cost - 5.801545319394553) &lt; TOLERANCE
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgb7c6b45">
<h3 id="orgb7c6b45">d) Optimization</h3>
<div class="outline-text-3" id="text-orgb7c6b45">
<ul class="org-ul">
<li>You have initialized your parameters.</li>
<li>You are also able to compute a cost function and its gradient.</li>
<li>Now, you want to update the parameters using gradient descent.</li>
</ul>
<p><b>Exercise:</b> Write down the optimization function. The goal is to learn \(w\) and \(b\) by minimizing the cost function \(J\). For a parameter \(\theta\), the update rule is $ Î¸ = Î¸ - Î± \text{ } dÎ¸$, where \(\alpha\) is the learning rate.</p>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: optimize

def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    This function optimizes w and b by running a gradient descent algorithm

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of shape (num_px * num_px * 3, number of examples)
    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
    num_iterations -- number of iterations of the optimization loop
    learning_rate -- learning rate of the gradient descent update rule
    print_cost -- True to print the loss every 100 steps

    Returns:
    params -- dictionary containing the weights w and bias b
    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.

    Tips:
    You basically need to write down two steps and iterate through them:
        1) Calculate the cost and the gradient for the current parameters. Use propagate().
        2) Update the parameters using gradient descent rule for w and b.
    """

    costs = []

    for i in range(num_iterations):


        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)
        ### START CODE HERE ### 
        grads, cost = propagate(w, b, X, Y)
        ### END CODE HERE ###

        # Retrieve derivatives from grads
        dw = grads["dw"]
        db = grads["db"]

        # update rule (â‰ˆ 2 lines of code)
        ### START CODE HERE ###
        w = w - learning_rate * dw
        b = b - learning_rate * db
        ### END CODE HERE ###

        # Record the costs
        if i % 100 == 0:
            costs.append(cost)

        # Print the cost every 100 training examples
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads, costs
</pre></div>
<div class="highlight">
<pre><span></span>params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)
</pre></div>
<div class="highlight">
<pre><span></span>print ("w = " + str(params["w"]))
print ("b = " + str(params["b"]))
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
</pre></div>
<div class="highlight">
<pre><span></span>assert (np.abs(params["w"] - np.array([[ 0.19033591], [ 0.12259159]])) &lt; TOLERANCE).all()
assert abs(params["b"] - 1.92535983008) &lt; TOLERANCE
assert (np.abs(grads['dw'] - np.array([[ 0.67752042], [ 1.41625495]])) &lt; TOLERANCE).all()
assert abs(grads["db"] - 0.219194504541) &lt; TOLERANCE
</pre></div>
<p><b>Exercise:</b> The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There is two steps to computing predictions:</p>
<ol class="org-ol">
<li>Calculate \(\hat{Y} = A = \sigma(w^T X + b)\)</li>
<li>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this).</li>
</ol>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: predict

def predict(w, b, X):
    '''
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)

    Returns:
    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    '''

    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    # Compute vector "A" predicting the probabilities of a cat being present in the picture
    ### START CODE HERE ### (â‰ˆ 1 line of code)
    A = sigmoid(np.dot(w.T, X) + b)
    ### END CODE HERE ###

    for i in range(A.shape[1]):

        # Convert probabilities A[0,i] to actual predictions p[0,i]
        ### START CODE HERE ### (â‰ˆ 4 lines of code)
        Y_prediction[0, i] = 1 if A[0, i] &gt; 0.5 else 0
        ### END CODE HERE ###

    assert(Y_prediction.shape == (1, m))

    return Y_prediction
</pre></div>
<div class="highlight">
<pre><span></span>w = np.array([[0.1124579],[0.23106775]])
b = -0.3
X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])
predictions = predict(w, b, X)
print ("predictions = " + str(predictions))
</pre></div>
<div class="highlight">
<pre><span></span>assert (np.abs(predictions - np.array([1., 1., 0.])) &lt; TOLERANCE).all()
</pre></div>
<p><b>What to remember:</b> You've implemented several functions that:</p>
<ul class="org-ul">
<li>Initialize (w,b)</li>
<li>Optimize the loss iteratively to learn parameters (w,b):
<ul class="org-ul">
<li>computing the cost and its gradient</li>
<li>updating the parameters using gradient descent</li>
</ul>
</li>
<li>Use the learned (w,b) to predict the labels for a given set of examples</li>
</ul>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org76d469c">
<h2 id="org76d469c">5 - Merge all functions into a model</h2>
<div class="outline-text-2" id="text-org76d469c">
<p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p>
<p><b>Exercise:</b> Implement the model function. Use the following notation:</p>
<ul class="org-ul">
<li>Y_prediction for your predictions on the test set</li>
<li>Y_prediction_train for your predictions on the train set</li>
<li>w, costs, grads for the outputs of optimize()</li>
</ul>
<div class="highlight">
<pre><span></span># GRADED FUNCTION: model

def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    Builds the logistic regression model by calling the function you've implemented previously

    Arguments:
    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
    print_cost -- Set to true to print the cost every 100 iterations

    Returns:
    d -- dictionary containing information about the model.
    """

    ### START CODE HERE ###

    # initialize parameters with zeros (â‰ˆ 1 line of code)
    w, b = initialize_with_zeros(X_train.shape[0])

    # Gradient descent (â‰ˆ 1 line of code)
    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    # Retrieve parameters w and b from dictionary "parameters"
    w = parameters["w"]
    b = parameters["b"]

    # Predict test/train set examples (â‰ˆ 2 lines of code)
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    ### END CODE HERE ###

    # Print train/test Errors
    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))


    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test, 
         "Y_prediction_train" : Y_prediction_train, 
         "w" : w, 
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}

    return d
</pre></div>
<div class="highlight">
<pre><span></span>d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)
</pre></div>
<p><b>Comment</b>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you'll build an even better classifier next week!</p>
<p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the `index` variable) you can look at predictions on pictures of the test set.</p>
<div class="highlight">
<pre><span></span># Example of a picture that was wrongly classified.
index = 1
plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))
</pre></div>
<pre class="example">
&lt;matplotlib.image.AxesImage at 0x7f02ff4aaa90&gt;
</pre>
<div class="figure">
<p><img alt="wrong_classification.png" src="file:///tmp/wrong_classification.png"></p>
</div>
<div class="highlight">
<pre><span></span>y_actual = test_set_y[0,index]
y_prediction_test = d["Y_prediction_test"]
prediction = classes[int(y_prediction_test[0,index])].decode("utf-8")
print ("y = " + str(y_actual) + ", you predicted that it is a \"" + prediction +  "\" picture.")
</pre></div>
<div class="highlight">
<pre><span></span># Plot learning curve (with costs)
costs = np.squeeze(d['costs'])
plt.plot(costs)
plt.ylabel('cost')
plt.xlabel('iterations (per hundreds)')
plt.title("Learning rate =" + str(d["learning_rate"]))
plt.show()
</pre></div>
<div class="figure">
<p><img alt="learning_curve.png" src="file:///tmp/learning_curve.png"></p>
</div>
<p><b>Interpretation</b>: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orge42fbeb">
<h2 id="orge42fbeb">6 - Further analysis (optional/ungraded exercise)</h2>
<div class="outline-text-2" id="text-orge42fbeb">
<p>Congratulations on building your first image classification model. Let's analyze it further, and examine possible choices for the learning rate \(\alpha\).</p>
<p>#### Choice of learning rate ####</p>
<p><b>Reminder</b>: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate \(\alpha\) determines how rapidly we update the parameters. If the learning rate is too large we may "overshoot" the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That's why it is crucial to use a well-tuned learning rate.</p>
<p>Let's compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the `learning_rates` variable to contain, and see what happens.</p>
<div class="highlight">
<pre><span></span>learning_rates = [0.01, 0.001, 0.0001]
models = {}
for i in learning_rates:
    print ("learning rate is: " + str(i))
    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)
    print ('\n' + "-------------------------------------------------------" + '\n')
</pre></div>
<div class="highlight">
<pre><span></span>for i in learning_rates:
    plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))

plt.ylabel('cost')
plt.xlabel('iterations')

legend = plt.legend(loc='upper center', shadow=True)
frame = legend.get_frame()
frame.set_facecolor('0.90')
plt.show()
</pre></div>
<div class="figure">
<p><img alt="tuning_alpha.png" src="file:///tmp/tuning_alpha.png"></p>
</div>
<p><b>Interpretation</b>:</p>
<ul class="org-ul">
<li>Different learning rates give different costs and thus different predictions results.</li>
<li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</li>
<li>A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li>
<li>In deep learning, we usually recommend that you:
<ul class="org-ul">
<li>Choose the learning rate that better minimizes the cost function.</li>
<li>If your model overfits, use other techniques to reduce overfitting. (We'll talk about this in later videos.)</li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org3ba4db6">
<h2 id="org3ba4db6">7 - Test with your own image (optional/ungraded exercise)</h2>
<div class="outline-text-2" id="text-org3ba4db6">
<p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:</p>
<ol class="org-ol">
<li>Click on "File" in the upper bar of this notebook, then click "Open" to go on your Coursera Hub.</li>
<li>Add your image to this Jupyter Notebook's directory, in the "images" folder</li>
<li>Change your image's name in the following code</li>
<li>Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</li>
</ol>
<div class="highlight">
<pre><span></span>## START CODE HERE ## (PUT YOUR IMAGE NAME) 
my_image = "my_image.jpg"   # change this to the name of your image file 
## END CODE HERE ##
</pre></div>
<div class="highlight">
<pre><span></span># We preprocess the image to fit your algorithm.
fname = "images/" + my_image
image = np.array(ndimage.imread(fname, flatten=False))
my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T
my_predicted_image = predict(d["w"], d["b"], my_image)
</pre></div>
<div class="highlight">
<pre><span></span>plt.imshow(image)
</pre></div>
<div class="highlight">
<pre><span></span>print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")
</pre></div>
<p><b>What to remember from this assignment:</b></p>
<ol class="org-ol">
<li>Preprocessing the dataset is important.</li>
<li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li>
<li>Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm. You will see more examples of this later in this course!</li>
</ol>
<p>Finally, if you'd like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:</p>
<ul class="org-ul">
<li>Play with the learning rate and the number of iterations</li>
<li>Try different initialization methods and compare the results</li>
<li>Test other preprocessings (center the data, or divide each row by its standard deviation)</li>
</ul>
<p>Bibliography:</p>
<ul class="org-ul">
<li><a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a></li>
<li><a href="https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/first-course/basic-numpy-for-neural-networks/">Basic Numpy for Neural Networks</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/first-course/basic-numpy-for-neural-networks/" rel="bookmark"><time class="published dt-published" datetime="2021-02-18T12:55:15-08:00" itemprop="datePublished" title="2021-02-18 12:55">2021-02-18 12:55</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org4bbcc1f">Beginning</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org38b08bb">Imports</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orged611ff">Set Up</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgc962205">Building basic functions with numpy</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgbcd70da">sigmoid function, np.exp()</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org155059e">Build a function that returns the sigmoid of a real number x using math.exp(x) for the exponential function.</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org3ed3e90">One reason why we use "numpy" instead of "math" in Deep Learning</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orge13a8b9">Implement the sigmoid function using numpy.</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org031b829">Sigmoid gradient</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgd0c46f7">Plotting The Sigmoid and Its Derivative</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org159cf80">Reshaping arrays</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgb08fdc3">Normalizing rows</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org5522f98">Broadcasting and the softmax function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgd53111e">Vectorization</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org1db1de9">Classic (Non-Vectorized)</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org163aab2">Dot Product Of Vectors Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org1641012">Outer Product Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org90f1e67">Elementwise Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orga39a7be">General Dot Product Implementation</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgac7a56b">Vectorized</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgbdc2169">Dot Product Of Vectors</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org45af8cb">Outer Product</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orga4b9d19">Elementwise Multiplication</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org4881daf">General Dot Product</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org6dc11c3">The L1 and L2 loss functions</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orga7d2884">L1</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org8ee7fec">L2 Loss</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgc368f80">End</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org16ab0cb">Source</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org4bbcc1f">
<h2 id="org4bbcc1f">Beginning</h2>
<div class="outline-text-2" id="text-org4bbcc1f">
<p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape.</p>
</div>
<div class="outline-3" id="outline-container-org38b08bb">
<h3 id="org38b08bb">Imports</h3>
<div class="outline-text-3" id="text-org38b08bb">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orged611ff">
<h3 id="orged611ff">Set Up</h3>
<div class="outline-text-3" id="text-orged611ff">
<div class="highlight">
<pre><span></span><span class="n">slug</span> <span class="o">=</span> <span class="s2">"basic-numpy-for-neural-networks"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/first-course/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc962205">
<h2 id="orgc962205">Building basic functions with numpy</h2>
<div class="outline-text-2" id="text-orgc962205"></div>
<div class="outline-3" id="outline-container-orgbcd70da">
<h3 id="orgbcd70da">sigmoid function, np.exp()</h3>
<div class="outline-text-3" id="text-orgbcd70da">
<p>Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp(). (see <a href="https://docs.python.org/3/library/math.html#power-and-logarithmic-functions">python's power and logarithmic functions</a>)</p>
</div>
</div>
<div class="outline-3" id="outline-container-org155059e">
<h3 id="org155059e">Build a function that returns the sigmoid of a real number x using math.exp(x) for the exponential function.</h3>
<div class="outline-text-3" id="text-org155059e">
<p>\(sigmoid(x) = \frac{1}{1+e^{-x}}\) is sometimes also known as the <i>logistic function</i>. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.</p>
<div class="highlight">
<pre><span></span><span class="n">TOLERANCE</span> <span class="o">=</span> <span class="mf">0.000001</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Compute sigmoid of x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: sigmoid(x)</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="mf">0.9525741268224334</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">basic_sigmoid</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"sigmoid of 3: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>Actually, we rarely use the "math" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org3ed3e90">
<h3 id="org3ed3e90">One reason why we use "numpy" instead of "math" in Deep Learning</h3>
<div class="outline-text-3" id="text-org3ed3e90">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># you will see this give an error when you run it, because x is a vector.</span>
<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
</pre></div>
<p>In fact, if \(x = (x_1, x_2, ..., x_n)\) is a row vector then \(np.exp(x)\) will apply the exponential function to every element of x. The output will thus be: \(np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})\). (see <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy.exp</a>).</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<pre class="example">
[ 2.71828183  7.3890561  20.08553692]
</pre>
<p>Furthermore, if x is a vector, then a Python operation such as \(s = x + 3\) or \(s = \frac{1}{x}\) will output s as a vector of the same size as x.</p>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
<pre class="example">
[4 5 6]
</pre>
<p>Any time you need more info on a numpy function, we encourage you to look at <a href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html">the official documentation</a>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orge13a8b9">
<h3 id="orge13a8b9">Implement the sigmoid function using numpy.</h3>
<div class="outline-text-3" id="text-orge13a8b9">
<p><b>Instructions</b>: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matricesâ€¦) are called numpy arrays. You don't need to know more for now.</p>
<p>\[ \text{For } x \in \mathbb{R}^n \text{, } sigmoid(x) = sigmoid\begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_n \\ \end{pmatrix} = \begin{pmatrix} \frac{1}{1+e^{-x_1}} \\ \frac{1}{1+e^{-x_2}} \\ ... \\ \frac{1}{1+e^{-x_n}} \\ \end{pmatrix}\tag{1} \]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute the sigmoid of x</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar or numpy array of any size</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: sigmoid(x)</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.73105858</span><span class="p">,</span>  <span class="mf">0.88079708</span><span class="p">,</span>  <span class="mf">0.95257413</span><span class="p">])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.73105858 0.88079708 0.95257413]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org031b829">
<h3 id="org031b829">Sigmoid gradient</h3>
<div class="outline-text-3" id="text-org031b829">
<p>You will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.</p>
<p>The formula is:</p>
<p>\[ sigmoid\_derivative(x) = \sigma'(x) = \sigma(x) (1 - \sigma(x))\tag{2} \]</p>
<p>You often code this function in two steps:</p>
<ol class="org-ol">
<li>Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.</li>
<li>Compute \(\sigma'(x) = s(1-s)\)</li>
</ol>
<p><a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html">numpy.random.randn</a> generates a sample from the standard normal distribution.</p>
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute the gradient (also called the slope or derivative) of the sigmoid</span>
<span class="sd">    function with respect to its input x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar or numpy array</span>

<span class="sd">    Returns:</span>
<span class="sd">     ds: Your computed gradient.</span>
<span class="sd">    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.19661193</span><span class="p">,</span> <span class="mf">0.10499359</span><span class="p">,</span> <span class="mf">0.04517666</span><span class="p">])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"sigmoid_derivative(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]
</pre></div>
<div class="outline-4" id="outline-container-orgd0c46f7">
<h4 id="orgd0c46f7">Plotting The Sigmoid and Its Derivative</h4>
<div class="outline-text-4" id="text-orgd0c46f7">
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">siggy</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">siggy_slope</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Sigmoid</span><span class="o">=</span><span class="n">siggy</span><span class="p">,</span> <span class="n">Slope</span><span class="o">=</span><span class="n">siggy_slope</span><span class="p">))</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Sigmoid and Derivative"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/first-course/basic-numpy-for-neural-networks/sigmoid.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
</div>
<div class="outline-3" id="outline-container-org159cf80">
<h3 id="org159cf80">Reshaping arrays</h3>
<div class="outline-text-3" id="text-org159cf80">
<p>Two common numpy functions used in deep learning are <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">np.shape</a> and <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html">np.reshape()</a>.</p>
<ul class="org-ul">
<li>X.shape is used to get the shape (dimension) of a matrix/vector X.</li>
<li>X.reshape(â€¦) is used to reshape X into some other dimension.</li>
</ul>
<p>For example, in computer science, an image is represented by a 3D array of shape \((length, height, depth = 3)\). However, when you read an image as the input of an algorithm you convert it to a vector of shape \((length \times height \times 3, 1)\). In other words, you "unroll", or reshape, the 3D array into a 1D vector.</p>
<p>We'll implemnt <code>image2vector()</code>, a function that takes an input of shape (length, height, 3) and returns a vector of shape \((length\times height\times 3, 1)\). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (\(a \times b, c\)) you would do:</p>
<pre class="example">
v = v.reshape((v.shape[0] * v.shape[1], v.shape[2]))
</pre>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">image2vector</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Unroll the image</span>

<span class="sd">    Args:</span>
<span class="sd">     image: array of shape (length, height, depth)</span>

<span class="sd">    Returns:</span>
<span class="sd">     v: vector of shape (length*height*depth, 1)</span>
<span class="sd">    """</span>
    <span class="n">length</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">length</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">depth</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
<p>Our image will a 3 by 3 by 2 array, typically images will be \((\textrm{number of pixels}_x, \textrm{number of pixels}_y,3)\) where 3 represents the RGB values</p>
<div class="highlight">
<pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span> <span class="mf">0.67826139</span><span class="p">,</span>  <span class="mf">0.29380381</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.90714982</span><span class="p">,</span>  <span class="mf">0.52835647</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.4215251</span> <span class="p">,</span>  <span class="mf">0.45017551</span><span class="p">]],</span>

                     <span class="p">[[</span> <span class="mf">0.92814219</span><span class="p">,</span>  <span class="mf">0.96677647</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.85304703</span><span class="p">,</span>  <span class="mf">0.52351845</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.19981397</span><span class="p">,</span>  <span class="mf">0.27417313</span><span class="p">]],</span>

                     <span class="p">[[</span> <span class="mf">0.60659855</span><span class="p">,</span>  <span class="mf">0.00533165</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.10820313</span><span class="p">,</span>  <span class="mf">0.49978937</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.34144279</span><span class="p">,</span>  <span class="mf">0.94630077</span><span class="p">]]])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.67826139</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.29380381</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.90714982</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.52835647</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.4215251</span> <span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.45017551</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.92814219</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.96677647</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.85304703</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.52351845</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.19981397</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.27417313</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.60659855</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.00533165</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.10820313</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.49978937</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.34144279</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.94630077</span><span class="p">]])</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">image2vector</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"image2vector(image) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">length</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">length</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">depth</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
image2vector(image) = [[0.67826139]
 [0.29380381]
 [0.90714982]
 [0.52835647]
 [0.4215251 ]
 [0.45017551]
 [0.92814219]
 [0.96677647]
 [0.85304703]
 [0.52351845]
 [0.19981397]
 [0.27417313]
 [0.60659855]
 [0.00533165]
 [0.10820313]
 [0.49978937]
 [0.34144279]
 [0.94630077]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgb08fdc3">
<h3 id="orgb08fdc3">Normalizing rows</h3>
<div class="outline-text-3" id="text-orgb08fdc3">
<p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to \) \frac{x}{\| x\|} \) (dividing each row vector of x by its norm).</p>
<p>For example, if \[ x = \begin{bmatrix} 0 & 3 & 4 \\ 2 & 6 & 4 \\ \end{bmatrix}\tag{3} \]</p>
<p>then</p>
<p>\[ \| x\| = np.linalg.norm(x, axis = 1, keepdims = True) = \begin{bmatrix} 5 \\ \sqrt{56} \\ \end{bmatrix}\tag{4} \] and</p>
<p>\[ x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix} 0 & \frac{3}{5} & \frac{4}{5} \\ \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\ \end{bmatrix}\tag{5} \]</p>
<p>Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it further down.</p>
<p>Now we'll implement <code>normalizeRows()</code> to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p>
<p>See: <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a></p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">normalizeRows</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Implement a function that normalizes each row of the matrix x </span>
<span class="sd">    (to have unit length).</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A numpy matrix of shape (n, m)</span>

<span class="sd">    Returns:</span>
<span class="sd">     x: The normalized (by row) numpy matrix.</span>
<span class="sd">    """</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">/</span><span class="n">x_norm</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.13736056</span><span class="p">,</span>  <span class="mf">0.82416338</span><span class="p">,</span>  <span class="mf">0.54944226</span><span class="p">]])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">normalizeRows</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"normalizeRows(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
normalizeRows(x) = [[0.         0.6        0.8       ]
 [0.13736056 0.82416338 0.54944226]]
</pre>
<p>We can check that each row is a unit vector by calculating the Euclidean distance.</p>
<p>\[ Euclidean = \sqrt{\sum X^2} \]</p>
<div class="highlight">
<pre><span></span><span class="n">SUM_ROWS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">actual</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">SUM_ROWS</span><span class="p">)))</span>
</pre></div>
<pre class="example">
[1. 1.]
</pre>
<p><b>Note</b>: <code>x_norm</code> and <code>x</code> have different shapes. This is normal given that <code>x_norm</code> takes the norm of each row of <code>x</code>. So <code>x_norm</code> has the same number of rows but only 1 column. As a consequence you can't use <code>x /= x_norm</code> instead of <code>x = x/x_norm</code>. So how did it work when you divided <code>x</code> by <code>x_norm</code>? This is called broadcasting and we'll talk about it next.</p>
</div>
<div class="outline-4" id="outline-container-org5522f98">
<h4 id="org5522f98">Broadcasting and the softmax function</h4>
<div class="outline-text-4" id="text-org5522f98">
<p>A very important concept to understand in numpy is "broadcasting". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcasting documentation</a>.</p>
<p>We'll implement a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes.</p>
<p><b>The Mathy Definitions</b>: \[ \text{for } x \in \mathbb{R}^{1\times n} \text{, } softmax(x) = softmax(\begin{bmatrix} x_1 &amp;& x_2 &amp;& \ldots &amp;& x_n \end{bmatrix}) = \begin{bmatrix} \frac{e^{x_1}}{\sum_{j}e^{x_j}} &amp;& \frac{e^{x_2}}{\sum_{j}e^{x_j}} &amp;& \ldots &amp;& \frac{e^{x_n}}{\sum_{j}e^{x_j}} \end{bmatrix} \]</p>
<p>\(\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{, x_{ij}}\) maps to the element in the \(i^{th}\) row and \(j^{th}\) column of <i>x</i>, thus we have: \[ softmax(x) = softmax\begin{bmatrix} x_{11} & x_{12} & x_{13} & \dots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \dots & x_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{m1} & x_{m2} & x_{m3} & \dots & x_{mn} \end{bmatrix} = \begin{bmatrix} \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\ \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}} \end{bmatrix} = \begin{pmatrix} softmax\text{(first row of x)} \\ softmax\text{(second row of x)} \\ \ldots \\ softmax\text{(last row of x)} \\ \end{pmatrix} \]</p>
<p>See also: <a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html">numpy.sum</a></p>
<div class="highlight">
<pre><span></span><span class="n">ROW_SUMS</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the softmax for each row of the input x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A numpy matrix of shape (n,m)</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: A numpy matrix equal to the softmax of x, of shape (n,m)</span>
<span class="sd">    """</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_sum</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROW_SUMS</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_exp</span><span class="o">/</span><span class="n">x_sum</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">9.80897665e-01</span><span class="p">,</span> <span class="mf">8.94462891e-04</span><span class="p">,</span> <span class="mf">1.79657674e-02</span><span class="p">,</span>
                          <span class="mf">1.21052389e-04</span><span class="p">,</span> <span class="mf">1.21052389e-04</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">8.78679856e-01</span><span class="p">,</span> <span class="mf">1.18916387e-01</span><span class="p">,</span> <span class="mf">8.01252314e-04</span><span class="p">,</span>
                          <span class="mf">8.01252314e-04</span><span class="p">,</span> <span class="mf">8.01252314e-04</span><span class="p">]])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"softmax(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04
  1.21052389e-04]
 [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04
  8.01252314e-04]]
</pre>
<p><b>Note</b>:</p>
<ul class="org-ul">
<li>If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). <b>x_exp/x_sum</b> works due to python broadcasting.</li>
</ul>
<p><b>What you need to remember:</b></p>
<ul class="org-ul">
<li>np.exp(x) works for any np.array x and applies the exponential function to every coordinate</li>
<li>the sigmoid function and its gradient</li>
<li>Some equivalent of <code>image2vector</code> is commonly used in deep learning</li>
<li>np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.</li>
<li>numpy has efficient built-in functions</li>
<li>broadcasting is extremely useful</li>
</ul>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgd53111e">
<h2 id="orgd53111e">Vectorization</h2>
<div class="outline-text-2" id="text-orgd53111e">
<p>In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.</p>
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">CLASSIC</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-org1db1de9">
<h3 id="org1db1de9">Classic (Non-Vectorized)</h3>
<div class="outline-text-3" id="text-org1db1de9"></div>
<div class="outline-4" id="outline-container-org163aab2">
<h4 id="org163aab2">Dot Product Of Vectors Implementation</h4>
<div class="outline-text-4" id="text-org163aab2">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">dot</span><span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"dot"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"dot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'dot'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
dot = 278 
 ----- Computation time = 0.09222100000005895 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-org1641012">
<h4 id="org1641012">Outer Product Implementation</h4>
<div class="outline-text-4" id="text-org1641012">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)):</span>
        <span class="n">outer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"outer"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"outer = </span><span class="si">{</span><span class="n">outer</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'outer'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
 ----- Computation time = 0.2285300000002266 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-org90f1e67">
<h4 id="org90f1e67">Elementwise Implementation</h4>
<div class="outline-text-4" id="text-org90f1e67">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">mul</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"elementwise"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"elementwise multiplication = </span><span class="si">{</span><span class="n">mul</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'elementwise'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]
 ----- Computation time = 0.10630600000016699 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga39a7be">
<h4 id="orga39a7be">General Dot Product Implementation</h4>
<div class="outline-text-4" id="text-orga39a7be">
<div class="highlight">
<pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">gdot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
        <span class="n">gdot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">x1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"general_dot"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"gdot = </span><span class="si">{</span><span class="n">gdot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'general_dot'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
gdot = [26.7997887  21.98533453 17.23427487]
 ----- Computation time = 0.14043400000041117 ms
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-orgac7a56b">
<h3 id="orgac7a56b">Vectorized</h3>
<div class="outline-text-3" id="text-orgac7a56b"></div>
<div class="outline-4" id="outline-container-orgbdc2169">
<h4 id="orgbdc2169">Dot Product Of Vectors</h4>
<div class="outline-text-4" id="text-orgbdc2169">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">DOT</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"dot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">DOT</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'dot'</span><span class="p">]</span> <span class="o">-</span> <span class="n">DOT</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
dot = 278
 ----- Computation time = 0.11425399999964725 ms
Difference: -0.0220329999995883 ms
</pre>
<p>So for this small set, the pure python is faster.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org45af8cb">
<h4 id="org45af8cb">Outer Product</h4>
<div class="outline-text-4" id="text-org45af8cb">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">OUTER</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"outer = </span><span class="si">{</span><span class="n">outer</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">OUTER</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'outer'</span><span class="p">]</span> <span class="o">-</span> <span class="n">OUTER</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]
 ----- Computation time = 0.09857899999943243 ms
Difference: 0.12995100000079418 ms
</pre>
<p>Now numpy is a little faster.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orga4b9d19">
<h4 id="orga4b9d19">Elementwise Multiplication</h4>
<div class="outline-text-4" id="text-orga4b9d19">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">ELEMENTWISE</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"elementwise multiplication = </span><span class="si">{</span><span class="n">mul</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">ELEMENTWISE</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'elementwise'</span><span class="p">]</span> <span class="o">-</span> <span class="n">ELEMENTWISE</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]
 ----- Computation time = 0.07506199999962604 ms
Difference: 0.03124400000054095 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-org4881daf">
<h4 id="org4881daf">General Dot Product</h4>
<div class="outline-text-4" id="text-org4881daf">
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">GENERAL</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"gdot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">GENERAL</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'general_dot'</span><span class="p">]</span> <span class="o">-</span> <span class="n">GENERAL</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
gdot = [26.7997887  21.98533453 17.23427487]
 ----- Computation time = 0.10962399999936423 ms
Difference: 0.030810000001046944 ms
</pre>
<p>As you may have noticed, the vectorized implementation is much cleaner and somewhat more efficient. For bigger vectors/matrices, the differences in running time become even bigger.</p>
<p><b>Note</b> that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to <code>.*</code> in Matlab/Octave), which performs an element-wise multiplication.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org6dc11c3">
<h3 id="org6dc11c3">The L1 and L2 loss functions</h3>
<div class="outline-text-3" id="text-org6dc11c3"></div>
<div class="outline-4" id="outline-container-orga7d2884">
<h4 id="orga7d2884">L1</h4>
<div class="outline-text-4" id="text-orga7d2884">
<p>Now we'll implement the numpy vectorized version of the L1 loss.</p>
<p><b>Reminder</b>: The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions (\( \hat{y} \)) are from the true values (<i>y</i>). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost. L1 loss is defined as:</p>
<p>\[ \begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m \left|y^{(i)} - \hat{y}^{(i)}\right| \end{align*}\tag{6} \]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""L1 Loss</span>

<span class="sd">    Args:</span>
<span class="sd">     yhat: vector of size m (predicted labels)</span>
<span class="sd">     y: vector of size m (true labels)</span>

<span class="sd">    Returns:</span>
<span class="sd">     loss: the value of the L1 loss function defined above</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">))</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">1.1</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"L1 = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<pre class="example">
L1 = 1.1
</pre></div>
</div>
<div class="outline-4" id="outline-container-org8ee7fec">
<h4 id="org8ee7fec">L2 Loss</h4>
<div class="outline-text-4" id="text-org8ee7fec">
<p>Next we'll implement the numpy vectorized version of the L2 loss. There are several ways of implementing the L2 loss but we'll use the function np.dot(). As a reminder, if \(x = [x_1, x_2, \ldots, x_n]\), then <code>np.dot(x,x)</code> = \(\sum_{j=0}^n x_j^{2}\).</p>
<p>L2 loss is defined as \[ L_2(\hat{y},y) = \sum_{i=0}^m\left(y^{(i)} - \hat{y}^{(i)}\right)^2\tag{7} \]</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the L2 Loss</span>

<span class="sd">    Args:</span>
<span class="sd">     yhat: vector of size m (predicted labels)</span>
<span class="sd">     y: vector of size m (true labels)</span>

<span class="sd">    Returns:</span>
<span class="sd">     loss: the value of the L2 loss function defined above</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.43</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"L2 = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<pre class="example">
L2 = 0.43
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc368f80">
<h2 id="orgc368f80">End</h2>
<div class="outline-text-2" id="text-orgc368f80">
<p><b>What to remember:</b></p>
<ul class="org-ul">
<li>Vectorization is very important in deep learning. It provides computational efficiency and clarity.</li>
<li>You have reviewed the L1 and L2 loss.</li>
<li>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etcâ€¦</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org16ab0cb">
<h3 id="org16ab0cb">Source</h3>
<div class="outline-text-3" id="text-org16ab0cb">
<p>This was an exercise from DeepLearning.ai's first Coursera course. (link to come)</p>
</div>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="next"><a href="index-21.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
