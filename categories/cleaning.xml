<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about cleaning)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/cleaning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2023 &lt;a href="mailto:cloisteredmonkey.jmark@slmail.me"&gt;Cloistered Monkey&lt;/a&gt; 
&lt;div id="license"xmlns:cc="http://creativecommons.org/ns#" &gt;This work is licensed under
&lt;a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"&gt;CC BY 4.0
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"&gt;
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"&gt;&lt;/a&gt;
&lt;/div&gt;
</copyright><lastBuildDate>Mon, 26 Jun 2023 20:42:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Cleaning the BBC News Archive</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org31b4ca1"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org0b83f82"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#orga9d758f"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org0517737"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org811655e"&gt;Graeae&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#orgdf251d7"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org79bb188"&gt;The Environment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org5a5f78a"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org2d5279d"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org6ca233c"&gt;The DataSet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org89a4624"&gt;The Tokenizer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org667e1cc"&gt;Convert the Texts To Sequences&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#orgb83e033"&gt;End&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#orgc43c9df"&gt;Sources&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html#org2d28166"&gt;The Original Dataset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org31b4ca1" class="outline-2"&gt;
&lt;h2 id="org31b4ca1"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org31b4ca1"&gt;
&lt;p&gt;
This is an initial look at cleaning up &lt;a href="http://mlg.ucd.ie/datasets/bbc.html"&gt;a text dataset&lt;/a&gt; from the BBC News archives. Although the exercise sites this as the source the dataset provided doesn't look like the actual raw dataset which is broken up into folders that classify the contents and each news item is in a separate file. Instead we're starting with a &lt;a href="https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv"&gt;partially pre-processed&lt;/a&gt; CSV that has been lower-cased and the classification is given as the first column in the dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0b83f82" class="outline-3"&gt;
&lt;h3 id="org0b83f82"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org0b83f82"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga9d758f" class="outline-4"&gt;
&lt;h4 id="orga9d758f"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orga9d758f"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from pathlib import Path
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org0517737" class="outline-4"&gt;
&lt;h4 id="org0517737"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org0517737"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pandas
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org811655e" class="outline-4"&gt;
&lt;h4 id="org811655e"&gt;Graeae&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org811655e"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import SubPathLoader, Timer
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdf251d7" class="outline-3"&gt;
&lt;h3 id="orgdf251d7"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgdf251d7"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org79bb188" class="outline-4"&gt;
&lt;h4 id="org79bb188"&gt;The Environment&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org79bb188"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ENVIRONMENT = SubPathLoader("DATASETS")
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org5a5f78a" class="outline-4"&gt;
&lt;h4 id="org5a5f78a"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org5a5f78a"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2d5279d" class="outline-2"&gt;
&lt;h2 id="org2d5279d"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org2d5279d"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6ca233c" class="outline-3"&gt;
&lt;h3 id="org6ca233c"&gt;The DataSet&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6ca233c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bbc_path = Path(ENVIRONMENT["BBC_NEWS"]).expanduser()
with TIMER:
    data = pandas.read_csv(bbc_path/"bbc-text.csv")
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-08-25 18:51:38,411 graeae.timers.timer start: Started: 2019-08-25 18:51:38.411196
2019-08-25 18:51:38,658 graeae.timers.timer end: Ended: 2019-08-25 18:51:38.658181
2019-08-25 18:51:38,658 graeae.timers.timer end: Elapsed: 0:00:00.246985
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
(2225, 2)
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.sample().iloc[0])
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
category                                                sport
text        bell set for england debut bath prop duncan be...
Name: 2134, dtype: object
&lt;/pre&gt;


&lt;p&gt;
So we have two columns - &lt;code&gt;category&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;, text being the one we have to clean up.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(data.text.dtype)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
object
&lt;/pre&gt;


&lt;p&gt;
That's not such an informative answer, but I checked and each row of text is a single string.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org89a4624" class="outline-3"&gt;
&lt;h3 id="org89a4624"&gt;The Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org89a4624"&gt;
&lt;p&gt;
The &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"&gt;Keras Tokenizer&lt;/a&gt; tokenizes the text for us as well as removing the punctuation, lower-casing the text, and some other things. We're also going to use a Out-of-Vocabulary token of "&amp;lt;OOV&amp;gt;" to identify words that are outside of the vocabulary when converting new texts to sequences.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer(oov_token="&amp;lt;OOV&amp;gt;", num_words=100)
tokenizer.fit_on_texts(data.text)
word_index = tokenizer.word_index
print(len(word_index))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
29727
&lt;/pre&gt;


&lt;p&gt;
The word-index is a dict that maps words found in the documents to counts.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org667e1cc" class="outline-4"&gt;
&lt;h4 id="org667e1cc"&gt;Convert the Texts To Sequences&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org667e1cc"&gt;
&lt;p&gt;
We're going to convert each of our texts to a sequence of numbers representing the words in them (one-hot-encoding). The &lt;code&gt;pad_sequences&lt;/code&gt; function adds zeros to the end of sequences that are shorter than the longest one so that they are all the same size.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sequences = tokenizer.texts_to_sequences(data.text)
padded = pad_sequences(sequences, padding="post")
print(padded[0])
print(padded.shape)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
[1 1 7 ... 0 0 0]
(2225, 4491)
&lt;/pre&gt;


&lt;p&gt;
Strangely there doesn't appear to be a good way to use stopwords. Maybe sklearn is more appropriate here.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vectorizer = CountVectorizer(stop_words=stopwords.words("english"),
			     lowercase=True, min_df=3,
			     max_df=0.9, max_features=5000)
vectors = vectorizer.fit_transform(data.text)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb83e033" class="outline-2"&gt;
&lt;h2 id="orgb83e033"&gt;End&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb83e033"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgc43c9df" class="outline-3"&gt;
&lt;h3 id="orgc43c9df"&gt;Sources&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgc43c9df"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org2d28166" class="outline-4"&gt;
&lt;h4 id="org2d28166"&gt;The Original Dataset&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org2d28166"&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;D. Greene and P. Cunningham. "Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering", Proc. ICML 2006. [PDF] [BibTeX].&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>cleaning</category><category>nlp</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/keras/cleaning-the-bbc-news-archive/index.html</guid><pubDate>Mon, 26 Aug 2019 00:14:54 GMT</pubDate></item></channel></rss>