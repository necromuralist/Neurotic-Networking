<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>In Too Deep (Posts about rnn)</title><link>https://necromuralist.github.io/In-Too-Deep/</link><description></description><atom:link href="https://necromuralist.github.io/In-Too-Deep/categories/rnn.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2019 &lt;a href="mailto:necromuralist@protonmail.com"&gt;Cloistered Monkey&lt;/a&gt; </copyright><lastBuildDate>Wed, 06 Feb 2019 17:40:28 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>TV Script Generation</title><link>https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/#orgb224de0"&gt;What is this about?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/#orgeeb2191"&gt;Set Up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/#orgd1c1622"&gt;Get the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/#org9fc01e3"&gt;Explore the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/#org785a27b"&gt;Pre-Processing the Text&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgb224de0" class="outline-2"&gt;
&lt;h2 id="orgb224de0"&gt;What is this about?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgb224de0"&gt;
&lt;p&gt;
In this project, you'll generate your own &lt;a href="https://en.wikipedia.org/wiki/Seinfeld"&gt;Seinfeld&lt;/a&gt; TV scripts using RNNs.  You'll be using part of the &lt;a href="https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv"&gt;Seinfeld dataset&lt;/a&gt; of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,"fake" TV script, based on patterns it recognizes in this training data.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgeeb2191" class="outline-2"&gt;
&lt;h2 id="orgeeb2191"&gt;Set Up&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgeeb2191"&gt;
&lt;/div&gt;
&lt;div id="outline-container-orge53924b" class="outline-3"&gt;
&lt;h3 id="orge53924b"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge53924b"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org83faf6d" class="outline-4"&gt;
&lt;h4 id="org83faf6d"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org83faf6d"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from collections import Counter
from pathlib import Path
import os
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgcaf0e70" class="outline-4"&gt;
&lt;h4 id="orgcaf0e70"&gt;PyPi&lt;/h4&gt;
&lt;/div&gt;
&lt;div id="outline-container-org21ed799" class="outline-4"&gt;
&lt;h4 id="org21ed799"&gt;Support Code&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org21ed799"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;import udacity.project_tv_script_generation.problem_unittests as unittests
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org14c2281" class="outline-3"&gt;
&lt;h3 id="org14c2281"&gt;Load Dotenv&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org14c2281"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;load_dotenv()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd1c1622" class="outline-2"&gt;
&lt;h2 id="orgd1c1622"&gt;Get the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgd1c1622"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class Scripts:
    """Seinfeld Scripts

    Args:
     environment_key: environment variable with the source location
    """
    def __init__(self, environment_key: str="SCRIPTS") -&amp;gt; None:
	self.environment_key = environment_key        
	self._script_blob = None
	self._path = None
	self._lines = None
	self._tokens = None
	return

    @property
    def path(self) -&amp;gt; Path:
	"""The path to the file"""
	if self._path is None:
	    load_dotenv()
	    self._path = Path(os.environ.get("SCRIPTS")).expanduser()
	    assert self._path.is_file()
	return self._path

    @property
    def script_blob(self) -&amp;gt; str:
	"""The input file as a string"""
	if self._script_blob is None:
	    with open(self.path) as reader:
		self._script_blob = reader.read()
	return self._script_blob

    @property
    def lines(self) -&amp;gt; list:
	"""The lines of the script"""
	if self._lines is None:
	    self._lines = self.script_blob.split("\n")
	return self._lines

    @property
    def tokens(self) -&amp;gt; Counter:
	"""The tokens and their counts"""
	if self._tokens is None:
	    self._tokens = Counter()
	    for token in self.script_blob.split():
		self._tokens[token] += 1
	return self._tokens
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;class ScriptInspector:
    """gets some basic counts

    Args:
     scripts: object with the scripts
    """
    def __init__(self, scripts: Scripts=None) -&amp;gt; None:
	self._scripts = scripts
	self._line_count = None
	self._count_per_line = None
	self._mean_words_per_line = None
	self._token_count = None
	return

    @property
    def scripts(self) -&amp;gt; Scripts:
	"""The scripts object"""
	if self._scripts is None:
	    self._scripts = Scripts()
	return self._scripts

    @property
    def line_count(self) -&amp;gt; int:
	"""Number of lines in the source"""
	if self._line_count is None:
	    self._line_count = len(self.scripts.lines)
	return self._line_count

    @property
    def count_per_line(self) -&amp;gt; list:
	"""tokens per line"""
	if self._count_per_line is None:
	    self._count_per_line = [len(line.split(" "))
				    for line in self.scripts.lines]
	return self._count_per_line

    @property
    def mean_words_per_line(self) -&amp;gt; float:
	"""Average number of words per line"""
	if self._mean_words_per_line is None:
	    self._mean_words_per_line = (sum(self.count_per_line)
					 /self.line_count)
	return self._mean_words_per_line

    @property
    def token_count(self) -&amp;gt; int:
	"""Number of tokens in the text"""
	if self._token_count is None:
	    self._token_count = sum(self.scripts.tokens.values())
	return self._token_count

    def most_common_tokens(self, count: int=10) -&amp;gt; list:
	"""token, count tuples in descending rank

	Args:
	 count: number of tuples to return in the list
	"""
	return self.scripts.tokens.most_common(count)

    def line_range(self, start: int=0, stop: int=10) -&amp;gt; list:
	"""lines within range

	Args:
	 start: index of first line
	 stop: upper bound for last line
	"""
	return self.scripts.lines[start:stop]
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
The scripts aren't really in a format that is optimized for pandas, at least not for this initial look, so we'll just load it as text.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;inspector = ScriptInspector()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9fc01e3" class="outline-2"&gt;
&lt;h2 id="org9fc01e3"&gt;Explore the Data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org9fc01e3"&gt;
&lt;p&gt;
Note that the first line is a header, but we're ignoring that and including it with the counts. So this is &lt;i&gt;very&lt;/i&gt; rough.
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;view_line_range = (0, 10)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6a359c6" class="outline-3"&gt;
&lt;h3 id="org6a359c6"&gt;Dataset Statistics&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6a359c6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lines = (("Number of unique tokens", "{:,}".format(inspector.token_count)),
	 ("Number of lines", "{:,}".format(inspector.line_count)),
	 ("Average number of words in each line", "{:.2f}".format(
	     inspector.mean_words_per_line)))
print(tabulate(lines, headers="Statistic Value".split(), tablefmt="orgtbl"))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Statistic&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;Number of unique tokens&lt;/td&gt;
&lt;td class="org-left"&gt;550,996&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Number of lines&lt;/td&gt;
&lt;td class="org-left"&gt;54,618&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;Average number of words in each line&lt;/td&gt;
&lt;td class="org-left"&gt;10.09&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8f74d88" class="outline-3"&gt;
&lt;h3 id="org8f74d88"&gt;Top Ten Words&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org8f74d88"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;lines = ((token, "{:,}".format(count))
	 for token, count in inspector.most_common_tokens())
print(tabulate(lines,
	       tablefmt="orgtbl", headers=["Token", "Count"]))
&lt;/pre&gt;&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="org-left"&gt;

&lt;col class="org-left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="org-left"&gt;Token&lt;/th&gt;
&lt;th scope="col" class="org-left"&gt;Count&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;the&lt;/td&gt;
&lt;td class="org-left"&gt;16,373&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;I&lt;/td&gt;
&lt;td class="org-left"&gt;13,911&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;you&lt;/td&gt;
&lt;td class="org-left"&gt;12,831&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;a&lt;/td&gt;
&lt;td class="org-left"&gt;12,096&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;to&lt;/td&gt;
&lt;td class="org-left"&gt;11,594&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;of&lt;/td&gt;
&lt;td class="org-left"&gt;5,490&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;and&lt;/td&gt;
&lt;td class="org-left"&gt;5,210&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;in&lt;/td&gt;
&lt;td class="org-left"&gt;4,741&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;is&lt;/td&gt;
&lt;td class="org-left"&gt;4,283&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="org-left"&gt;that&lt;/td&gt;
&lt;td class="org-left"&gt;4,047&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
So it looks like the stop words are the most common, as you might expect.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgaea3838" class="outline-3"&gt;
&lt;h3 id="orgaea3838"&gt;The First five Lines&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgaea3838"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for line in inspector.line_range(stop=5):
    print(line)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
,Character,Dialogue,EpisodeNo,SEID,Season
0,JERRY,"Do you know what this is all about? Do you know, why were here? To be out, this is out...and out is one of the single most enjoyable experiences of life. People...did you ever hear people talking about We should go out? This is what theyre talking about...this whole thing, were all out now, no one is home. Not one person here is home, were all out! There are people tryin to find us, they dont know where we are. (on an imaginary phone) Did you ring?, I cant find him. Where did he go? He didnt tell me where he was going. He must have gone out. You wanna go out you get ready, you pick out the clothes, right? You take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...Then youre standing around, whatta you do? You go We gotta be getting back. Once youre out, you wanna get back! You wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? Where ever you are in life, its my feeling, youve gotta go.",1.0,S01E01,1.0
1,JERRY,"(pointing at Georges shirt) See, to me, that button is in the worst possible spot. The second button literally makes or breaks the shirt, look at it. Its too high! Its in no-mans-land. You look like you live with your mother.",1.0,S01E01,1.0
2,GEORGE,Are you through?,1.0,S01E01,1.0
3,JERRY,"You do of course try on, when you buy?",1.0,S01E01,1.0

&lt;/pre&gt;

&lt;p&gt;
As you can see it is a comma-separated file with a header. What's not so obvious is how the index works. Is it for all the lines? Since the episode number is in the row-data I would assume so.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org785a27b" class="outline-2"&gt;
&lt;h2 id="org785a27b"&gt;Pre-Processing the Text&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org785a27b"&gt;
&lt;p&gt;
The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Lookup Table&lt;/li&gt;
&lt;li&gt;Tokenize Punctuation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id="outline-container-org74e1e0d" class="outline-3"&gt;
&lt;h3 id="org74e1e0d"&gt;Lookup Table&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org74e1e0d"&gt;
&lt;p&gt;
To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;Dictionary to go from the words to an id, we'll call &lt;code&gt;vocab_to_int&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Dictionary to go from the id to word, we'll call &lt;code&gt;int_to_vocab&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
Return these dictionaries in the following &lt;b&gt;&lt;b&gt;tuple&lt;/b&gt;&lt;/b&gt; &lt;code&gt;(vocab_to_int, int_to_vocab)&lt;/code&gt;
&lt;/p&gt;


&lt;p&gt;
import problem_unittests as tests
&lt;/p&gt;

&lt;p&gt;
def create_lookup_tables(text):
    """
    Create lookup tables for vocabulary
    :param text: The text of tv scripts split into words
    :return: A tuple of dicts (vocab_to_int, int_to_vocab)
    """
&lt;/p&gt;

&lt;p&gt;
return (None, None)
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
tests.test_create_lookup_tables(create_lookup_tables)
&lt;/p&gt;


&lt;p&gt;
def token_lookup():
    """
    Generate a dict to turn punctuation into a token.
    :return: Tokenized dictionary where the key is the punctuation and the value is the token
    """
&lt;/p&gt;

&lt;p&gt;
return None
&lt;/p&gt;

&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
tests.test_tokenize(token_lookup)
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
&lt;/p&gt;

&lt;p&gt;
helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
import helper
import problem_unittests as tests
&lt;/p&gt;

&lt;p&gt;
int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
import torch
&lt;/p&gt;

&lt;p&gt;
train_on_gpu = torch.cuda.is_available()
if not train_on_gpu:
    print('No GPU found. Please use a GPU to train your neural network.')
&lt;/p&gt;


&lt;p&gt;
from torch.utils.data import TensorDataset, DataLoader
&lt;/p&gt;


&lt;p&gt;
def batch_data(words, sequence_length, batch_size):
    """
    Batch the neural network data using DataLoader
    :param words: The word ids of the TV scripts
    :param sequence_length: The sequence length of each batch
    :param batch_size: The size of each batch; the number of sequences in a batch
    :return: DataLoader with batched data
    """
&lt;/p&gt;

&lt;p&gt;
return None
&lt;/p&gt;


&lt;p&gt;
test_text = range(50)
t_loader = batch_data(test_text, sequence_length=5, batch_size=10)
&lt;/p&gt;

&lt;p&gt;
data_iter = iter(t_loader)
sample_x, sample_y = data_iter.next()
&lt;/p&gt;

&lt;p&gt;
print(sample_x.shape)
print(sample_x)
print()
print(sample_y.shape)
print(sample_y)
&lt;/p&gt;


&lt;p&gt;
import torch.nn as nn
&lt;/p&gt;

&lt;p&gt;
class RNN(nn.Module):
&lt;/p&gt;

&lt;p&gt;
def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):
    """
    Initialize the PyTorch RNN Module
    :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)
    :param output_size: The number of output dimensions of the neural network
    :param embedding_dim: The size of embeddings, should you choose to use them        
    :param hidden_dim: The size of the hidden layer outputs
    :param dropout: dropout to add in between LSTM/GRU layers
    """
    super(RNN, self).__init__()
&lt;/p&gt;


&lt;p&gt;
def forward(self, nn_input, hidden):
    """
    Forward propagation of the neural network
    :param nn_input: The input to the neural network
    :param hidden: The hidden state        
    :return: Two Tensors, the output of the neural network and the latest hidden state
    """
&lt;/p&gt;

&lt;p&gt;
return None, None
&lt;/p&gt;


&lt;p&gt;
def init_hidden(self, batch_size):
    '''
    Initialize the hidden state of an LSTM/GRU
    :param batch_size: The batch_size of the hidden state
    :return: hidden state of dims (n_layers, batch_size, hidden_dim)
    '''
&lt;/p&gt;

&lt;p&gt;
return None
&lt;/p&gt;

&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
tests.test_rnn(RNN, train_on_gpu)
&lt;/p&gt;


&lt;p&gt;
def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):
    """
    Forward and backward propagation on the neural network
    :param decoder: The PyTorch Module that holds the neural network
    :param decoder_optimizer: The PyTorch optimizer for the neural network
    :param criterion: The PyTorch loss function
    :param inp: A batch of input to the neural network
    :param target: The target output for the batch of input
    :return: The loss and the latest hidden state Tensor
    """
&lt;/p&gt;

&lt;p&gt;
return None, None
&lt;/p&gt;

&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
&lt;/p&gt;

&lt;p&gt;
def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):
    batch_losses = []
&lt;/p&gt;

&lt;p&gt;
rnn.train()
&lt;/p&gt;

&lt;p&gt;
print("Training for %d epoch(s)…" % n_epochs)
for epoch_i in range(1, n_epochs + 1):
&lt;/p&gt;

&lt;p&gt;
hidden = rnn.init_hidden(batch_size)
&lt;/p&gt;

&lt;p&gt;
for batch_i, (inputs, labels) in enumerate(train_loader, 1):
&lt;/p&gt;

&lt;p&gt;
n_batches = len(train_loader.dataset)//batch_size
if(batch_i &amp;gt; n_batches):
    break
&lt;/p&gt;

&lt;p&gt;
loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          
&lt;/p&gt;

&lt;p&gt;
batch_losses.append(loss)
&lt;/p&gt;

&lt;p&gt;
if batch_i % show_every_n_batches == 0:
    print('Epoch: {:&amp;gt;4}/{:&amp;lt;4}  Loss: {}\n'.format(
	epoch_i, n_epochs, np.average(batch_losses)))
    batch_losses = []
&lt;/p&gt;

&lt;p&gt;
return rnn
&lt;/p&gt;


&lt;p&gt;
sequence_length =   # of words in a sequence
&lt;/p&gt;

&lt;p&gt;
batch_size = 
&lt;/p&gt;

&lt;p&gt;
train_loader = batch_data(int_text, sequence_length, batch_size)
&lt;/p&gt;


&lt;p&gt;
num_epochs = 
&lt;/p&gt;

&lt;p&gt;
learning_rate = 
&lt;/p&gt;

&lt;p&gt;
vocab_size = 
&lt;/p&gt;

&lt;p&gt;
output_size = 
&lt;/p&gt;

&lt;p&gt;
embedding_dim = 
&lt;/p&gt;

&lt;p&gt;
hidden_dim = 
&lt;/p&gt;

&lt;p&gt;
n_layers = 
&lt;/p&gt;

&lt;p&gt;
show_every_n_batches = 500
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
&lt;/p&gt;

&lt;p&gt;
rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)
if train_on_gpu:
    rnn.cuda()
&lt;/p&gt;

&lt;p&gt;
optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss()
&lt;/p&gt;

&lt;p&gt;
trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)
&lt;/p&gt;

&lt;p&gt;
helper.save_model('./save/trained_rnn', trained_rnn)
print('Model Trained and Saved')
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL
"""
import torch
import helper
import problem_unittests as tests
&lt;/p&gt;

&lt;p&gt;
_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()
trained_rnn = helper.load_model('./save/trained_rnn')
&lt;/p&gt;


&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
import torch.nn.functional as F
&lt;/p&gt;

&lt;p&gt;
def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):
    """
    Generate text using the neural network
    :param decoder: The PyTorch Module that holds the trained neural network
    :param prime_id: The word id to start the first prediction
    :param int_to_vocab: Dict of word id keys to word values
    :param token_dict: Dict of puncuation tokens keys to puncuation values
    :param pad_value: The value used to pad a sequence
    :param predict_len: The length of text to generate
    :return: The generated text
    """
    rnn.eval()
&lt;/p&gt;

&lt;p&gt;
current_seq = np.full((1, sequence_length), pad_value)
current_seq[-1][-1] = prime_id
predicted = [int_to_vocab[prime_id]]
&lt;/p&gt;

&lt;p&gt;
for _ in range(predict_len):
    if train_on_gpu:
	current_seq = torch.LongTensor(current_seq).cuda()
    else:
	current_seq = torch.LongTensor(current_seq)
&lt;/p&gt;

&lt;p&gt;
hidden = rnn.init_hidden(current_seq.size(0))
&lt;/p&gt;

&lt;p&gt;
output, _ = rnn(current_seq, hidden)
&lt;/p&gt;

&lt;p&gt;
p = F.softmax(output, dim=1).data
if(train_on_gpu):
    p = p.cpu() # move to cpu
&lt;/p&gt;

&lt;p&gt;
top_k = 5
p, top_i = p.topk(top_k)
top_i = top_i.numpy().squeeze()
&lt;/p&gt;

&lt;p&gt;
p = p.numpy().squeeze()
word_i = np.random.choice(top_i, p=p/p.sum())
&lt;/p&gt;

&lt;p&gt;
word = int_to_vocab[word_i]
predicted.append(word)     
&lt;/p&gt;

&lt;p&gt;
current_seq = np.roll(current_seq, -1, 1)
current_seq[-1][-1] = word_i
&lt;/p&gt;

&lt;p&gt;
gen_sentences = ' '.join(predicted)
&lt;/p&gt;

&lt;p&gt;
for key, token in token_dict.items():
    ending = ' ' if key in ['\n', '(', '"'] else ''
    gen_sentences = gen_sentences.replace(' ' + token.lower(), key)
gen_sentences = gen_sentences.replace('\n ', '\n')
gen_sentences = gen_sentences.replace('( ', '(')
&lt;/p&gt;

&lt;p&gt;
return gen_sentences
&lt;/p&gt;


&lt;p&gt;
gen_length = 400 # modify the length to your preference
prime_word = 'jerry' # name for starting the script
&lt;/p&gt;

&lt;p&gt;
"""
DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
"""
pad_word = helper.SPECIAL_WORDS['PADDING']
generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)
print(generated_script)
&lt;/p&gt;


&lt;p&gt;
f =  open("generated_script_1.txt","w")
f.write(generated_script)
f.close()
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>project</category><category>rnn</category><guid>https://necromuralist.github.io/In-Too-Deep/posts/nano/tv-script-generation/tv-script-generation/</guid><pubDate>Tue, 05 Feb 2019 23:29:20 GMT</pubDate></item></channel></rss>