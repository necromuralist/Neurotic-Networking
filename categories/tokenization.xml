<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neurotic Networking (Posts about tokenization)</title><link>https://necromuralist.github.io/Neurotic-Networking/</link><description></description><atom:link href="https://necromuralist.github.io/Neurotic-Networking/categories/tokenization.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2023 &lt;a href="mailto:cloisteredmonkey.jmark@slmail.me"&gt;Cloistered Monkey&lt;/a&gt; 
&lt;div id="license"xmlns:cc="http://creativecommons.org/ns#" &gt;This work is licensed under
&lt;a href="http://creativecommons.org/licenses/by/4.0/?ref=chooser-v1" target="_blank" rel="license noopener noreferrer" style="display:inline-block;"&gt;CC BY 4.0
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"&gt;
&lt;img src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"&gt;&lt;/a&gt;
&lt;/div&gt;
</copyright><lastBuildDate>Mon, 26 Jun 2023 20:42:30 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>IMDB GRU With Tokenization</title><link>https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html</link><dc:creator>Cloistered Monkey</dc:creator><description>&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org1fab38c"&gt;Beginning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#orgd4730f4"&gt;Imports&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org35d3732"&gt;Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org7b83b61"&gt;PyPi&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#orgd51ec3c"&gt;Other&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org6acef15"&gt;Set Up&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org6db3238"&gt;The Timer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#orgddbe129"&gt;Plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#orgcab94e1"&gt;Middle&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org59f2b5b"&gt;Set Up the Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org364c600"&gt;Building Up the Tokenizer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org9975803"&gt;Split Up the Sentences and Their Labels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org7aff4ef"&gt;Some Constants&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org4cfcde6"&gt;Build the Tokenizer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#orge404775"&gt;Decoder Ring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org464339b"&gt;Build the Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org9e70028"&gt;Train it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org4bbe2bc"&gt;Plot It&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html#org7c479b2"&gt;Raw&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1fab38c" class="outline-2"&gt;
&lt;h2 id="org1fab38c"&gt;Beginning&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org1fab38c"&gt;
&lt;p&gt;
This is another version of the RNN model to classify the IMDB reviews, but this time we're going to tokenize it ourselves and use a GRU, instead of using the tensorflow-datasets version.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd4730f4" class="outline-3"&gt;
&lt;h3 id="orgd4730f4"&gt;Imports&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orgd4730f4"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org35d3732" class="outline-4"&gt;
&lt;h4 id="org35d3732"&gt;Python&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org35d3732"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from argparse import Namespace
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7b83b61" class="outline-4"&gt;
&lt;h4 id="org7b83b61"&gt;PyPi&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7b83b61"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import hvplot.pandas
import numpy
import pandas
import tensorflow
import tensorflow_datasets
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgd51ec3c" class="outline-4"&gt;
&lt;h4 id="orgd51ec3c"&gt;Other&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-orgd51ec3c"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;from graeae import Timer, EmbedHoloviews
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6acef15" class="outline-3"&gt;
&lt;h3 id="org6acef15"&gt;Set Up&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org6acef15"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org6db3238" class="outline-4"&gt;
&lt;h4 id="org6db3238"&gt;The Timer&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org6db3238"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;TIMER = Timer()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgddbe129" class="outline-4"&gt;
&lt;h4 id="orgddbe129"&gt;Plotting&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgcab94e1" class="outline-2"&gt;
&lt;h2 id="orgcab94e1"&gt;Middle&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-orgcab94e1"&gt;
&lt;/div&gt;
&lt;div id="outline-container-org59f2b5b" class="outline-3"&gt;
&lt;h3 id="org59f2b5b"&gt;Set Up the Data&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org59f2b5b"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;imdb, info = tensorflow_datasets.load("imdb_reviews",
				      with_info=True,
				      as_supervised=True)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
WARNING: Logging before flag parsing goes to stderr.
W0924 21:52:10.158111 139862640383808 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.
&lt;/pre&gt;



&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training, testing = imdb["train"], imdb["test"]
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org364c600" class="outline-3"&gt;
&lt;h3 id="org364c600"&gt;Building Up the Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org364c600"&gt;
&lt;p&gt;
Since we didn't pass in a specifier for the configuration we wanted (e.g. &lt;code&gt;imdb/subwords8k&lt;/code&gt;) it defaulted to giving us the plain text reviews (and their labels) so we have to build the tokenizer ourselves.
&lt;/p&gt;
&lt;/div&gt;
&lt;div id="outline-container-org9975803" class="outline-4"&gt;
&lt;h4 id="org9975803"&gt;Split Up the Sentences and Their Labels&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org9975803"&gt;
&lt;p&gt;
As you might recall, the data set consists of 50,000 IMDB movie reviews categorized as positive or negative. To build the tokenize we first have to split the sentences from their labels
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_sentences = []
training_labels = []
testing_sentences = []
testing_labels = []
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;with TIMER:
    for sentence, label in training:
	training_sentences.append(str(sentence.numpy()))
	training_labels.append(str(label.numpy()))


    for sentence, label in testing:
	testing_sentences.append(str(sentence.numpy))
	testing_labels.append(str(label.numpy()))
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-24 21:52:11,396 graeae.timers.timer start: Started: 2019-09-24 21:52:11.395126
I0924 21:52:11.396310 139862640383808 timer.py:70] Started: 2019-09-24 21:52:11.395126
2019-09-24 21:52:18,667 graeae.timers.timer end: Ended: 2019-09-24 21:52:18.667789
I0924 21:52:18.667830 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:18.667789
2019-09-24 21:52:18,670 graeae.timers.timer end: Elapsed: 0:00:07.272663
I0924 21:52:18.670069 139862640383808 timer.py:78] Elapsed: 0:00:07.272663
&lt;/pre&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;training_labels_final = numpy.array(training_labels)
testing_labels_final = numpy.array(testing_labels)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7aff4ef" class="outline-4"&gt;
&lt;h4 id="org7aff4ef"&gt;Some Constants&lt;/h4&gt;
&lt;div class="outline-text-4" id="text-org7aff4ef"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Text = Namespace(
    vocab_size = 10000,
    embedding_dim = 16,
    max_length = 120,
    trunc_type='post',
    oov_token = "&amp;lt;OOV&amp;gt;",
)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org4cfcde6" class="outline-3"&gt;
&lt;h3 id="org4cfcde6"&gt;Build the Tokenizer&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4cfcde6"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tokenizer = Tokenizer(num_words=Text.vocab_size, oov_token=Text.oov_token)
with TIMER:
    tokenizer.fit_on_texts(training_sentences)

    word_index = tokenizer.word_index
    sequences = tokenizer.texts_to_sequences(training_sentences)
    padded = pad_sequences(sequences, maxlen=Text.max_length, truncating=Text.trunc_type)

    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
    testing_padded = pad_sequences(testing_sequences, maxlen=Text.max_length)
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example"&gt;
2019-09-24 21:52:21,705 graeae.timers.timer start: Started: 2019-09-24 21:52:21.705287
I0924 21:52:21.705317 139862640383808 timer.py:70] Started: 2019-09-24 21:52:21.705287
2019-09-24 21:52:32,152 graeae.timers.timer end: Ended: 2019-09-24 21:52:32.152267
I0924 21:52:32.152314 139862640383808 timer.py:77] Ended: 2019-09-24 21:52:32.152267
2019-09-24 21:52:32,154 graeae.timers.timer end: Elapsed: 0:00:10.446980
I0924 21:52:32.154620 139862640383808 timer.py:78] Elapsed: 0:00:10.446980
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orge404775" class="outline-3"&gt;
&lt;h3 id="orge404775"&gt;Decoder Ring&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-orge404775"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;index_to_word = {value: key for key, value in word_index.items()}

def decode_review(text: numpy.array) -&amp;gt; str:
    return " ".join([index_to_word.get(item, "&amp;lt;?&amp;gt;") for item in text])
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org464339b" class="outline-3"&gt;
&lt;h3 id="org464339b"&gt;Build the Model&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org464339b"&gt;
&lt;p&gt;
This time we're going to build a four-layer model with one Bidirectional layer that uses a &lt;a href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU"&gt;GRU&lt;/a&gt; (&lt;a href="https://www.wikiwand.com/en/Gated_recurrent_unit"&gt;Gated Recurrent Unit&lt;/a&gt;) instead of a LSTM.
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;model = tensorflow.keras.Sequential([
    tensorflow.keras.layers.Embedding(Text.vocab_size, Text.embedding_dim, input_length=Text.max_length),
    tensorflow.keras.layers.Bidirectional(tensorflow.compat.v2.keras.layers.GRU(32)),
    tensorflow.keras.layers.Dense(6, activation='relu'),
    tensorflow.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
&lt;/pre&gt;&lt;/div&gt;

&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;print(model.summary())
&lt;/pre&gt;&lt;/div&gt;

&lt;pre class="example" id="orgd8a1f5a"&gt;
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0
_________________________________________________________________
None
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org9e70028" class="outline-3"&gt;
&lt;h3 id="org9e70028"&gt;Train it&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org9e70028"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EPOCHS = 50
ONCE_PER_EPOCH = 2
batch_size = 8
history = model.fit(padded, training_labels_final,
		    epochs=EPOCHS,
		    batch_size=batch_size,
		    validation_data=(testing_padded, testing_labels_final),
		    verbose=ONCE_PER_EPOCH)
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org4bbe2bc" class="outline-3"&gt;
&lt;h3 id="org4bbe2bc"&gt;Plot It&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-org4bbe2bc"&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = pandas.DataFrame(history.history)
plot = data.hvplot().opts(title="GRU Training Performance", width=1000, height=800)
Embed(plot=plot, file_name="gru_training")()
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org7c479b2" class="outline-2"&gt;
&lt;h2 id="org7c479b2"&gt;Raw&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-org7c479b2"&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>gru</category><category>nlp</category><category>tokenization</category><guid>https://necromuralist.github.io/Neurotic-Networking/posts/keras/imdb-lstm-with-tokenization/index.html</guid><pubDate>Mon, 23 Sep 2019 21:14:04 GMT</pubDate></item></channel></rss>