<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 13) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-13.html" rel="canonical">
<link href="." rel="prev" type="text/html">
<link href="index-12.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="https://necromuralist.github.io/Neurotic-Networking/"><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/machine-translation-k-nearest-neighbors/">Implementing k-Nearest Neighbors for Machine Translation</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/machine-translation-k-nearest-neighbors/" rel="bookmark"><time class="published dt-published" datetime="2020-10-22T17:38:25-07:00" itemprop="datePublished" title="2020-10-22 17:38">2020-10-22 17:38</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org58e777c">Beginning</a>
<ul>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org561b031">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#orgc70ec3f">Middle</a>
<ul>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#orgfa4c476">Testing the translation</a></li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org30bfb0e">Test your implementation:</a></li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org6f3d326">Test your translation and compute its accuracy</a></li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org7eb178a">Bundling It Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-k-nearest-neighbors/#org224dd6f">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org58e777c">
<h2 id="org58e777c">Beginning</h2>
<div class="outline-text-2" id="text-org58e777c">
<p>This continues from the post where we found the <a href="posts/nlp/machine-translation-transformation-matrix/">transformation matrix</a>. It's part of a series of posts whose links are gathered in the <a href="posts/nlp/machine-translation/">Machine Translation</a> post.</p>
</div>
<div class="outline-3" id="outline-container-org561b031">
<h3 id="org561b031">Imports</h3>
<div class="outline-text-3" id="text-org561b031">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">Timer</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.english_french</span> <span class="kn">import</span> <span class="n">TrainingData</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.training</span> <span class="kn">import</span> <span class="n">TheTrainer</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc70ec3f">
<h2 id="orgc70ec3f">Middle</h2>
<div class="outline-text-2" id="text-orgc70ec3f"></div>
<div class="outline-3" id="outline-container-orgfa4c476">
<h3 id="orgfa4c476">Testing the translation</h3>
<div class="outline-text-3" id="text-orgfa4c476"></div>
<div class="outline-4" id="outline-container-orge5d3f31">
<h4 id="orge5d3f31">k-Nearest neighbors algorithm</h4>
<div class="outline-text-4" id="text-orge5d3f31">
<ul class="org-ul">
<li>The <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">k-Nearest neighbors algorithm</a> is a method which takes a vector as input and finds the other vectors in the dataset that are closest to it.</li>
<li>The 'k' is the number of "nearest neighbors" to find (e.g. k=2 finds the closest two neighbors).</li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-orgce8aa57">
<h4 id="orgce8aa57">Searching for the translation embedding</h4>
<div class="outline-text-4" id="text-orgce8aa57">
<p>Since we're approximating the translation function from English to French embeddings with a linear transformation matrix \(\mathbf{R}\), most of the time we won't get the exact embedding of a French word when we transform embedding \(\mathbf{e}\) of some particular English word into the French embedding space.</p>
<p>This is where <i>k</i>-NN becomes really useful! By using <i>1</i>-NN with \(\mathbf{eR}\) as input, we can search for an embedding \(\mathbf{f}\) (as a row) in the matrix \(\mathbf{Y}\) which is the closest to the transformed vector \(\mathbf{eR}\).</p>
</div>
</div>
<div class="outline-4" id="outline-container-org87aa728">
<h4 id="org87aa728">Cosine similarity</h4>
<div class="outline-text-4" id="text-org87aa728">
<p>Cosine similarity between vectors <i>u</i> and <i>v</i> calculated as the cosine of the angle between them.</p>
<p>The formula is:</p>
<p>\[ \cos(u,v)=\frac{u\cdot v}{\left\|u\right\|\left\|v\right\|} \]</p>
<ul class="org-ul">
<li>\(\cos(u,v) = 1\) when <i>u</i> and <i>v</i> lie on the same line and have the same direction.</li>
<li>\(\cos(u,v) = -1\) when they have exactly opposite directions.</li>
<li>\(\cos(u,v) = 0\) when the vectors are orthogonal (perpendicular) to each other.</li>
</ul>
<p><b>Note:</b> Distance and similarity are pretty much opposite things.</p>
<ul class="org-ul">
<li>We can obtain distance metric from cosine similarity, but the cosine similarity can't be used directly as the distance metric.</li>
<li>When the cosine similarity increases (towards <i>1</i>), the "distance" between the two vectors decreases (towards <i>0</i>).</li>
<li>We can define the cosine distance between <i>u</i> and <i>v</i> as</li>
</ul>
<p>\[ d_{\text{cos}}(u,v)=1-\cos(u,v) \]</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgf5ccca2">
<h4 id="orgf5ccca2">The Cosine Similarity</h4>
<div class="outline-text-4" id="text-orgf5ccca2">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">vector_1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">vector_2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculates the similarity between two vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     vector_1: array to compare</span>
<span class="sd">     vector_2: array to compare to vector_1</span>

<span class="sd">    Returns:</span>
<span class="sd">     cosine similarity between the two vectors</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vector_1</span><span class="p">,</span> <span class="n">vector_2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector_1</span><span class="p">)</span> <span class="o">*</span>
                                          <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgb60bee9">
<h4 id="orgb60bee9">the <code>nearest_neighbor()</code> function</h4>
<div class="outline-text-4" id="text-orgb60bee9">
<p>Inputs:</p>
<ul class="org-ul">
<li>Vector <i>v</i></li>
<li>A set of possible nearest neighbors <i>candidates</i></li>
<li><i>k</i> nearest neighbors to find.</li>
<li>The distance metric should be based on cosine similarity.</li>
<li><i>cosine_similarity</i> function is already implemented and imported for you. It's arguments are two vectors and it returns the cosine of the angle between them.</li>
<li>Iterate over rows in <i>candidates</i>, and save the result of similarities between current row and vector <i>v</i> in a python list. Take care that similarities are in the same order as row vectors of <i>candidates</i>.</li>
<li>Now you can use <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort">numpy argsort</a> to sort the indices for the rows of <i>candidates</i>.</li>
</ul>
</div>
<ul class="org-ul">
<li><a id="org0be6c02"></a>Hints<br>
<div class="outline-text-6" id="text-org0be6c02">
<ul class="org-ul">
<li><code>numpy.argsort</code> sorts values from most negative to most positive (smallest to largest)</li>
<li>The candidates that are nearest to <i>v</i> should have the highest cosine similarity</li>
<li>To get the last element of a list <i>tmp</i>, the notation is <code>tmp[-1:]</code></li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="k">def</span> <span class="nf">nearest_neighbor</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">candidates</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Input:</span>
<span class="sd">      - v, the vector you are going find the nearest neighbor for</span>
<span class="sd">      - candidates: a set of vectors where we will find the neighbors</span>
<span class="sd">      - k: top k nearest neighbors to find</span>
<span class="sd">    Output:</span>
<span class="sd">      - k_idx: the indices of the top k closest vectors in sorted form</span>
<span class="sd">    """</span>
    <span class="c1"># cosine_similarities = [cosine_similarity(v, row) for row in candidates]</span>

    <span class="c1"># for each candidate vector...</span>
    <span class="c1">#for row in candidates:</span>
    <span class="c1">#    # get the cosine similarity</span>
    <span class="c1">#    cos_similarity = cosine_similarity(v, row)</span>
    <span class="c1">#</span>
    <span class="c1">#    # append the similarity to the list</span>
    <span class="c1">#    similarity_l.append(cos_similarity)</span>

    <span class="c1"># sort the similarity list and get the indices of the sorted list</span>
    <span class="c1"># sorted_ids = numpy.argsort(similarity_l)</span>

    <span class="c1"># get the indices of the k most similar candidate vectors</span>
    <span class="c1"># k_idx = sorted_ids[-k:]</span>
    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argsort</span><span class="p">([</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span> <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">candidates</span><span class="p">])[</span><span class="o">-</span><span class="n">k</span><span class="p">:]</span>
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org30bfb0e">
<h3 id="org30bfb0e">Test your implementation:</h3>
<div class="outline-text-3" id="text-org30bfb0e">
<div class="highlight">
<pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"float64"</span><span class="p">)</span>
<span class="n">candidates</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">candidates</span><span class="p">[</span><span class="n">nearest_neighbor</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">candidates</span><span class="p">,</span> <span class="mi">3</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
<pre class="example">
[[9 9 9]
 [1 0 5]
 [2 0 1]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org6f3d326">
<h3 id="org6f3d326">Test your translation and compute its accuracy</h3>
<div class="outline-text-3" id="text-org6f3d326">
<p>Complete the function <code>test_vocabulary</code> which takes in English embedding matrix <i>X</i>, French embedding matrix <i>Y</i> and the <i>R</i> matrix and returns the accuracy of translations from <i>X</i> to <i>Y</i> by <i>R</i>.</p>
<ul class="org-ul">
<li>Iterate over transformed English word embeddings and check if the closest French word vector belongs to French word that is the actual translation.</li>
<li>Obtain an index of the closest French embedding by using <code>nearest_neighbor</code> (with argument <i>k=1</i>), and compare it to the index of the English embedding you have just transformed.</li>
<li>Keep track of the number of times you get the correct translation.</li>
<li>
<p>Calculate accuracy as</p>
<p>\[ \text{accuracy}=\frac{\#(\text{correct predictions})}{\#(\text{total predictions})} \]</p>
</li>
</ul>
<div class="highlight">
<pre><span></span><span class="c1"># UNQ_C10 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)</span>
<span class="k">def</span> <span class="nf">test_vocabulary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">R</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Input:</span>
<span class="sd">       X: a matrix where the columns are the English embeddings.</span>
<span class="sd">       Y: a matrix where the columns correspond to the French embeddings.</span>
<span class="sd">       R: the transform matrix which translates word embeddings from</span>
<span class="sd">       English to French word vector space.</span>
<span class="sd">    Output:</span>
<span class="sd">       accuracy: for the English to French capitals</span>
<span class="sd">    '''</span>

    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
    <span class="c1"># The prediction is X times R</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>

    <span class="c1"># initialize the number correct to zero</span>
    <span class="c1">#num_correct = 0</span>
    <span class="c1">#predictions = (nearest_neighbor(row, Y) == index for index, row in enumerate(pred))</span>
    <span class="c1"># accuracy = sum(predictions)/len(red)</span>
    <span class="c1"># loop through each row in pred (each transformed embedding)</span>
    <span class="c1">#for index, row_vector in enumerate(pred):</span>
    <span class="c1">#    # get the index of the nearest neighbor of pred at row 'i'; also pass in the candidates in Y</span>
    <span class="c1">#    pred_idx = nearest_neighbor(row_vector, Y)</span>
    <span class="c1">#</span>
    <span class="c1">#    # if the index of the nearest neighbor equals the row of i... \</span>
    <span class="c1">#    if pred_idx == index:</span>
    <span class="c1">#        # increment the number correct by 1.</span>
    <span class="c1">#        num_correct += 1</span>
    <span class="c1">#</span>
    <span class="c1">## accuracy is the number correct divided by the number of rows in 'pred' (also number of rows in X)</span>
    <span class="c1">#accuracy = num_correct/len(pred)</span>
    <span class="c1">#</span>
    <span class="c1">#### END CODE HERE ###</span>

    <span class="k">return</span> <span class="nb">sum</span><span class="p">([</span><span class="n">nearest_neighbor</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="o">==</span> <span class="n">index</span> <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pred</span><span class="p">)])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
</pre></div>
<p>Let's see how is your translation mechanism working on the unseen data:</p>
<div class="highlight">
<pre><span></span><span class="c1"># X_val, Y_val = get_matrices(en_fr_test, fr_embeddings_subset, en_embeddings_subset)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">TrainingData</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
<p>You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything.</p>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">test_vocabulary</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">transformation</span><span class="p">)</span>  <span class="c1"># this might take a minute or two</span>
</pre></div>
<pre class="example">
2020-10-24 19:57:36,633 graeae.timers.timer start: Started: 2020-10-24 19:57:36.632998
2020-10-24 20:05:48,225 graeae.timers.timer end: Ended: 2020-10-24 20:05:48.225526
2020-10-24 20:05:48,226 graeae.timers.timer end: Elapsed: 0:08:11.592528
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"accuracy on test set is </span><span class="si">{</span><span class="n">acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<pre class="example">
accuracy on test set is 0.552
</pre>
<p><b>Expected Output</b>:</p>
<p>#+RESULTS 0.557</p>
<p>You managed to translate words from one language to another language without ever seing them with almost 56% accuracy by using some basic linear algebra and learning a mapping of words from one language to another!</p>
</div>
</div>
<div class="outline-3" id="outline-container-org7eb178a">
<h3 id="org7eb178a">Bundling It Up</h3>
<div class="outline-text-3" id="text-org7eb178a">
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">nearest</span><span class="o">-</span><span class="n">neighbor</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">nearest</span><span class="o">-</span><span class="n">cosine</span><span class="o">-</span><span class="n">similarity</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">nearest</span><span class="o">-</span><span class="n">nearest</span><span class="o">-</span><span class="n">neighbor</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">nearest</span><span class="o">-</span><span class="n">call</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf8849f8">
<h4 id="orgf8849f8">Imports</h4>
<div class="outline-text-4" id="text-orgf8849f8">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org19b28e7">
<h4 id="org19b28e7">Nearest Neighbor</h4>
<div class="outline-text-4" id="text-org19b28e7">
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">NearestNeighbors</span><span class="p">:</span>
    <span class="sd">"""Finds the nearest neighbor(s) to a vector</span>

<span class="sd">    Args:</span>
<span class="sd">     candidates: set of vectors that are potential neighbors</span>
<span class="sd">     k: number of neighbors to find</span>
<span class="sd">    """</span>
    <span class="n">candidates</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>    
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgc8583be">
<h4 id="orgc8583be">Cosine Similarity Method</h4>
<div class="outline-text-4" id="text-orgc8583be">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">cosine_similarity</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector_1</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">vector_2</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Calculates the similarity between two vectors</span>

<span class="sd">    Args:</span>
<span class="sd">     vector_1: array to compare</span>
<span class="sd">     vector_2: array to compare to vector_1</span>

<span class="sd">    Returns:</span>
<span class="sd">     cosine similarity between the two vectors</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vector_1</span><span class="p">,</span> <span class="n">vector_2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector_1</span><span class="p">)</span> <span class="o">*</span>
                                          <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vector_2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org31526c0">
<h4 id="org31526c0">Nearest Neighbor Method</h4>
<div class="outline-text-4" id="text-org31526c0">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">nearest_neighbors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Find the nearest neghbor(s) to a vector</span>

<span class="sd">    Args:</span>
<span class="sd">      - vector, the vector you are going find the nearest neighbor for</span>

<span class="sd">    Returns:</span>
<span class="sd">      - k_idx: the indices of the top k closest vectors in sorted form</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">argsort</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">vector</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>
                          <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">candidates</span><span class="p">])[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:]</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5882771">
<h4 id="org5882771">Nearest Neighbor Call</h4>
<div class="outline-text-4" id="text-org5882771">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vector</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Alias for the `nearest_neighbors` method</span>

<span class="sd">    Args:</span>
<span class="sd">      - vector, the vector you are going find the nearest neighbor for</span>

<span class="sd">    Returns:</span>
<span class="sd">      - k_idx: the indices of the top k closest vectors in sorted form</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">nearest_neighbors</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org3d1bd8e">
<h4 id="org3d1bd8e">Testing It</h4>
<div class="outline-text-4" id="text-org3d1bd8e">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.nearest_neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>


<span class="n">v</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">"float64"</span><span class="p">)</span>
<span class="n">candidates</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mi">9</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="n">testing</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">candidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">candidates</span><span class="p">[</span><span class="n">testing</span><span class="o">.</span><span class="n">nearest_neighbors</span><span class="p">(</span><span class="n">v</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="k">assert</span> <span class="p">(</span><span class="n">actual</span> <span class="o">==</span> <span class="n">expected</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">()</span>
</pre></div>
<pre class="example">
[[9 9 9]
 [1 0 5]
 [2 0 1]]
</pre>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">TrainingData</span><span class="p">()</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">trainer</span><span class="o">.</span><span class="n">transformation</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">with</span> <span class="n">TIMER</span><span class="p">:</span>
    <span class="n">tester</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">candidates</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">tester</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">==</span> <span class="n">index</span>
                    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predictions</span><span class="p">)])</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</pre></div>
<pre class="example">
2020-10-31 19:35:14,133 graeae.timers.timer start: Started: 2020-10-31 19:35:14.133884
2020-10-31 19:43:29,695 graeae.timers.timer end: Ended: 2020-10-31 19:43:29.695745
2020-10-31 19:43:29,697 graeae.timers.timer end: Elapsed: 0:08:15.561861
</pre>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Accuracy: </span><span class="si">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">accuracy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2"> 0.2f</span><span class="si">}</span><span class="s2"> %"</span><span class="p">)</span>
</pre></div>
<pre class="example">
Accuracy:  55.23 %
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org224dd6f">
<h2 id="org224dd6f">End</h2>
<div class="outline-text-2" id="text-org224dd6f">
<ul class="org-ul">
<li>The next post in this series is <a href="posts/nlp/machine-translation-with-locality-sensitive-hashing/">Locality-Sensitive Hashing (LSH) for Machine Translation</a></li>
</ul>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/machine-translation-transformation-matrix/">Training the Machine Translation Transformation Matrix</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/machine-translation-transformation-matrix/" rel="bookmark"><time class="published dt-published" datetime="2020-10-22T17:34:49-07:00" itemprop="datePublished" title="2020-10-22 17:34">2020-10-22 17:34</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org399bd1c">Beginning</a>
<ul>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org8548977">Imports</a></li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org69de2f4">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org966f6ca">Middle</a>
<ul>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#orgd8b79d9">Translation As Linear Transformation of Embeddings</a></li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#orgbb47539">Saving It For Later</a></li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org7a7f09c">Check the Tester</a></li>
</ul>
</li>
<li><a href="posts/nlp/machine-translation-transformation-matrix/#org76c77f1">End</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org399bd1c">
<h2 id="org399bd1c">Beginning</h2>
<div class="outline-text-2" id="text-org399bd1c">
<p>In a prior post we <a href="posts/nlp/building-the-machine-translation-data-set/">built the translation training set</a>. In this post we'll find the <a href="https://en.wikipedia.org/wiki/Transformation_matrix">Transformation Matrix</a> that maps our English Embeddings to the French ones.</p>
</div>
<div class="outline-3" id="outline-container-org8548977">
<h3 id="org8548977">Imports</h3>
<div class="outline-text-3" id="text-org8548977">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># My Stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.english_french</span> <span class="kn">import</span> <span class="n">TrainingData</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org69de2f4">
<h3 id="org69de2f4">Set Up</h3>
<div class="outline-text-3" id="text-org69de2f4">
<div class="highlight">
<pre><span></span><span class="n">load_dotenv</span><span class="p">(</span><span class="s2">"posts/nlp/.env"</span><span class="p">)</span>
<span class="n">slug</span> <span class="o">=</span> <span class="s2">"machine-translation-transformation-matrix"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span>
                <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org966f6ca">
<h2 id="org966f6ca">Middle</h2>
<div class="outline-text-2" id="text-org966f6ca"></div>
<div class="outline-3" id="outline-container-orgd8b79d9">
<h3 id="orgd8b79d9">Translation As Linear Transformation of Embeddings</h3>
<div class="outline-text-3" id="text-orgd8b79d9">
<p>The problem we're working on is creating a translator that converts an English word to a French one. To do this we're using Word Embeddings which allows us to re-state the problem form being about translation to one of finding the transformation matrix that will give us a new embedding that close enough to the French translation that we can use some kind of algorithm to find the French embedding that is closest to it.</p>
<ul class="org-ul">
<li>Given dictionaries of English and French word embeddings we'll create a transformation matrix <i>R</i>.</li>
<li>Given an English word embedding, \(\mathbf{e}\), we can multiply \(\mathbf{eR}\) to get a new word embedding \(\mathbf{f}\).</li>
<li>Both \(\mathbf{e}\) and \(\mathbf{f}\) are <a href="https://en.wikipedia.org/wiki/Row_and_column_vectors">row vectors</a>.</li>
<li>We can then compute the nearest neighbors to \(\mathbf{f}\) in the French embeddings and recommend the word that is most similar to the transformed word embedding.</li>
</ul>
<p><b>Note:</b> <i>e</i> was called <code>X_train</code> in the original assigment and corresponds to the <code>TrainData.x_train</code> property that we built in <a href="posts/nlp/building-the-machine-translation-data-set/">the previous post</a>.</p>
</div>
<div class="outline-4" id="outline-container-org1d8b027">
<h4 id="org1d8b027">Rethinking Translation as the Minimization Problem</h4>
<div class="outline-text-4" id="text-org1d8b027">
<p>Find a matrix <i>R</i> that minimizes the following equation.</p>
<p>\[ \arg \min _{\mathbf{R}}\| \mathbf{X R} - \mathbf{Y}\|_{F} \]</p>
<p>So we're trying to find the transformation matrix that minimizes the distance between an English embedding and its corresponding French embedding. The subscript for the norm (<i>F</i>) means that we're using the <a href="https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm">Frobenius norm</a>.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgb90bf57">
<h4 id="orgb90bf57">Frobenius norm</h4>
<div class="outline-text-4" id="text-orgb90bf57">
<p>The Frobenius Norm of a matrix <i>A</i> (assuming it is of dimension <i>m, n</i>) is defined as the square root of the sum of the absolute squares of its elements:</p>
<p>\[ \|\mathbf{A}\|_{F} \equiv \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n}\left|a_{i j}\right|^{2}} \]</p>
</div>
</div>
<div class="outline-4" id="outline-container-org87d31de">
<h4 id="org87d31de">Actual loss function</h4>
<div class="outline-text-4" id="text-org87d31de">
<p>In the real world applications, the Frobenius norm loss:</p>
<p>\[ \| \mathbf{XR} - \mathbf{Y}\|_{F} \]</p>
<p>is often replaced by it's squared value divided by <i>m</i>:</p>
<p>\[ \frac{1}{m} \| \mathbf{X R} - \mathbf{Y} \|_{F}^{2} \]</p>
<p>where <i>m</i> is the number of examples (rows in \(\mathbf{X}\)).</p>
<ul class="org-ul">
<li>The same <i>R</i> is found when using this loss function versus the original Frobenius norm.</li>
<li>The reason for taking the square is that it's easier to compute the gradient of the squared Frobenius.</li>
<li>The reason for dividing by <i>m</i> is that we're more interested in the average loss per embedding than the loss for the entire training set.</li>
<li>The loss for all training sets increases with more words (training examples), so taking the average helps us to track the average loss regardless of the size of the training set.</li>
</ul>
</div>
</div>
<div class="outline-4" id="outline-container-org195048b">
<h4 id="org195048b">Implementing the Translation Mechanism Described</h4>
<div class="outline-text-4" id="text-org195048b"></div>
<ul class="org-ul">
<li><a id="orgce6cd99"></a>Step 1: Computing the loss<br>
<div class="outline-text-5" id="text-orgce6cd99">
<ul class="org-ul">
<li>The loss function will be the squared Frobenoius norm of the difference between the matrix and its approximation, divided by the number of training examples <i>m</i>.</li>
<li>Its formula is:</li>
</ul>
<p>\[ L(X, Y, R)=\frac{1}{m}\sum_{i=1}^{m} \sum_{j=1}^{n}\left( a_{i j} \right)^{2} \]</p>
<p>where \(a_{i j}\) is value in the \(i^{th}\) row and /j/th column of the matrix \(\mathbf{XR}-\mathbf{Y}\).</p>
<p><b>Instructions</b>: complete the <code>compute_loss()</code> function.</p>
<ul class="org-ul">
<li>Compute the approximation of <i>Y</i> by matrix multiplying <i>X</i> and <i>R</i></li>
<li>Compute the difference <i>XR - Y</i></li>
<li>Compute the squared Frobenius norm of the difference and divide it by <i>m</i>.</li>
</ul>
<p>Use matrix operations instead of the <code>numpy.norm</code> function.</p>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">R</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Inputs: </span>
<span class="sd">       X: a matrix of dimension (m,n) where the columns are the English embeddings.</span>
<span class="sd">       Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.</span>
<span class="sd">       R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.</span>
<span class="sd">    Outputs:</span>
<span class="sd">       L: a matrix of dimension (m,n) - the value of the loss function for given X, Y and R.</span>
<span class="sd">    '''</span>
    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
    <span class="c1"># m is the number of rows in X</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="c1"># diff is XR - Y</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span>

    <span class="c1"># diff_squared is the element-wise square of the difference</span>
    <span class="n">diff_squared</span> <span class="o">=</span> <span class="n">diff</span><span class="o">**</span><span class="mi">2</span>

    <span class="c1"># sum_diff_squared is the sum of the squared elements</span>
    <span class="n">sum_diff_squared</span> <span class="o">=</span> <span class="n">diff_squared</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># loss is the sum_diff_squared divided by the number of examples (m)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">sum_diff_squared</span><span class="o">/</span><span class="n">m</span>
    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-orgf8b3649">
<h4 id="orgf8b3649">Step 2: Computing the gradient of loss in respect to transform matrix R</h4>
<div class="outline-text-4" id="text-orgf8b3649">
<ul class="org-ul">
<li>Calculate the gradient of the loss with respect to transform matrix <i>R</i>.</li>
<li>The gradient is a matrix that encodes how much a small change in <i>R</i> affects the change in the loss function.</li>
<li>The gradient gives us the direction in which we should decrease <i>R</i> to minimize the loss.</li>
<li>\(m\) is the number of training examples (number of rows in <i>X</i>).</li>
<li>The formula for the gradient of the loss function <i>(,,)</i> is:</li>
</ul>
<p>\[ \frac{d}{dR}(,,)=\frac{d}{dR}\Big(\frac{1}{m}\| X R -Y\|_{F}^{2}\Big) = \frac{2}{m}X^{T} (X R - Y) \]</p>
</div>
<ul class="org-ul">
<li><a id="org0616514"></a><b>Instructions</b>: Complete the `compute_gradient` function below.<br>
<ul class="org-ul">
<li><a id="org3dbcce3"></a>Hints<br>
<div class="outline-text-6" id="text-org3dbcce3">
<ul class="org-ul">
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matrix.T.html">Transposing in numpy</a></li>
<li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">Finding out the dimensions of matrices in numpy</a></li>
<li>Remember to use numpy.dot for matrix multiplication</li>
</ul>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">R</span><span class="p">):</span>
    <span class="sd">'''</span>
<span class="sd">    Inputs: </span>
<span class="sd">       X: a matrix of dimension (m,n) where the columns are the English embeddings.</span>
<span class="sd">       Y: a matrix of dimension (m,n) where the columns correspond to the French embeddings.</span>
<span class="sd">       R: a matrix of dimension (n,n) - transformation matrix from English to French vector space embeddings.</span>
<span class="sd">    Outputs:</span>
<span class="sd">       g: a matrix of dimension (n,n) - gradient of the loss function L for given X, Y and R.</span>
<span class="sd">    '''</span>
    <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
    <span class="c1"># m is the number of rows in X</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># gradient is X^T(XR - Y) * 2/m</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">rows</span>
    <span class="k">assert</span> <span class="n">gradient</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
    <span class="c1">### END CODE HERE ###</span>
    <span class="k">return</span> <span class="n">gradient</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div class="outline-4" id="outline-container-org61c7c75">
<h4 id="org61c7c75">Step 3: Finding the optimal R with gradient descent algorithm</h4>
<div class="outline-text-4" id="text-org61c7c75"></div>
<ul class="org-ul">
<li><a id="orgb209c12"></a>Gradient descent<br>
<div class="outline-text-5" id="text-orgb209c12">
<p><a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html">Gradient descent</a> is an iterative algorithm which is used in searching for the optimum of the function.</p>
<ul class="org-ul">
<li>Earlier, we mentioned that the gradient of the loss with respect to the matrix encodes how much a tiny change in some coordinate of that matrix affect the change of loss function.</li>
<li>Gradient descent uses that information to iteratively change matrix <i>R</i> until we reach a point where the loss is minimized.</li>
</ul>
</div>
<ul class="org-ul">
<li><a id="orgcb5d18b"></a>Training with a fixed number of iterations<br>
<div class="outline-text-6" id="text-orgcb5d18b">
<p>Most of the time we iterate for a fixed number of training steps rather than iterating until the loss falls below a threshold.</p>
<p>Pseudocode:</p>
<ol class="org-ol">
<li>Calculate gradient <i>g</i> of the loss with respect to the matrix <i>R</i>.</li>
<li>Update <i>R</i> with the formula:</li>
</ol>
<p>\[ R_{\text{new}}= R_{\text{old}}-\alpha g \]</p>
<p>Where \(\alpha\) is the learning rate, which is a scalar.</p>
</div>
</li>
<li><a id="org30755c1"></a>Learning rate<br>
<div class="outline-text-6" id="text-org30755c1">
<ul class="org-ul">
<li>The learning rate or "step size" \(\alpha\) is a coefficient which decides how much we want to change <i>R</i> in each step.</li>
<li>If we change <i>R</i> too much, we could skip the optimum by taking too large of a step.</li>
<li>If we make only small changes to <i>R</i>, we will need many steps to reach the optimum.</li>
<li>Learning rate \(\alpha\) is used to control those changes.</li>
<li>Values of \(\alpha\) are chosen depending on the problem, and we'll use <code>learning_rate</code> <i>=0.0003</i> as the default value for our algorithm.</li>
</ul>
</div>
</li>
<li><a id="org6b4620e"></a>Exercise 04<br>
<div class="outline-text-6" id="text-org6b4620e">
<p>Instructions: Implement <code>align_embeddings()</code></p>
<div class="highlight">
<pre><span></span> <span class="k">def</span> <span class="nf">align_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span>
                      <span class="n">train_steps</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                      <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0003</span><span class="p">,</span>
                      <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">129</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
     <span class="sd">'''</span>
<span class="sd">     Inputs:</span>
<span class="sd">        X: a matrix of dimension (m,n) where the columns are the English embeddings.</span>
<span class="sd">        Y: a matrix of dimension (m,n) where the columns correspong to the French embeddings.</span>
<span class="sd">        train_steps: positive int - describes how many steps will gradient descent algorithm do.</span>
<span class="sd">        learning_rate: positive float - describes how big steps will  gradient descent algorithm do.</span>
<span class="sd">     Outputs:</span>
<span class="sd">        R: a matrix of dimension (n,n) - the projection matrix that minimizes the F norm ||X R -Y||^2</span>
<span class="sd">     '''</span>
     <span class="c1"># the number of columns in X is the number of dimensions for a word vector (e.g. 300)</span>
     <span class="c1"># R is a square matrix with length equal to the number of dimensions in th  word embedding</span>
     <span class="n">R</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">train_steps</span><span class="p">):</span>
         <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
             <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"loss at iteration </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> is: </span><span class="si">{</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
         <span class="c1">### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###</span>
         <span class="c1"># use the function that you defined to compute the gradient</span>
         <span class="n">gradient</span> <span class="o">=</span> <span class="n">compute_gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>

         <span class="c1"># update R by subtracting the learning rate times gradient</span>
         <span class="n">R</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradient</span>
         <span class="c1">### END CODE HERE ###</span>
     <span class="k">return</span> <span class="n">R</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><a id="org7246af5"></a>Testing Your Implementation.<br>
<div class="outline-text-5" id="text-org7246af5">
<div class="highlight">
<pre><span></span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">129</span><span class="p">)</span>
 <span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>
 <span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
 <span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
 <span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="o">.</span><span class="mi">1</span>
 <span class="n">R</span> <span class="o">=</span> <span class="n">align_embeddings</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
<pre class="example">
loss at iteration 0 is: 3.4697
loss at iteration 25 is: 3.3795
loss at iteration 50 is: 3.2918
loss at iteration 75 is: 3.2064
</pre>
<p><b>Expected Output:</b></p>
<p>loss at iteration 0 is: 3.7242 loss at iteration 25 is: 3.6283 loss at iteration 50 is: 3.5350 loss at iteration 75 is: 3.4442</p>
</div>
</li>
<li><a id="org658e1fa"></a>Calculate transformation matrix<br>
<div class="outline-text-5" id="text-org658e1fa">
<p>Using those the training set, find the transformation matrix \(\mathbf{R}\) by calling the function <code>align_embeddings()</code>.</p>
<p><b>NOTE:</b> The code cell below will take a few minutes to fully execute (~3 mins)</p>
<div class="highlight">
<pre><span></span> <span class="n">data</span> <span class="o">=</span> <span class="n">TrainingData</span><span class="p">()</span>
 <span class="n">R_train</span> <span class="o">=</span> <span class="n">align_embeddings</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_steps</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>
<pre class="example">
loss at iteration 0 is: 968.1416
loss at iteration 25 is: 97.6094
loss at iteration 50 is: 26.7949
loss at iteration 75 is: 9.7902
loss at iteration 100 is: 4.3831
loss at iteration 125 is: 2.3324
loss at iteration 150 is: 1.4509
loss at iteration 175 is: 1.0356
loss at iteration 200 is: 0.8263
loss at iteration 225 is: 0.7152
loss at iteration 250 is: 0.6539
loss at iteration 275 is: 0.6188
loss at iteration 300 is: 0.5983
loss at iteration 325 is: 0.5859
loss at iteration 350 is: 0.5783
loss at iteration 375 is: 0.5736
</pre>
<p>Expected Output</p>
<p>#+RESULTS loss at iteration 0 is: 963.0146 loss at iteration 25 is: 97.8292 loss at iteration 50 is: 26.8329 loss at iteration 75 is: 9.7893 loss at iteration 100 is: 4.3776 loss at iteration 125 is: 2.3281 loss at iteration 150 is: 1.4480 loss at iteration 175 is: 1.0338 loss at iteration 200 is: 0.8251 loss at iteration 225 is: 0.7145 loss at iteration 250 is: 0.6534 loss at iteration 275 is: 0.6185 loss at iteration 300 is: 0.5981 loss at iteration 325 is: 0.5858 loss at iteration 350 is: 0.5782 loss at iteration 375 is: 0.5735</p>
</div>
</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-orgbb47539">
<h3 id="orgbb47539">Saving It For Later</h3>
<div class="outline-text-3" id="text-orgbb47539">
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">trainer</span><span class="o">-</span><span class="n">imports</span><span class="o">&gt;&gt;</span>


<span class="o">&lt;&lt;</span><span class="n">the</span><span class="o">-</span><span class="n">trainer</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">trainer</span><span class="o">-</span><span class="n">timer</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">trainer</span><span class="o">-</span><span class="n">loss</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">trainer</span><span class="o">-</span><span class="n">gradient</span><span class="o">&gt;&gt;</span>

    <span class="o">&lt;&lt;</span><span class="n">trainer</span><span class="o">-</span><span class="n">align</span><span class="o">-</span><span class="n">embeddings</span><span class="o">&gt;&gt;</span>
</pre></div>
</div>
<div class="outline-4" id="outline-container-orga52e416">
<h4 id="orga52e416">Imports</h4>
<div class="outline-text-4" id="text-orga52e416">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">import</span> <span class="nn">attr</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orgba08ee9">
<h4 id="orgba08ee9">The Trainer Class</h4>
<div class="outline-text-4" id="text-orgba08ee9">
<p>We could keep it as just functions like it is here, but, nah.</p>
<div class="highlight">
<pre><span></span><span class="nd">@attr</span><span class="o">.</span><span class="n">s</span><span class="p">(</span><span class="n">auto_attribs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TheTrainer</span><span class="p">:</span>
    <span class="sd">"""Trains the word-embeddings data</span>

<span class="sd">    Args:</span>
<span class="sd">     x_train: the training input</span>
<span class="sd">     y_train: the training labels</span>
<span class="sd">     training_steps: number of times to run the training loop</span>
<span class="sd">     learning_rate: multiplier for the gradient (alpha)</span>
<span class="sd">     seed: random-seed for numpy</span>
<span class="sd">     loss_every: if verbose, how often to show loss during fitting</span>
<span class="sd">     verbose: whether to emit messages</span>
<span class="sd">    """</span>
    <span class="n">x_train</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">y_train</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span>
    <span class="n">_timer</span><span class="p">:</span> <span class="n">Timer</span><span class="o">=</span><span class="kc">None</span>
    <span class="n">training_steps</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">400</span>
    <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.8</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">129</span>
    <span class="n">loss_every</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">25</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="nb">bool</span><span class="o">=</span><span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org9089abb">
<h4 id="org9089abb">A Timer</h4>
<div class="outline-text-4" id="text-org9089abb">
<p>Just something to keep track of how long things take.</p>
<div class="highlight">
<pre><span></span><span class="nd">@property</span>
<span class="k">def</span> <span class="nf">timer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Timer</span><span class="p">:</span>
    <span class="sd">"""A timer"""</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timer</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">emit</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timer</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-orged6a60c">
<h4 id="orged6a60c">The Loss Method</h4>
<div class="outline-text-4" id="text-orged6a60c">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformation</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">float</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Calculates the loss between XR and Y as the average sum of difference squared</span>

<span class="sd">    Args: </span>
<span class="sd">       transformation: a matrix of dimension (n,n) - transformation matrix.</span>

<span class="sd">    Returns:</span>
<span class="sd">       loss: value of loss function for X, Y and R</span>
<span class="sd">    """</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">difference</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">transformation</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span>
    <span class="n">difference_squared</span> <span class="o">=</span> <span class="n">difference</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">sum_of_difference_squared</span> <span class="o">=</span> <span class="n">difference_squared</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">sum_of_difference_squared</span><span class="o">/</span><span class="n">rows</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org1865904">
<h4 id="org1865904">The Gradient</h4>
<div class="outline-text-4" id="text-org1865904">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">transformation</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""computes the gradient (slope) of the loss</span>

<span class="sd">    Args: </span>
<span class="sd">       transformation: transformation matrix of dimension (n,n)</span>

<span class="sd">    Returns:</span>
<span class="sd">       gradient: a matrix of dimension (n,n)</span>
<span class="sd">    """</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">gradient</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                  <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">transformation</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="p">)</span><span class="o">/</span><span class="n">rows</span>
    <span class="k">assert</span> <span class="n">gradient</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradient</span>
</pre></div>
</div>
</div>
<div class="outline-4" id="outline-container-org5206c22">
<h4 id="org5206c22">The Embeddings Aligner</h4>
<div class="outline-text-4" id="text-org5206c22">
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Fits the transformation matrix to the data</span>

<span class="sd">    Side Effect:</span>
<span class="sd">     sets self.transformation  and self.losses</span>

<span class="sd">    Returns:</span>
<span class="sd">     the projection matrix that minimizes the F norm ||X R -Y||^2</span>
<span class="sd">    """</span>
    <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transformation</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Step</span><span class="se">\t</span><span class="s2">Loss"</span><span class="p">)</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_steps</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">transformation</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">step</span><span class="si">}</span><span class="se">\t</span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">0.4f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transformation</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">transformation</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformation</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformation</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org7a7f09c">
<h3 id="org7a7f09c">Check the Tester</h3>
<div class="outline-text-3" id="text-org7a7f09c"></div>
<div class="outline-4" id="outline-container-org4dd6066">
<h4 id="org4dd6066">Sanity Check</h4>
<div class="outline-text-4" id="text-org4dd6066">
<div class="highlight">
<pre><span></span><span class="kn">from</span> <span class="nn">neurotic.nlp.word_embeddings.training</span> <span class="kn">import</span> <span class="n">TheTrainer</span>
<span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">129</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="o">.</span><span class="mi">1</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">training_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-10-23 18:27:50,195 graeae.timers.timer start: Started: 2020-10-23 18:27:50.195052
2020-10-23 18:27:50,201 graeae.timers.timer end: Ended: 2020-10-23 18:27:50.201767
2020-10-23 18:27:50,203 graeae.timers.timer end: Elapsed: 0:00:00.006715
The loss at step 0 is: 3.7242
The loss at step 25 is: 2.8709
The loss at step 50 is: 2.2201
The loss at step 75 is: 1.7235
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgec425e2">
<h4 id="orgec425e2">The Real Data</h4>
<div class="outline-text-4" id="text-orgec425e2">
<div class="highlight">
<pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">TheTrainer</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">x_train</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
<pre class="example">
2020-10-23 18:30:45,558 graeae.timers.timer start: Started: 2020-10-23 18:30:45.558693
Step    Loss
0       963.0146
25      97.8292
50      26.8329
75      9.7893
100     4.3776
125     2.3281
150     1.4480
175     1.0338
200     0.8251
225     0.7145
250     0.6534
275     0.6185
300     0.5981
325     0.5858
350     0.5782
375     0.5735
2020-10-23 18:31:16,471 graeae.timers.timer end: Ended: 2020-10-23 18:31:16.471708
2020-10-23 18:31:16,473 graeae.timers.timer end: Elapsed: 0:00:30.913015
</pre></div>
</div>
<div class="outline-4" id="outline-container-org26f1ddd">
<h4 id="org26f1ddd">Plotting the Losses</h4>
<div class="outline-text-4" id="text-org26f1ddd">
<div class="highlight">
<pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Step</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">))),</span>
                               <span class="n">Loss</span><span class="o">=</span><span class="n">trainer</span><span class="o">.</span><span class="n">losses</span><span class="p">))</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Step"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Loss"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Training Losses"</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">990</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">780</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">outcome</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"train_loss"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">outcome</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/machine-translation-transformation-matrix/train_loss.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>Although the losses continue to go down, it looks like most of the gains come in the first 100 rounds of training.</p>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org76c77f1">
<h2 id="org76c77f1">End</h2>
<div class="outline-text-2" id="text-org76c77f1">
<ul class="org-ul">
<li>The master post with links to all the posts in this series is <a href="posts/nlp/machine-translation/">Machine Translation</a>.</li>
<li>The next post in this series is <a href="posts/nlp/machine-translation-k-nearest-neighbors/">Implementing k-Nearest Neighbors for Machine Translation</a>.</li>
<li>This is part of an Assignment for Coursera's Natural Language Processing Specialization, Course 1, Week 4.</li>
</ul>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="." rel="prev">Newer posts</a></li>
<li class="next"><a href="index-12.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="http://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:necromuralist@protonmail.com">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
</script>
</body>
</html>
