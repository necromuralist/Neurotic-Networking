<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#">
<head>
<meta charset="utf-8">
<meta content="Studies in Deep Learning." name="description">
<meta content="width=device-width, initial-scale=1" name="viewport">
<title>Neurotic Networking (old posts, page 21) | Neurotic Networking</title>
<link href="assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<link href="assets/css/ipython.min.css" rel="stylesheet" type="text/css">
<link href="assets/css/nikola_ipython.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<meta content="Nikola (getnikola.com)" name="generator">
<link href="rss.xml" hreflang="en" rel="alternate" title="RSS" type="application/rss+xml">
<link href="https://necromuralist.github.io/Neurotic-Networking/index-21.html" rel="canonical">
<link href="." rel="prev" type="text/html">
<link href="index-20.html" rel="next" type="text/html"><!--[if lt IE 9]><script src="assets/js/html5.js"></script><![endif]-->
<link href="apple-touch-icon.png" rel="apple-touch-icon" sizes="180x180">
<link href="favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="site.webmanifest" rel="manifest">
</head>
<body>
<a class="sr-only sr-only-focusable" href="#content">Skip to main content</a> <!-- Menubar -->
<nav class="navbar navbar-expand-md static-top mb-4 navbar-light bg-light">
<div class="container"><!-- This keeps the margins nice -->
 <a class="navbar-brand" href="."><span id="blog-title">Neurotic Networking</span></a> <button aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation" class="navbar-toggler" data-target="#bs-navbar" data-toggle="collapse" type="button"><span class="navbar-toggler-icon"></span></button>
<div class="collapse navbar-collapse" id="bs-navbar">
<ul class="navbar-nav mr-auto">
<li class="nav-item"><a class="nav-link" href="archive.html">Archive</a></li>
<li class="nav-item"><a class="nav-link" href="categories/">Tags</a></li>
<li class="nav-item"><a class="nav-link" href="rss.xml">RSS feed</a></li>
<li class="nav-item"><a class="nav-link" href="https://necromuralist.github.io/">Cloistered Monkey</a></li>
</ul>
<!-- Google custom search -->
<form action="https://www.google.com/search" class="navbar-form navbar-right" method="get" role="search">
<div class="form-group"><input class="form-control" name="q" placeholder="Search" type="text"></div>
<!-- 
<button type="submit" class="btn btn-primary">
        <span class="glyphicon glyphicon-search"></span>
</button>
-->
<input name="sitesearch" type="hidden" value="https://necromuralist.github.io/Neurotic-Networking/"></form>
<!-- End of custom search -->
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse --></div>
<!-- /.container --></nav>
<!-- End of Menubar -->
<div class="container" id="content" role="main">
<div class="body-content"><!--Body content-->
<div class="postindex">
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/first-course/basic-numpy-for-neural-networks/">Basic Numpy for Neural Networks</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/first-course/basic-numpy-for-neural-networks/" rel="bookmark"><time class="published dt-published" datetime="2021-02-18T12:55:15-08:00" itemprop="datePublished" title="2021-02-18 12:55">2021-02-18 12:55</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org7e04747">Beginning</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org67c456f">Imports</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org14b3891">Set Up</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org065bd7c">Building basic functions with numpy</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org4207267">sigmoid function, np.exp()</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org140386f">Build a function that returns the sigmoid of a real number x using math.exp(x) for the exponential function.</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org2193c20">One reason why we use "numpy" instead of "math" in Deep Learning</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org1ff2ca6">Implement the sigmoid function using numpy.</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org52df31d">Sigmoid gradient</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgc0fb0d6">Plotting The Sigmoid and Its Derivative</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org033f438">Reshaping arrays</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orga755010">Normalizing rows</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgc4230cc">Broadcasting and the softmax function</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgc2c830a">Vectorization</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgfe5bb1d">Classic (Non-Vectorized)</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org50f870b">Dot Product Of Vectors Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgd76e78c">Outer Product Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgf15441f">Elementwise Implementation</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org76679aa">General Dot Product Implementation</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org7f9cd49">Vectorized</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgb5fc421">Dot Product Of Vectors</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#orgfdf57a7">Outer Product</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org473a22c">Elementwise Multiplication</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org68bbbc1">General Dot Product</a></li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org5f03c4c">The L1 and L2 loss functions</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org337ba8b">L1</a></li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org309e1f3">L2 Loss</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org047eeea">End</a>
<ul>
<li><a href="posts/first-course/basic-numpy-for-neural-networks/#org9ef33e8">Source</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org7e04747">
<h2 id="org7e04747">Beginning</h2>
<div class="outline-text-2" id="text-org7e04747">
<p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape.</p>
</div>
<div class="outline-3" id="outline-container-org67c456f">
<h3 id="org67c456f">Imports</h3>
<div class="outline-text-3" id="text-org67c456f">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">expects</span> <span class="kn">import</span> <span class="n">be_true</span><span class="p">,</span> <span class="n">equal</span><span class="p">,</span> <span class="n">expect</span>

<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># my stuff</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org14b3891">
<h3 id="org14b3891">Set Up</h3>
<div class="outline-text-3" id="text-org14b3891">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">slug</span> <span class="o">=</span> <span class="s2">"basic-numpy-for-neural-networks"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/first-course/</span><span class="si">{</span><span class="n">slug</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org065bd7c">
<h2 id="org065bd7c">Building basic functions with numpy</h2>
<div class="outline-text-2" id="text-org065bd7c"></div>
<div class="outline-3" id="outline-container-org4207267">
<h3 id="org4207267">sigmoid function, np.exp()</h3>
<div class="outline-text-3" id="text-org4207267">
<p>Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp(). (see <a href="https://docs.python.org/3/library/math.html#power-and-logarithmic-functions">python's power and logarithmic functions</a>)</p>
</div>
</div>
<div class="outline-3" id="outline-container-org140386f">
<h3 id="org140386f">Build a function that returns the sigmoid of a real number x using math.exp(x) for the exponential function.</h3>
<div class="outline-text-3" id="text-org140386f">
<p>\(sigmoid(x) = \frac{1}{1+e^{-x}}\) is sometimes also known as the <i>logistic function</i>. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">TOLERANCE</span> <span class="o">=</span> <span class="mf">0.000001</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sd">"""Compute sigmoid of x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: sigmoid(x)</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">expected</span> <span class="o">=</span> <span class="mf">0.9525741268224334</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">basic_sigmoid</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

<span class="n">expect</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">rel_tol</span><span class="o">=</span><span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"sigmoid of 3: </span><span class="si">{</span><span class="n">actual</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
<p>Actually, we rarely use the "math" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org2193c20">
<h3 id="org2193c20">One reason why we use "numpy" instead of "math" in Deep Learning</h3>
<div class="outline-text-3" id="text-org2193c20">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="k">try</span><span class="p">:</span>
    <span class="n">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># you will see this give an error when you run it, because x is a vector.</span>
<span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
</pre></div>
<p>In fact, if \(x = (x_1, x_2, ..., x_n)\) is a row vector then \(np.exp(x)\) will apply the exponential function to every element of x. The output will thus be: \(np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})\). (see <a href="https://numpy.org/doc/stable/reference/generated/numpy.exp.html">numpy.exp</a>).</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
<pre class="example">
[ 2.71828183  7.3890561  20.08553692]
</pre>
<p>Furthermore, if x is a vector, then a Python operation such as \(s = x + 3\) or \(s = \frac{1}{x}\) will output s as a vector of the same size as x.</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
<pre class="example">
[4 5 6]
</pre>
<p>Any time you need more info on a numpy function, we encourage you to look at <a href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html">the official documentation</a>.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org1ff2ca6">
<h3 id="org1ff2ca6">Implement the sigmoid function using numpy.</h3>
<div class="outline-text-3" id="text-org1ff2ca6">
<p><b>Instructions</b>: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don't need to know more for now.</p>
<p>\[ \text{For } x \in \mathbb{R}^n \text{, } sigmoid(x) = sigmoid\begin{pmatrix} x_1 \\ x_2 \\ ... \\ x_n \\ \end{pmatrix} = \begin{pmatrix} \frac{1}{1+e^{-x_1}} \\ \frac{1}{1+e^{-x_2}} \\ ... \\ \frac{1}{1+e^{-x_n}} \\ \end{pmatrix}\tag{1} \]</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute the sigmoid of x</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar or numpy array of any size</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: sigmoid(x)</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span> <span class="mf">0.73105858</span><span class="p">,</span>  <span class="mf">0.88079708</span><span class="p">,</span>  <span class="mf">0.95257413</span><span class="p">])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
[0.73105858 0.88079708 0.95257413]
</pre></div>
</div>
<div class="outline-3" id="outline-container-org52df31d">
<h3 id="org52df31d">Sigmoid gradient</h3>
<div class="outline-text-3" id="text-org52df31d">
<p>You will need to compute gradients to optimize loss functions using backpropagation. Let's code your first gradient function.</p>
<p>The formula is:</p>
<p>\[ sigmoid\_derivative(x) = \sigma'(x) = \sigma(x) (1 - \sigma(x))\tag{2} \]</p>
<p>You often code this function in two steps:</p>
<ol class="org-ol">
<li>Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.</li>
<li>Compute \(\sigma'(x) = s(1-s)\)</li>
</ol>
<p><a href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html">numpy.random.randn</a> generates a sample from the standard normal distribution.</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Compute the gradient (also called the slope or derivative) of the sigmoid</span>
<span class="sd">    function with respect to its input x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A scalar or numpy array</span>

<span class="sd">    Returns:</span>
<span class="sd">     ds: Your computed gradient.</span>
<span class="sd">    """</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.19661193</span><span class="p">,</span> <span class="mf">0.10499359</span><span class="p">,</span> <span class="mf">0.04517666</span><span class="p">])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"sigmoid_derivative(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]
</pre></div>
<div class="outline-4" id="outline-container-orgc0fb0d6">
<h4 id="orgc0fb0d6">Plotting The Sigmoid and Its Derivative</h4>
<div class="outline-text-4" id="text-orgc0fb0d6">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">siggy</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">siggy_slope</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">Sigmoid</span><span class="o">=</span><span class="n">siggy</span><span class="p">,</span> <span class="n">Slope</span><span class="o">=</span><span class="n">siggy_slope</span><span class="p">))</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">"Sigmoid and Derivative"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">fontscale</span><span class="p">,</span>
    <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"sigmoid"</span><span class="p">)()</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/first-course/basic-numpy-for-neural-networks/sigmoid.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object></div>
</div>
</div>
<div class="outline-3" id="outline-container-org033f438">
<h3 id="org033f438">Reshaping arrays</h3>
<div class="outline-text-3" id="text-org033f438">
<p>Two common numpy functions used in deep learning are <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">np.shape</a> and <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html">np.reshape()</a>.</p>
<ul class="org-ul">
<li>X.shape is used to get the shape (dimension) of a matrix/vector X.</li>
<li>X.reshape(…) is used to reshape X into some other dimension.</li>
</ul>
<p>For example, in computer science, an image is represented by a 3D array of shape \((length, height, depth = 3)\). However, when you read an image as the input of an algorithm you convert it to a vector of shape \((length \times height \times 3, 1)\). In other words, you "unroll", or reshape, the 3D array into a 1D vector.</p>
<p>We'll implemnt <code>image2vector()</code>, a function that takes an input of shape (length, height, 3) and returns a vector of shape \((length\times height\times 3, 1)\). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (\(a \times b, c\)) you would do:</p>
<pre class="example">
v = v.reshape((v.shape[0] * v.shape[1], v.shape[2]))
</pre>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">image2vector</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Unroll the image</span>

<span class="sd">    Args:</span>
<span class="sd">     image: array of shape (length, height, depth)</span>

<span class="sd">    Returns:</span>
<span class="sd">     v: vector of shape (length*height*depth, 1)</span>
<span class="sd">    """</span>
    <span class="n">length</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">return</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">length</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">depth</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
<p>Our image will a 3 by 3 by 2 array, typically images will be \((\textrm{number of pixels}_x, \textrm{number of pixels}_y,3)\) where 3 represents the RGB values</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span> <span class="mf">0.67826139</span><span class="p">,</span>  <span class="mf">0.29380381</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.90714982</span><span class="p">,</span>  <span class="mf">0.52835647</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.4215251</span> <span class="p">,</span>  <span class="mf">0.45017551</span><span class="p">]],</span>

                     <span class="p">[[</span> <span class="mf">0.92814219</span><span class="p">,</span>  <span class="mf">0.96677647</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.85304703</span><span class="p">,</span>  <span class="mf">0.52351845</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.19981397</span><span class="p">,</span>  <span class="mf">0.27417313</span><span class="p">]],</span>

                     <span class="p">[[</span> <span class="mf">0.60659855</span><span class="p">,</span>  <span class="mf">0.00533165</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.10820313</span><span class="p">,</span>  <span class="mf">0.49978937</span><span class="p">],</span>
                      <span class="p">[</span> <span class="mf">0.34144279</span><span class="p">,</span>  <span class="mf">0.94630077</span><span class="p">]]])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.67826139</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.29380381</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.90714982</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.52835647</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.4215251</span> <span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.45017551</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.92814219</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.96677647</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.85304703</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.52351845</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.19981397</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.27417313</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.60659855</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.00533165</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.10820313</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.49978937</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.34144279</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.94630077</span><span class="p">]])</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">image2vector</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"image2vector(image) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">length</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">length</span> <span class="o">*</span> <span class="n">height</span> <span class="o">*</span> <span class="n">depth</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
image2vector(image) = [[0.67826139]
 [0.29380381]
 [0.90714982]
 [0.52835647]
 [0.4215251 ]
 [0.45017551]
 [0.92814219]
 [0.96677647]
 [0.85304703]
 [0.52351845]
 [0.19981397]
 [0.27417313]
 [0.60659855]
 [0.00533165]
 [0.10820313]
 [0.49978937]
 [0.34144279]
 [0.94630077]]
</pre></div>
</div>
<div class="outline-3" id="outline-container-orga755010">
<h3 id="orga755010">Normalizing rows</h3>
<div class="outline-text-3" id="text-orga755010">
<p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to \) \frac{x}{\| x\|} \) (dividing each row vector of x by its norm).</p>
<p>For example, if \[ x = \begin{bmatrix} 0 & 3 & 4 \\ 2 & 6 & 4 \\ \end{bmatrix}\tag{3} \]</p>
<p>then</p>
<p>\[ \| x\| = np.linalg.norm(x, axis = 1, keepdims = True) = \begin{bmatrix} 5 \\ \sqrt{56} \\ \end{bmatrix}\tag{4} \] and</p>
<p>\[ x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix} 0 & \frac{3}{5} & \frac{4}{5} \\ \frac{2}{\sqrt{56}} & \frac{6}{\sqrt{56}} & \frac{4}{\sqrt{56}} \\ \end{bmatrix}\tag{5} \]</p>
<p>Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you're going to learn about it further down.</p>
<p>Now we'll implement <code>normalizeRows()</code> to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p>
<p>See: <a href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a></p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">normalizeRows</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""</span>
<span class="sd">    Implement a function that normalizes each row of the matrix x </span>
<span class="sd">    (to have unit length).</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A numpy matrix of shape (n, m)</span>

<span class="sd">    Returns:</span>
<span class="sd">     x: The normalized (by row) numpy matrix.</span>
<span class="sd">    """</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">/</span><span class="n">x_norm</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">0.13736056</span><span class="p">,</span>  <span class="mf">0.82416338</span><span class="p">,</span>  <span class="mf">0.54944226</span><span class="p">]])</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">normalizeRows</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"normalizeRows(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
normalizeRows(x) = [[0.         0.6        0.8       ]
 [0.13736056 0.82416338 0.54944226]]
</pre>
<p>We can check that each row is a unit vector by calculating the Euclidean distance.</p>
<p>\[ Euclidean = \sqrt{\sum X^2} \]</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">SUM_ROWS</span> <span class="o">=</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">actual</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">SUM_ROWS</span><span class="p">)))</span>
</pre></div>
<pre class="example">
[1. 1.]
</pre>
<p><b>Note</b>: <code>x_norm</code> and <code>x</code> have different shapes. This is normal given that <code>x_norm</code> takes the norm of each row of <code>x</code>. So <code>x_norm</code> has the same number of rows but only 1 column. As a consequence you can't use <code>x /= x_norm</code> instead of <code>x = x/x_norm</code>. So how did it work when you divided <code>x</code> by <code>x_norm</code>? This is called broadcasting and we'll talk about it next.</p>
</div>
<div class="outline-4" id="outline-container-orgc4230cc">
<h4 id="orgc4230cc">Broadcasting and the softmax function</h4>
<div class="outline-text-4" id="text-orgc4230cc">
<p>A very important concept to understand in numpy is "broadcasting". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">broadcasting documentation</a>.</p>
<p>We'll implement a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes.</p>
<p><b>The Mathy Definitions</b>: \[ \text{for } x \in \mathbb{R}^{1\times n} \text{, } softmax(x) = softmax(\begin{bmatrix} x_1 &amp;& x_2 &amp;& \ldots &amp;& x_n \end{bmatrix}) = \begin{bmatrix} \frac{e^{x_1}}{\sum_{j}e^{x_j}} &amp;& \frac{e^{x_2}}{\sum_{j}e^{x_j}} &amp;& \ldots &amp;& \frac{e^{x_n}}{\sum_{j}e^{x_j}} \end{bmatrix} \]</p>
<p>\(\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{, x_{ij}}\) maps to the element in the \(i^{th}\) row and \(j^{th}\) column of <i>x</i>, thus we have: \[ softmax(x) = softmax\begin{bmatrix} x_{11} & x_{12} & x_{13} & \dots & x_{1n} \\ x_{21} & x_{22} & x_{23} & \dots & x_{2n} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ x_{m1} & x_{m2} & x_{m3} & \dots & x_{mn} \end{bmatrix} = \begin{bmatrix} \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} & \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} & \dots & \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\ \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} & \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} & \dots & \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} & \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} & \dots & \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}} \end{bmatrix} = \begin{pmatrix} softmax\text{(first row of x)} \\ softmax\text{(second row of x)} \\ \ldots \\ softmax\text{(last row of x)} \\ \end{pmatrix} \]</p>
<p>See also: <a href="https://numpy.org/doc/stable/reference/generated/numpy.sum.html">numpy.sum</a></p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">ROW_SUMS</span> <span class="o">=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculates the softmax for each row of the input x.</span>

<span class="sd">    Args:</span>
<span class="sd">     x: A numpy matrix of shape (n,m)</span>

<span class="sd">    Returns:</span>
<span class="sd">     s: A numpy matrix equal to the softmax of x, of shape (n,m)</span>
<span class="sd">    """</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x_sum</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">ROW_SUMS</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_exp</span><span class="o">/</span><span class="n">x_sum</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mf">9.80897665e-01</span><span class="p">,</span> <span class="mf">8.94462891e-04</span><span class="p">,</span> <span class="mf">1.79657674e-02</span><span class="p">,</span>
                          <span class="mf">1.21052389e-04</span><span class="p">,</span> <span class="mf">1.21052389e-04</span><span class="p">],</span>
                        <span class="p">[</span> <span class="mf">8.78679856e-01</span><span class="p">,</span> <span class="mf">1.18916387e-01</span><span class="p">,</span> <span class="mf">8.01252314e-04</span><span class="p">,</span>
                          <span class="mf">8.01252314e-04</span><span class="p">,</span> <span class="mf">8.01252314e-04</span><span class="p">]])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span><span class="mi">0</span><span class="p">]])</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"softmax(x) = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">actual</span><span class="p">,</span> <span class="n">TOLERANCE</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">be_true</span><span class="p">)</span>
</pre></div>
<pre class="example">
softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04
  1.21052389e-04]
 [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04
  8.01252314e-04]]
</pre>
<p><b>Note</b>:</p>
<ul class="org-ul">
<li>If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). <b>x_exp/x_sum</b> works due to python broadcasting.</li>
</ul>
<p><b>What you need to remember:</b></p>
<ul class="org-ul">
<li>np.exp(x) works for any np.array x and applies the exponential function to every coordinate</li>
<li>the sigmoid function and its gradient</li>
<li>Some equivalent of <code>image2vector</code> is commonly used in deep learning</li>
<li>np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.</li>
<li>numpy has efficient built-in functions</li>
<li>broadcasting is extremely useful</li>
</ul>
</div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgc2c830a">
<h2 id="orgc2c830a">Vectorization</h2>
<div class="outline-text-2" id="text-orgc2c830a">
<p>In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">CLASSIC</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</pre></div>
</div>
<div class="outline-3" id="outline-container-orgfe5bb1d">
<h3 id="orgfe5bb1d">Classic (Non-Vectorized)</h3>
<div class="outline-text-3" id="text-orgfe5bb1d"></div>
<div class="outline-4" id="outline-container-org50f870b">
<h4 id="org50f870b">Dot Product Of Vectors Implementation</h4>
<div class="outline-text-4" id="text-org50f870b">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">dot</span><span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"dot"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"dot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'dot'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
dot = 278 
 ----- Computation time = 0.09222100000005895 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgd76e78c">
<h4 id="orgd76e78c">Outer Product Implementation</h4>
<div class="outline-text-4" id="text-orgd76e78c">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)):</span>
        <span class="n">outer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"outer"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="sa">f</span><span class="s2">"outer = </span><span class="si">{</span><span class="n">outer</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'outer'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
 ----- Computation time = 0.2285300000002266 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-orgf15441f">
<h4 id="orgf15441f">Elementwise Implementation</h4>
<div class="outline-text-4" id="text-orgf15441f">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">mul</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"elementwise"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"elementwise multiplication = </span><span class="si">{</span><span class="n">mul</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'elementwise'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]
 ----- Computation time = 0.10630600000016699 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-org76679aa">
<h4 id="org76679aa">General Dot Product Implementation</h4>
<div class="outline-text-4" id="text-org76679aa">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">W</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">gdot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
        <span class="n">gdot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">*</span><span class="n">x1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">CLASSIC</span><span class="p">[</span><span class="s2">"general_dot"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"gdot = </span><span class="si">{</span><span class="n">gdot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'general_dot'</span><span class="p">]</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
gdot = [26.7997887  21.98533453 17.23427487]
 ----- Computation time = 0.14043400000041117 ms
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org7f9cd49">
<h3 id="org7f9cd49">Vectorized</h3>
<div class="outline-text-3" id="text-org7f9cd49"></div>
<div class="outline-4" id="outline-container-orgb5fc421">
<h4 id="orgb5fc421">Dot Product Of Vectors</h4>
<div class="outline-text-4" id="text-orgb5fc421">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">DOT</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"dot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">DOT</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'dot'</span><span class="p">]</span> <span class="o">-</span> <span class="n">DOT</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
dot = 278
 ----- Computation time = 0.11425399999964725 ms
Difference: -0.0220329999995883 ms
</pre>
<p>So for this small set, the pure python is faster.</p>
</div>
</div>
<div class="outline-4" id="outline-container-orgfdf57a7">
<h4 id="orgfdf57a7">Outer Product</h4>
<div class="outline-text-4" id="text-orgfdf57a7">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">OUTER</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"outer = </span><span class="si">{</span><span class="n">outer</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">OUTER</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'outer'</span><span class="p">]</span> <span class="o">-</span> <span class="n">OUTER</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]
 ----- Computation time = 0.09857899999943243 ms
Difference: 0.12995100000079418 ms
</pre>
<p>Now numpy is a little faster.</p>
</div>
</div>
<div class="outline-4" id="outline-container-org473a22c">
<h4 id="org473a22c">Elementwise Multiplication</h4>
<div class="outline-text-4" id="text-org473a22c">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">ELEMENTWISE</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"elementwise multiplication = </span><span class="si">{</span><span class="n">mul</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">ELEMENTWISE</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'elementwise'</span><span class="p">]</span> <span class="o">-</span> <span class="n">ELEMENTWISE</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]
 ----- Computation time = 0.07506199999962604 ms
Difference: 0.03124400000054095 ms
</pre></div>
</div>
<div class="outline-4" id="outline-container-org68bbbc1">
<h4 id="org68bbbc1">General Dot Product</h4>
<div class="outline-text-4" id="text-org68bbbc1">WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">GENERAL</span> <span class="o">=</span> <span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"gdot = </span><span class="si">{</span><span class="n">dot</span><span class="si">}</span><span class="se">\n</span><span class="s2"> ----- Computation time = </span><span class="si">{</span><span class="n">GENERAL</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Difference: </span><span class="si">{</span><span class="n">CLASSIC</span><span class="p">[</span><span class="s1">'general_dot'</span><span class="p">]</span> <span class="o">-</span> <span class="n">GENERAL</span><span class="si">}</span><span class="s2"> ms"</span><span class="p">)</span>
</pre></div>
<pre class="example">
gdot = [26.7997887  21.98533453 17.23427487]
 ----- Computation time = 0.10962399999936423 ms
Difference: 0.030810000001046944 ms
</pre>
<p>As you may have noticed, the vectorized implementation is much cleaner and somewhat more efficient. For bigger vectors/matrices, the differences in running time become even bigger.</p>
<p><b>Note</b> that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to <code>.*</code> in Matlab/Octave), which performs an element-wise multiplication.</p>
</div>
</div>
</div>
<div class="outline-3" id="outline-container-org5f03c4c">
<h3 id="org5f03c4c">The L1 and L2 loss functions</h3>
<div class="outline-text-3" id="text-org5f03c4c"></div>
<div class="outline-4" id="outline-container-org337ba8b">
<h4 id="org337ba8b">L1</h4>
<div class="outline-text-4" id="text-org337ba8b">
<p>Now we'll implement the numpy vectorized version of the L1 loss.</p>
<p><b>Reminder</b>: The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions (\( \hat{y} \)) are from the true values (<i>y</i>). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost. L1 loss is defined as:</p>
<p>\[ \begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m \left|y^{(i)} - \hat{y}^{(i)}\right| \end{align*}\tag{6} \]</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""L1 Loss</span>

<span class="sd">    Args:</span>
<span class="sd">     yhat: vector of size m (predicted labels)</span>
<span class="sd">     y: vector of size m (true labels)</span>

<span class="sd">    Returns:</span>
<span class="sd">     loss: the value of the L1 loss function defined above</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">))</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">1.1</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"L1 = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<pre class="example">
L1 = 1.1
</pre></div>
</div>
<div class="outline-4" id="outline-container-org309e1f3">
<h4 id="org309e1f3">L2 Loss</h4>
<div class="outline-text-4" id="text-org309e1f3">
<p>Next we'll implement the numpy vectorized version of the L2 loss. There are several ways of implementing the L2 loss but we'll use the function np.dot(). As a reminder, if \(x = [x_1, x_2, \ldots, x_n]\), then <code>np.dot(x,x)</code> = \(\sum_{j=0}^n x_j^{2}\).</p>
<p>L2 loss is defined as \[ L_2(\hat{y},y) = \sum_{i=0}^m\left(y^{(i)} - \hat{y}^{(i)}\right)^2\tag{7} \]</p>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
    <span class="sd">"""Calculate the L2 Loss</span>

<span class="sd">    Args:</span>
<span class="sd">     yhat: vector of size m (predicted labels)</span>
<span class="sd">     y: vector of size m (true labels)</span>

<span class="sd">    Returns:</span>
<span class="sd">     loss: the value of the L2 loss function defined above</span>
<span class="sd">    """</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
WARNING: `pyenv init -` no longer sets PATH. Run `pyenv init` to see the necessary changes to make to your configuration.
<div class="highlight">
<pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">expected</span> <span class="o">=</span> <span class="mf">0.43</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"L2 = </span><span class="si">{</span><span class="n">actual</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">expect</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">equal</span><span class="p">(</span><span class="n">expected</span><span class="p">))</span>
</pre></div>
<pre class="example">
L2 = 0.43
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org047eeea">
<h2 id="org047eeea">End</h2>
<div class="outline-text-2" id="text-org047eeea">
<p><b>What to remember:</b></p>
<ul class="org-ul">
<li>Vectorization is very important in deep learning. It provides computational efficiency and clarity.</li>
<li>You have reviewed the L1 and L2 loss.</li>
<li>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…</li>
</ul>
</div>
<div class="outline-3" id="outline-container-org9ef33e8">
<h3 id="org9ef33e8">Source</h3>
<div class="outline-text-3" id="text-org9ef33e8">
<p>This was an exercise from DeepLearning.ai's first Coursera course. (link to come)</p>
</div>
</div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/neural-machine-translation-testing-the-model/">Neural Machine Translation: Testing the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/neural-machine-translation-testing-the-model/" rel="bookmark"><time class="published dt-published" datetime="2021-02-14T14:54:56-08:00" itemprop="datePublished" title="2021-02-14 14:54">2021-02-14 14:54</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/neural-machine-translation-testing-the-model/#org1aaeea6">Testing the Model</a></li>
<li><a href="posts/nlp/neural-machine-translation-testing-the-model/#orgbaf1cf6">End</a></li>
<li><a href="posts/nlp/neural-machine-translation-testing-the-model/#org1ebb3f8">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-org1aaeea6">
<h2 id="org1aaeea6">Testing the Model</h2>
<div class="outline-text-2" id="text-org1aaeea6">
<p>In the <a href="posts/nlp/neural-machine-translation-training-the-model/">previous post</a> we trained our machine translation model so now it's time to test it and see how well it does.</p>
</div>
</div>
<div class="outline-2" id="outline-container-orgbaf1cf6">
<h2 id="orgbaf1cf6">End</h2>
<div class="outline-text-2" id="text-orgbaf1cf6">
<p>The overview post with links to all the posts in this series is <a href="posts/nlp/neural-machine-translation/">here</a>.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org1ebb3f8">
<h2 id="org1ebb3f8">Raw</h2>
<div class="outline-text-2" id="text-org1ebb3f8">
<pre class="example">
# # Part 4:  Testing
# 
# We will now be using the model you just trained to translate English sentences to German. We will implement this with two functions: The first allows you to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string.
# 
# We will start by first loading in a pre-trained copy of the model you just coded. Please run the cell below to do just that.

# In[ ]:


# instantiate the model we built in eval mode
model = NMTAttn(mode='eval')

# initialize weights from a pre-trained model
model.init_from_file("model.pkl.gz", weights_only=True)
model = tl.Accelerate(model)


# &lt;a name="4.1"&gt;&lt;/a&gt;
# ## 4.1  Decoding
# 
# As discussed in the lectures, there are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method. Let's briefly look at its implementation:
# 
# ```python
# def logsoftmax_sample(log_probs, temperature=1.0):  # pylint: disable=invalid-name
#   """Returns a sample from a log-softmax output, with temperature.
# 
#   Args:
#     log_probs: Logarithms of probabilities (often coming from LogSofmax)
#     temperature: For scaling before sampling (1.0 = default, 0.0 = pick argmax)
#   """
#   # This is equivalent to sampling from a softmax with temperature.
#   u = np.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)
#   g = -np.log(-np.log(u))
#   return np.argmax(log_probs + g * temperature, axis=-1)
# ```
# 
# The key things to take away here are: 1. it gets random samples with the same shape as your input (i.e. `log_probs`), and 2. the amount of "noise" added to the input by these random samples is scaled by a `temperature` setting. You'll notice that setting it to `0` will just make the return statement equal to getting the argmax of `log_probs`. This will come in handy later. 
# 
# &lt;a name="ex06"&gt;&lt;/a&gt;
# ### Exercise 06
# 
# **Instructions:** Implement the `next_symbol()` function that takes in the `input_tokens` and the `cur_output_tokens`, then return the index of the next word. You can click below for hints in completing this exercise.
# 
# &lt;details&gt;    
# &lt;summary&gt;
#     &lt;font size="3" color="darkgreen"&gt;&lt;b&gt;Click Here for Hints&lt;/b&gt;&lt;/font&gt;
# &lt;/summary&gt;
# &lt;p&gt;
# &lt;ul&gt;
#     &lt;li&gt;To get the next power of two, you can compute &lt;i&gt;2^log_2(token_length + 1)&lt;/i&gt; . We add 1 to avoid &lt;i&gt;log(0).&lt;/i&gt;&lt;/li&gt;
#     &lt;li&gt;You can use &lt;i&gt;np.ceil()&lt;/i&gt; to get the ceiling of a float.&lt;/li&gt;
#     &lt;li&gt;&lt;i&gt;np.log2()&lt;/i&gt; will get the logarithm base 2 of a value&lt;/li&gt;
#     &lt;li&gt;&lt;i&gt;int()&lt;/i&gt; will cast a value into an integer type&lt;/li&gt;
#     &lt;li&gt;From the model diagram in part 2, you know that it takes two inputs. You can feed these with this syntax to get the model outputs: &lt;i&gt;model((input1, input2))&lt;/i&gt;. It's up to you to determine which variables below to substitute for input1 and input2. Remember also from the diagram that the output has two elements: [log probabilities, target tokens]. You won't need the target tokens so we assigned it to _ below for you. &lt;/li&gt;
#     &lt;li&gt; The log probabilities output will have the shape: (batch size, decoder length, vocab size). It will contain log probabilities for each token in the &lt;i&gt;cur_output_tokens&lt;/i&gt; plus 1 for the start symbol introduced by the ShiftRight in the preattention decoder. For example, if cur_output_tokens is [1, 2, 5], the model will output an array of log probabilities each for tokens 0 (start symbol), 1, 2, and 5. To generate the next symbol, you just want to get the log probabilities associated with the last token (i.e. token 5 at index 3). You can slice the model output at [0, 3, :] to get this. It will be up to you to generalize this for any length of cur_output_tokens &lt;/li&gt;
# &lt;/ul&gt;
# 

# In[ ]:


# UNQ_C6
# GRADED FUNCTION
def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):
    """Returns the index of the next token.

    Args:
        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.
        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence
        cur_output_tokens (list): tokenized representation of previously translated words
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)

    Returns:
        int: index of the next token in the translated sentence
        float: log probability of the next symbol
    """

    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###

    # set the length of the current output tokens
    token_length = None

    # calculate next power of 2 for padding length 
    padded_length = None

    # pad cur_output_tokens up to the padded_length
    padded = cur_output_tokens + None
    
    # model expects the output to have an axis for the batch size in front so
    # convert `padded` list to a numpy array with shape (x, &lt;padded_length&gt;) where the
    # x position is the batch axis. (hint: you can use np.expand_dims() with axis=0 to insert a new axis)
    padded_with_batch = None

    # get the model prediction. remember to use the `NMTAttn` argument defined above.
    # hint: the model accepts a tuple as input (e.g. `my_model((input1, input2))`)
    output, _ = None
    
    # get log probabilities from the last token output
    log_probs = output[None]

    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)
    symbol = None
    
    ### END CODE HERE ###

    return symbol, float(log_probs[symbol])


# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_next_symbol(next_symbol, model)
# END UNIT TEST


# Now you will implement the `sampling_decode()` function. This will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string.
# 
# &lt;a name="ex07"&gt;&lt;/a&gt;
# ### Exercise 07
# 
# **Instructions**: Implement the `sampling_decode()` function.

# In[ ]:


# UNQ_C7
# GRADED FUNCTION
def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):
    """Returns the translated sentence.

    Args:
        input_sentence (str): sentence to translate.
        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)
        vocab_file (str): filename of the vocabulary
        vocab_dir (str): path to the vocabulary file

    Returns:
        tuple: (list, str, float)
            list of int: tokenized version of the translated sentence
            float: log probability of the translated sentence
            str: the translated sentence
    """
    
    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
    
    # encode the input sentence
    input_tokens = None
    
    # initialize the list of output tokens
    cur_output_tokens = None
    
    # initialize an integer that represents the current output index
    cur_output = None
    
    # Set the encoding of the "end of sentence" as 1
    EOS = None
    
    # check that the current output is not the end of sentence token
    while cur_output != EOS:
        
        # update the current output token by getting the index of the next word (hint: use next_symbol)
        cur_output, log_prob = None
        
        # append the current output token to the list of output tokens
        cur_output_tokens.append(cur_output)
    
    # detokenize the output tokens
    sentence = None
    
    ### END CODE HERE ###
    
    return cur_output_tokens, log_prob, sentence


# In[ ]:


# Test the function above. Try varying the temperature setting with values from 0 to 1.
# Run it several times with each setting and see how often the output changes.
sampling_decode("I love languages.", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)


# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_sampling_decode(sampling_decode, model)
# END UNIT TEST


# We have set a default value of `0` to the temperature setting in our implementation of `sampling_decode()` above. As you may have noticed in the `logsoftmax_sample()` method, this setting will ultimately result in greedy decoding. As mentioned in the lectures, this algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. See the testing function and sample inputs below. You'll notice that the output will remain the same each time you run it.

# In[ ]:


def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):
    """Prints the input and output of our NMTAttn model using greedy decode

    Args:
        sentence (str): a custom string.
        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.
        vocab_file (str): filename of the vocabulary
        vocab_dir (str): path to the vocabulary file

    Returns:
        str: the translated sentence
    """
    
    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)
    
    print("English: ", sentence)
    print("German: ", translated_sentence)
    
    return translated_sentence


# In[ ]:


# put a custom string here
your_sentence = 'I love languages.'

greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);


# In[ ]:


greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);


# &lt;a name="4.2"&gt;&lt;/a&gt;
# ## 4.2  Minimum Bayes-Risk Decoding
# 
# As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:
# 
# 1. take several random samples
# 2. score each sample against all other samples
# 3. select the one with the highest score
# 
# You will be building helper functions for these steps in the following sections.

# &lt;a name='4.2.1'&gt;&lt;/a&gt;
# ### 4.2.1 Generating samples
# 
# First, let's build a function to generate several samples. You can use the `sampling_decode()` function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step.

# In[ ]:


def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):
    """Generates samples using sampling_decode()

    Args:
        sentence (str): sentence to translate.
        n_samples (int): number of samples to generate
        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)
        vocab_file (str): filename of the vocabulary
        vocab_dir (str): path to the vocabulary file
        
    Returns:
        tuple: (list, list)
            list of lists: token list per sample
            list of floats: log probability per sample
    """
    # define lists to contain samples and probabilities
    samples, log_probs = [], []

    # run a for loop to generate n samples
    for _ in range(n_samples):
        
        # get a sample using the sampling_decode() function
        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)
        
        # append the token list to the samples list
        samples.append(sample)
        
        # append the log probability to the log_probs list
        log_probs.append(logp)
                
    return samples, log_probs


# In[ ]:


# generate 4 samples with the default temperature (0.6)
generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)


# ### 4.2.2 Comparing overlaps
# 
# Let us now build our functions to compare a sample against another. There are several metrics available as shown in the lectures and you can try experimenting with any one of these. For this assignment, we will be calculating scores for unigram overlaps. One of the more simple metrics is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets. We've already implemented it below for your perusal.

# In[ ]:


def jaccard_similarity(candidate, reference):
    """Returns the Jaccard similarity between two token lists

    Args:
        candidate (list of int): tokenized version of the candidate translation
        reference (list of int): tokenized version of the reference translation

    Returns:
        float: overlap between the two token lists
    """
    
    # convert the lists to a set to get the unique tokens
    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  
    
    # get the set of tokens common to both candidate and reference
    joint_elems = can_unigram_set.intersection(ref_unigram_set)
    
    # get the set of all tokens found in either candidate or reference
    all_elems = can_unigram_set.union(ref_unigram_set)
    
    # divide the number of joint elements by the number of all elements
    overlap = len(joint_elems) / len(all_elems)
    
    return overlap


# In[ ]:


# let's try using the function. remember the result here and compare with the next function below.
jaccard_similarity([1, 2, 3], [1, 2, 3, 4])


# One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:
# 
# $$score = 2* \frac{(precision * recall)}{(precision + recall)}$$
# 
# &lt;a name="ex08"&gt;&lt;/a&gt;
# ### Exercise 08
# 
# **Instructions**: Implement the `rouge1_similarity()` function.

# In[ ]:


# UNQ_C8
# GRADED FUNCTION

# for making a frequency table easily
from collections import Counter

def rouge1_similarity(system, reference):
    """Returns the ROUGE-1 score between two token lists

    Args:
        system (list of int): tokenized version of the system translation
        reference (list of int): tokenized version of the reference translation

    Returns:
        float: overlap between the two token lists
    """    
    
    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
    
    # make a frequency table of the system tokens (hint: use the Counter class)
    sys_counter = None
    
    # make a frequency table of the reference tokens (hint: use the Counter class)
    ref_counter = None
    
    # initialize overlap to 0
    overlap = None
    
    # run a for loop over the sys_counter object (can be treated as a dictionary)
    for token in sys_counter:
        
        # lookup the value of the token in the sys_counter dictionary (hint: use the get() method)
        token_count_sys = None
        
        # lookup the value of the token in the ref_counter dictionary (hint: use the get() method)
        token_count_ref = None
        
        # update the overlap by getting the smaller number between the two token counts above
        overlap += None
    
    # get the precision (i.e. number of overlapping tokens / number of system tokens)
    precision = None
    
    # get the recall (i.e. number of overlapping tokens / number of reference tokens)
    recall = None
    
    if precision + recall != 0:
        # compute the f1-score
        rouge1_score = None
    else:
        rouge1_score = 0 
    ### END CODE HERE ###
    
    return rouge1_score
    


# In[ ]:


# notice that this produces a different value from the jaccard similarity earlier
rouge1_similarity([1, 2, 3], [1, 2, 3, 4])


# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_rouge1_similarity(rouge1_similarity)
# END UNIT TEST


# ### 4.2.3 Overall score
# 
# We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.
# 
# 1. Get similarity score between sample 1 and sample 2
# 2. Get similarity score between sample 1 and sample 3
# 3. Get similarity score between sample 1 and sample 4
# 4. Get average score of the first 3 steps. This will be the overall score of sample 1.
# 5. Iterate and repeat until samples 1 to 4 have overall scores.
# 
# We will be storing the results in a dictionary for easy lookups.
# 
# &lt;a name="ex09"&gt;&lt;/a&gt;
# ### Exercise 09
# 
# **Instructions**: Implement the `average_overlap()` function.

# In[ ]:


# UNQ_C9
# GRADED FUNCTION
def average_overlap(similarity_fn, samples, *ignore_params):
    """Returns the arithmetic mean of each candidate sentence in the samples

    Args:
        similarity_fn (function): similarity function used to compute the overlap
        samples (list of lists): tokenized version of the translated sentences
        *ignore_params: additional parameters will be ignored

    Returns:
        dict: scores of each sample
            key: index of the sample
            value: score of the sample
    """  
    
    # initialize dictionary
    scores = {}
    
    # run a for loop for each sample
    for index_candidate, candidate in enumerate(samples):    
        
        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
        
        # initialize overlap to 0.0
        overlap = None
        
        # run a for loop for each sample
        for index_sample, sample in enumerate(samples): 

            # skip if the candidate index is the same as the sample index
            if index_candidate == index_sample:
                continue
                
            # get the overlap between candidate and sample using the similarity function
            sample_overlap = None
            
            # add the sample overlap to the total overlap
            overlap += None
            
        # get the score for the candidate by computing the average
        score = None
        
        # save the score in the dictionary. use index as the key.
        scores[index_candidate] = None
        
        ### END CODE HERE ###
    return scores


# In[ ]:


average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])


# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_average_overlap(average_overlap)
# END UNIT TEST


# In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below and you can use it in your experiements to see which one will give better results.

# In[ ]:


def weighted_avg_overlap(similarity_fn, samples, log_probs):
    """Returns the weighted mean of each candidate sentence in the samples

    Args:
        samples (list of lists): tokenized version of the translated sentences
        log_probs (list of float): log probability of the translated sentences

    Returns:
        dict: scores of each sample
            key: index of the sample
            value: score of the sample
    """
    
    # initialize dictionary
    scores = {}
    
    # run a for loop for each sample
    for index_candidate, candidate in enumerate(samples):    
        
        # initialize overlap and weighted sum
        overlap, weight_sum = 0.0, 0.0
        
        # run a for loop for each sample
        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):

            # skip if the candidate index is the same as the sample index            
            if index_candidate == index_sample:
                continue
                
            # convert log probability to linear scale
            sample_p = float(np.exp(logp))

            # update the weighted sum
            weight_sum += sample_p

            # get the unigram overlap between candidate and sample
            sample_overlap = similarity_fn(candidate, sample)
            
            # update the overlap
            overlap += sample_p * sample_overlap
            
        # get the score for the candidate
        score = overlap / weight_sum
        
        # save the score in the dictionary. use index as the key.
        scores[index_candidate] = score
    
    return scores


# In[ ]:


weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])


# ### 4.2.4 Putting it all together
# 
# We will now put everything together and develop the `mbr_decode()` function. Please use the helper functions you just developed to complete this. You will want to generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence.
# 
# &lt;a name="ex10"&gt;&lt;/a&gt;
# ### Exercise 10
# 
# **Instructions**: Implement the `mbr_overlap()` function.

# In[ ]:


# UNQ_C10
# GRADED FUNCTION
def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):
    """Returns the translated sentence using Minimum Bayes Risk decoding

    Args:
        sentence (str): sentence to translate.
        n_samples (int): number of samples to generate
        score_fn (function): function that generates the score for each sample
        similarity_fn (function): function used to compute the overlap between a pair of samples
        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.
        temperature (float): parameter for sampling ranging from 0.0 to 1.0.
            0.0: same as argmax, always pick the most probable token
            1.0: sampling from the distribution (can sometimes say random things)
        vocab_file (str): filename of the vocabulary
        vocab_dir (str): path to the vocabulary file

    Returns:
        str: the translated sentence
    """
    
    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###
    # generate samples
    samples, log_probs = None
    
    # use the scoring function to get a dictionary of scores
    # pass in the relevant parameters as shown in the function definition of 
    # the mean methods you developed earlier
    scores = None
    
    # find the key with the highest score
    max_index = None
    
    # detokenize the token list associated with the max_index
    translated_sentence = None
    
    ### END CODE HERE ###
    return (translated_sentence, max_index, scores)


# In[ ]:


TEMPERATURE = 1.0

# put a custom string here
your_sentence = 'She speaks English and German.'


# In[ ]:


mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]


# In[ ]:


mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]


# In[ ]:


mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]


# **This unit test take a while to run. Please be patient**

# In[ ]:


# BEGIN UNIT TEST
w1_unittest.test_mbr_decode(mbr_decode, model)
# END UNIT TEST


# #### Congratulations! Next week, you'll dive deeper into attention models and study the Transformer architecture. You will build another network but without the recurrent part. It will show that attention is all you need! It should be fun!


</pre></div>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/neural-machine-translation-training-the-model/">Neural Machine Translation: Training the Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/neural-machine-translation-training-the-model/" rel="bookmark"><time class="published dt-published" datetime="2021-02-14T14:54:34-08:00" itemprop="datePublished" title="2021-02-14 14:54">2021-02-14 14:54</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#orgdf1e30f">Training Our Model</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#orgae816bb">Imports</a></li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#org1814160">Set Up</a></li>
</ul>
</li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#org3ca749d">Training</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#orgf34687d">TrainTask</a></li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#orgf0f61bf">EvalTask</a></li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#org718bcf8">Loop</a></li>
</ul>
</li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#orge4fedd5">End</a></li>
<li><a href="posts/nlp/neural-machine-translation-training-the-model/#org828e9f4">Raw</a></li>
</ul>
</div>
</div>
<div class="outline-2" id="outline-container-orgdf1e30f">
<h2 id="orgdf1e30f">Training Our Model</h2>
<div class="outline-text-2" id="text-orgdf1e30f">
<p>In the <a href="posts/nlp/neural-machine-translation-the-attention-model/">previous post</a> we defined our model for machine translation. In this post we'll train the model on our data.</p>
<p>Doing supervised training in Trax is pretty straightforward (short example <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training">here</a>). We will be instantiating three classes for this: <code>TrainTask</code>, <code>EvalTask</code>, and <code>Loop</code>. Let's take a closer look at each of these in the sections below.</p>
</div>
<div class="outline-3" id="outline-container-orgae816bb">
<h3 id="orgae816bb">Imports</h3>
<div class="outline-text-3" id="text-orgae816bb">
<div class="highlight">
<pre><span></span><span class="c1"># python</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">redirect_stdout</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="kn">import</span> <span class="nn">sys</span>

<span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">holoviews</span> <span class="kn">import</span> <span class="n">opts</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span><span class="p">,</span> <span class="n">optimizers</span>
<span class="kn">from</span> <span class="nn">trax.supervised</span> <span class="kn">import</span> <span class="n">lr_schedules</span><span class="p">,</span> <span class="n">training</span>

<span class="kn">import</span> <span class="nn">holoviews</span>
<span class="kn">import</span> <span class="nn">hvplot.pandas</span>
<span class="kn">import</span> <span class="nn">pandas</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.machine_translation</span> <span class="kn">import</span> <span class="n">DataGenerator</span><span class="p">,</span> <span class="n">NMTAttn</span>

<span class="c1"># related</span>
<span class="kn">from</span> <span class="nn">graeae</span> <span class="kn">import</span> <span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">Timer</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org1814160">
<h3 id="org1814160">Set Up</h3>
<div class="outline-text-3" id="text-org1814160">
<div class="highlight">
<pre><span></span><span class="n">train_batch_stream</span> <span class="o">=</span> <span class="n">DataGenerator</span><span class="p">()</span><span class="o">.</span><span class="n">batch_generator</span>
<span class="n">eval_batch_stream</span> <span class="o">=</span> <span class="n">DataGenerator</span><span class="p">(</span><span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">batch_generator</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">SLUG</span> <span class="o">=</span> <span class="s2">"neural-machine-translation-training-the-model"</span>
<span class="n">Embed</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">EmbedHoloviews</span><span class="p">,</span> <span class="n">folder_path</span><span class="o">=</span><span class="sa">f</span><span class="s2">"files/posts/nlp/</span><span class="si">{</span><span class="n">SLUG</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="n">Plot</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">"Plot"</span><span class="p">,</span> <span class="p">[</span><span class="s2">"width"</span><span class="p">,</span> <span class="s2">"height"</span><span class="p">,</span> <span class="s2">"fontscale"</span><span class="p">,</span> <span class="s2">"tan"</span><span class="p">,</span> <span class="s2">"blue"</span><span class="p">,</span> <span class="s2">"red"</span><span class="p">])</span>
<span class="n">PLOT</span> <span class="o">=</span> <span class="n">Plot</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="mi">900</span><span class="p">,</span>
    <span class="n">height</span><span class="o">=</span><span class="mi">750</span><span class="p">,</span>
    <span class="n">fontscale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">tan</span><span class="o">=</span><span class="s2">"#ddb377"</span><span class="p">,</span>
    <span class="n">blue</span><span class="o">=</span><span class="s2">"#4687b7"</span><span class="p">,</span>
    <span class="n">red</span><span class="o">=</span><span class="s2">"#ce7b6d"</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">TIMER</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-org3ca749d">
<h2 id="org3ca749d">Training</h2>
<div class="outline-text-2" id="text-org3ca749d"></div>
<div class="outline-3" id="outline-container-orgf34687d">
<h3 id="orgf34687d">TrainTask</h3>
<div class="outline-text-3" id="text-orgf34687d">
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask">TrainTask</a> class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights.</p>
<div class="highlight">
<pre><span></span><span class="n">train_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="p">(</span>

    <span class="c1"># use the train batch stream as labeled data</span>
    <span class="n">labeled_data</span> <span class="o">=</span> <span class="n">train_batch_stream</span><span class="p">,</span>

    <span class="c1"># use the cross entropy loss</span>
    <span class="n">loss_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">WeightedCategoryCrossEntropy</span><span class="p">(),</span>

    <span class="c1"># use the Adam optimizer with learning rate of 0.01</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>

    <span class="c1"># use the `trax.lr.warmup_and_rsqrt_decay` as the learning rate schedule</span>
    <span class="c1"># have 1000 warmup steps with a max value of 0.01</span>
    <span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">lr_schedules</span><span class="o">.</span><span class="n">warmup_and_rsqrt_decay</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span>

    <span class="c1"># have a checkpoint every 10 steps</span>
    <span class="n">n_steps_per_checkpoint</span><span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_train_task</span><span class="p">(</span><span class="n">train_task</span><span class="p">):</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">train_task</span>
    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Test the labeled data parameter</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">strlabel</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">_labeled_data</span><span class="p">)</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">strlabel</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s2">"generator"</span><span class="p">)</span> <span class="ow">and</span> <span class="n">strlabel</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="s1">'add_loss_weights'</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong labeled data parameter"</span><span class="p">)</span>

    <span class="c1"># Test the cross entropy loss data parameter</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">strlabel</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">_loss_layer</span><span class="p">)</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">strlabel</span> <span class="o">==</span> <span class="s2">"CrossEntropyLoss_in3"</span><span class="p">)</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong loss functions. CrossEntropyLoss_in3 was expected"</span><span class="p">)</span>

     <span class="c1"># Test the optimizer parameter</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">trax</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">adam</span><span class="o">.</span><span class="n">Adam</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong optimizer"</span><span class="p">)</span>

    <span class="c1"># Test the schedule parameter</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">_lr_schedule</span><span class="p">,</span><span class="n">trax</span><span class="o">.</span><span class="n">supervised</span><span class="o">.</span><span class="n">lr_schedules</span><span class="o">.</span><span class="n">_BodyAndTail</span><span class="p">))</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong learning rate schedule type"</span><span class="p">)</span>

    <span class="c1"># Test the _n_steps_per_checkpoint parameter</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">_n_steps_per_checkpoint</span><span class="o">==</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">except</span><span class="p">:</span>
        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"Wrong checkpoint step frequency"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
    <span class="k">return</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_train_task</span><span class="p">(</span><span class="n">train_task</span><span class="p">)</span>
</pre></div>
<pre class="example">
Wrong loss functions. CrossEntropyLoss_in3 was expected
Wrong optimizer
Wrong learning rate schedule type
[92m 2  Tests passed
[91m 3  Tests failed
</pre>
<p>The code has changed a bit since the test was written so it won't pass without updates.</p>
</div>
</div>
<div class="outline-3" id="outline-container-orgf0f61bf">
<h3 id="orgf0f61bf">EvalTask</h3>
<div class="outline-text-3" id="text-orgf0f61bf">
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask">EvalTask</a> on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy.</p>
<div class="highlight">
<pre><span></span><span class="n">eval_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="p">(</span>

    <span class="c1">## use the eval batch stream as labeled data</span>
    <span class="n">labeled_data</span><span class="o">=</span><span class="n">eval_batch_stream</span><span class="p">,</span>

    <span class="c1">## use the cross entropy loss and accuracy as metrics</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">WeightedCategoryCrossEntropy</span><span class="p">(),</span> <span class="n">layers</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="outline-3" id="outline-container-org718bcf8">
<h3 id="org718bcf8">Loop</h3>
<div class="outline-text-3" id="text-org718bcf8">
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop">Loop</a> class defines the model we will train as well as the train and eval tasks to execute. Its <code>run()</code> method allows us to execute the training for a specified number of steps.</p>
<div class="highlight">
<pre><span></span><span class="n">output_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"~/models/machine_translation/"</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span>
</pre></div>
<p>Define the training loop.</p>
<div class="highlight">
<pre><span></span><span class="n">training_loop</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'train'</span><span class="p">),</span>
                              <span class="n">train_task</span><span class="p">,</span>
                              <span class="n">eval_tasks</span><span class="o">=</span><span class="p">[</span><span class="n">eval_task</span><span class="p">],</span>
                              <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">train_steps</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">with</span> <span class="n">TIMER</span><span class="p">,</span> \
     <span class="nb">open</span><span class="p">(</span><span class="s2">"/tmp/machine_translation_training.log"</span><span class="p">,</span> <span class="s2">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">temp_file</span><span class="p">,</span> \
     <span class="n">redirect_stdout</span><span class="p">(</span><span class="n">temp_file</span><span class="p">):</span>
            <span class="n">training_loop</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">train_steps</span><span class="p">)</span>
</pre></div>
<pre class="example">
Started: 2021-03-09 18:31:58.844878
Ended: 2021-03-09 20:14:43.090358
Elapsed: 1:42:44.245480
</pre>
<div class="highlight">
<pre><span></span><span class="n">frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">training_loop</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">,</span> <span class="s2">"metrics/WeightedCategoryCrossEntropy"</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="s2">"Batch CrossEntropy"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="n">minimum</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">frame</span><span class="o">.</span><span class="n">CrossEntropy</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
<span class="n">vline</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">minimum</span><span class="o">.</span><span class="n">Batch</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">red</span><span class="p">))</span>
<span class="n">hline</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">minimum</span><span class="o">.</span><span class="n">CrossEntropy</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">red</span><span class="p">))</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Batch"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"CrossEntropy"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">Curve</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">blue</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">hline</span> <span class="o">*</span> <span class="n">vline</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Evaluation Batch Cross Entropy Loss"</span><span class="p">,</span>
                                   <span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"evaluation_cross_entropy"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/neural-machine-translation-training-the-model/evaluation_cross_entropy.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<div class="highlight">
<pre><span></span><span class="n">frame</span> <span class="o">=</span> <span class="n">pandas</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">training_loop</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">,</span> <span class="s2">"metrics/Accuracy"</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="s2">"Batch Accuracy"</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>

<span class="n">minimum</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">frame</span><span class="o">.</span><span class="n">Accuracy</span><span class="o">.</span><span class="n">idxmin</span><span class="p">()]</span>
<span class="n">vline</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">minimum</span><span class="o">.</span><span class="n">Batch</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">VLine</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">red</span><span class="p">))</span>
<span class="n">hline</span> <span class="o">=</span> <span class="n">holoviews</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">minimum</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">HLine</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">red</span><span class="p">))</span>
<span class="n">line</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">hvplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">"Batch"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">"Accuracy"</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span><span class="n">opts</span><span class="o">.</span><span class="n">Curve</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">blue</span><span class="p">))</span>

<span class="n">plot</span> <span class="o">=</span> <span class="p">(</span><span class="n">line</span> <span class="o">*</span> <span class="n">hline</span> <span class="o">*</span> <span class="n">vline</span><span class="p">)</span><span class="o">.</span><span class="n">opts</span><span class="p">(</span>
    <span class="n">width</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">PLOT</span><span class="o">.</span><span class="n">height</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">"Evaluation Batch Accuracy"</span><span class="p">,</span>
                                   <span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span><span class="n">plot</span><span class="o">=</span><span class="n">plot</span><span class="p">,</span> <span class="n">file_name</span><span class="o">=</span><span class="s2">"evaluation_accuracy"</span><span class="p">)()</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
<object data="posts/nlp/neural-machine-translation-training-the-model/evaluation_accuracy.html" height="800" style="width:100%" type="text/html">
<p>Figure Missing</p>
</object>
<p>It seems to be stuck…</p>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orge4fedd5">
<h2 id="orge4fedd5">End</h2>
<div class="outline-text-2" id="text-orge4fedd5">
<p>Now that we've trained the model in the <a href="posts/nlp/neural-machine-translation-testing-the-model/">next post</a> we'll test our model to see how well it does. The overview post with links to all the posts in this series is <a href="posts/nlp/neural-machine-translation/">here</a>.</p>
</div>
</div>
<div class="outline-2" id="outline-container-org828e9f4">
<h2 id="org828e9f4">Raw</h2>
</div>
</div>
</article>
<article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article">
<header>
<h1 class="p-name entry-title"><a class="u-url" href="posts/nlp/neural-machine-translation-the-attention-model/">Neural Machine Translation: The Attention Model</a></h1>
<div class="metadata">
<p class="byline author vcard"><span class="byline-name fn" itemprop="author">Cloistered Monkey</span></p>
<p class="dateline"><a href="posts/nlp/neural-machine-translation-the-attention-model/" rel="bookmark"><time class="published dt-published" datetime="2021-02-14T14:54:08-08:00" itemprop="datePublished" title="2021-02-14 14:54">2021-02-14 14:54</time></a></p>
</div>
</header>
<div class="e-content entry-content">
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#org7734357">Defining the Model</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#org3ac4558">Attention Overview</a></li>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#org82df267">Imports</a></li>
</ul>
</li>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#orgcd79f24">Implementation</a>
<ul>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#org0897f0c">Overview</a></li>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#org0428639">The Implementation</a></li>
</ul>
</li>
<li><a href="posts/nlp/neural-machine-translation-the-attention-model/#orga4ea059">End</a></li>
</ul>
</div>
</div>
<div class="highlight">
<pre><span></span><span class="o">&lt;&lt;</span><span class="n">imports</span><span class="o">&gt;&gt;</span>

<span class="o">&lt;&lt;</span><span class="n">attention</span><span class="o">-</span><span class="n">model</span><span class="o">&gt;&gt;</span>
</pre></div>
<div class="outline-2" id="outline-container-org7734357">
<h2 id="org7734357">Defining the Model</h2>
<div class="outline-text-2" id="text-org7734357">
<p>In the <a href="posts/nlp/neural-machine-translation-helper-functions/">previous post</a> we made some helper functions to prepare inputs for some of the layers in the model. In this post we'll define the model itself.</p>
</div>
<div class="outline-3" id="outline-container-org3ac4558">
<h3 id="org3ac4558">Attention Overview</h3>
<div class="outline-text-3" id="text-org3ac4558">
<p>The model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. Just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.</p>
<p>Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction "Wie" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word "Wie" (i.e. first green rectangle). Given this information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. The result of the model training might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word "geht". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.</p>
<p>There are different ways to implement attention and the one we'll use is the Scaled Dot Product Attention which has the form:</p>
<p>\[ Attention(Q, K, V) = softmax \left(\frac{QK^T}{\sqrt{d_k}} \right)V \]</p>
<p>You can think of it as computing scores using queries (Q) and keys (K), followed by a multiplication of values (V) to get a context vector at a particular timestep of the decoder. This context vector is fed to the decoder RNN to get a set of probabilities for the next predicted word. The division by square root of the keys dimensionality (\(\sqrt{d_k}\)) is for improving model performance and you'll also learn more about it next week. For our machine translation application, the encoder activations (i.e. encoder hidden states) will be the keys and values, while the decoder activations (i.e. decoder hidden states) will be the queries.</p>
<p>You will see in the upcoming sections that this complex architecture and mechanism can be implemented with just a few lines of code.</p>
</div>
</div>
<div class="outline-3" id="outline-container-org82df267">
<h3 id="org82df267">Imports</h3>
<div class="outline-text-3" id="text-org82df267">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="kn">import</span> <span class="nn">trax</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">neurotic.nlp.machine_translation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">NMTAttn</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="outline-2" id="outline-container-orgcd79f24">
<h2 id="orgcd79f24">Implementation</h2>
<div class="outline-text-2" id="text-orgcd79f24"></div>
<div class="outline-3" id="outline-container-org0897f0c">
<h3 id="org0897f0c">Overview</h3>
<div class="outline-text-3" id="text-org0897f0c">
<p>We are now ready to implement our sequence-to-sequence model with attention. This will be a Serial network and is illustrated in the diagram below. It shows the layers you'll be using in Trax and you'll see that each step can be implemented quite easily with one line commands. We've placed several links to the documentation for each relevant layer in the discussion after the figure below.</p>
<ul class="org-ul">
<li><b>Step 0:</b> Prepare the input encoder and pre-attention decoder branches. We've already defined this earlier as helper functions so it's just a matter of calling those functions and assigning it to variables.</li>
<li><b>Step 1:</b> Create a Serial network. This will stack the layers in the next steps one after the other. As before, we'll use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial">tl.Serial</a>.</li>
<li><b>Step 2:</b> Make a copy of the input and target tokens. As you see in the diagram above, the input and target tokens will be fed into different layers of the model. We'll use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select">tl.Select</a> layer to create copies of these tokens, arranging them as <code>[input tokens, target tokens, input tokens, target tokens]</code>.</li>
<li><b>Step 3:</b> Create a parallel branch to feed the input tokens to the <code>input_encoder</code> and the target tokens to the <code>pre_attention_decoder</code>. We'll use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel">tl.Parallel</a> to create these sublayers in parallel, remembering to pass the variables defined in Step 0 as parameters to this layer.</li>
<li><b>Step 4:</b> Next, call the `prepare_attention_input` function to convert the encoder and pre-attention decoder activations to a format that the attention layer will accept. You can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn">tl.Fn</a> to call this function. Note: Pass the <code>prepare_attention_input</code> function as the <code>f</code> parameter in <code>tl.Fn</code> without any arguments or parenthesis.</li>
<li><b>Step 5:</b> We will now feed the (queries, keys, values, and mask) to the <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.AttentionQKV">tl.AttentionQKV</a> layer. This computes the scaled dot product attention and outputs the attention weights and mask. Take note that although it is a one liner, this layer is actually composed of a deep network made up of several branches. We'll show the implementation show <a href="https://github.com/google/trax/blob/master/trax/layers/attention.py#L61">here</a> (on github) to see the different layers used.</li>
</ul>
<pre class="example" id="org5edb6df">
def AttentionQKV(d_feature, n_heads=1, dropout=0.0, mode='train'):
  """Returns a layer that maps (q, k, v, mask) to (activations, mask).

  See `Attention` above for further context/details.

  Args:
    d_feature: Depth/dimensionality of feature embedding.
    n_heads: Number of attention heads.
    dropout: Probababilistic rate for internal dropout applied to attention
        activations (based on query-key pairs) before dotting them with values.
    mode: Either 'train' or 'eval'.
  """
  return cb.Serial(
      cb.Parallel(
          core.Dense(d_feature),
          core.Dense(d_feature),
          core.Dense(d_feature),
      ),
      PureAttention(  # pylint: disable=no-value-for-parameter
          n_heads=n_heads, dropout=dropout, mode=mode),
      core.Dense(d_feature),
  )
</pre>
<p>Having deep layers poses the risk of vanishing gradients during training and we would want to mitigate that. To improve the ability of the network to learn, we can insert a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual">tl.Residual</a> layer to add the output of AttentionQKV with the <code>queries</code> input. You can do this in trax by simply nesting the <code>AttentionQKV</code> layer inside the <code>Residual</code> layer. The library will take care of branching and adding for you.</p>
<ul class="org-ul">
<li><b>Step 6:</b> We will not need the mask for the model we're building so we can safely drop it. At this point in the network, the signal stack currently has <code>[attention activations, mask, target tokens]</code> and you can use <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Select">tl.Select</a> to output just <code>[attention activations, target tokens]</code>.</li>
<li><b>Step 7:</b> We can now feed the attention weighted output to the LSTM decoder. We can stack multiple <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM">tl.LSTM</a> layers to improve the output so remember to append LSTMs equal to the number defined by <code>n_decoder_layers</code> parameter to the model.</li>
<li><b>Step 8:</b> We want to determine the probabilities of each subword in the vocabulary and you can set this up easily with a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense">tl.Dense</a> layer by making its size equal to the size of our vocabulary.</li>
<li><b>Step 9:</b> Normalize the output to log probabilities by passing the activations in Step 8 to a <a href="https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax">tl.LogSoftmax</a> layer.</li>
</ul>
</div>
</div>
<div class="outline-3" id="outline-container-org0428639">
<h3 id="org0428639">The Implementation</h3>
<div class="outline-text-3" id="text-org0428639">
<div class="highlight">
<pre><span></span><span class="c1"># pypi</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="c1"># this project</span>
<span class="kn">from</span> <span class="nn">.help_me</span> <span class="kn">import</span> <span class="n">input_encoder</span> <span class="k">as</span> <span class="n">input_encoder_fn</span>
<span class="kn">from</span> <span class="nn">.help_me</span> <span class="kn">import</span> <span class="n">pre_attention_decoder</span> <span class="k">as</span> <span class="n">pre_attention_decoder_fn</span>
<span class="kn">from</span> <span class="nn">.help_me</span> <span class="kn">import</span> <span class="n">prepare_attention_input</span> <span class="k">as</span> <span class="n">prepare_attention_input_fn</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">NMTAttn</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">33300</span><span class="p">,</span>
            <span class="n">target_vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">33300</span><span class="p">,</span>
            <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
            <span class="n">n_encoder_layers</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">n_decoder_layers</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">n_attention_heads</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">attention_dropout</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">:</span>
    <span class="sd">"""Returns an LSTM sequence-to-sequence model with attention.</span>

<span class="sd">    The input to the model is a pair (input tokens, target tokens), e.g.,</span>
<span class="sd">    an English sentence (tokenized) and its translation into German (tokenized).</span>

<span class="sd">    Args:</span>
<span class="sd">    input_vocab_size: int: vocab size of the input</span>
<span class="sd">    target_vocab_size: int: vocab size of the target</span>
<span class="sd">    d_model: int:  depth of embedding (n_units in the LSTM cell)</span>
<span class="sd">    n_encoder_layers: int: number of LSTM layers in the encoder</span>
<span class="sd">    n_decoder_layers: int: number of LSTM layers in the decoder after attention</span>
<span class="sd">    n_attention_heads: int: number of attention heads</span>
<span class="sd">    attention_dropout: float, dropout for the attention layer</span>
<span class="sd">    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference</span>

<span class="sd">    Returns:</span>
<span class="sd">    A LSTM sequence-to-sequence model with attention.</span>
<span class="sd">    """</span>
    <span class="c1"># Step 0: call the helper function to create layers for the input encoder</span>
    <span class="n">input_encoder</span> <span class="o">=</span> <span class="n">input_encoder_fn</span><span class="p">(</span><span class="n">input_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_encoder_layers</span><span class="p">)</span>

    <span class="c1"># Step 0: call the helper function to create layers for the pre-attention decoder</span>
    <span class="n">pre_attention_decoder</span> <span class="o">=</span> <span class="n">pre_attention_decoder_fn</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="n">target_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Step 1: create a serial network</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Serial</span><span class="p">(</span> 

      <span class="c1"># Step 2: copy input tokens and target tokens as they will be needed later.</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Select</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>

      <span class="c1"># Step 3: run input encoder on the input and pre-attention decoder on the target.</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Parallel</span><span class="p">(</span><span class="n">input_encoder</span><span class="p">,</span> <span class="n">pre_attention_decoder</span><span class="p">),</span>

      <span class="c1"># Step 4: prepare queries, keys, values and mask for attention.</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Fn</span><span class="p">(</span><span class="s1">'PrepareAttentionInput'</span><span class="p">,</span> <span class="n">prepare_attention_input_fn</span><span class="p">,</span> <span class="n">n_out</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>

      <span class="c1"># Step 5: run the AttentionQKV layer</span>
      <span class="c1"># nest it inside a Residual layer to add to the pre-attention decoder activations(i.e. queries)</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Residual</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">AttentionQKV</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span>
                                          <span class="n">n_heads</span><span class="o">=</span><span class="n">n_attention_heads</span><span class="p">,</span>
                                          <span class="n">dropout</span><span class="o">=</span><span class="n">attention_dropout</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)),</span>

      <span class="c1"># Step 6: drop attention mask (i.e. index = None</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Select</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]),</span>

      <span class="c1"># Step 7: run the rest of the RNN decoder</span>
      <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_decoder_layers</span><span class="p">)],</span>

      <span class="c1"># Step 8: prepare output by making it the right size</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">target_vocab_size</span><span class="p">),</span>

      <span class="c1"># Step 9: Log-softmax for output</span>
      <span class="n">layers</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">()</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="k">def</span> <span class="nf">test_NMTAttn</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">):</span>
    <span class="n">test_cases</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">"name"</span><span class="p">:</span><span class="s2">"simple_test_check"</span><span class="p">,</span>
                    <span class="s2">"expected"</span><span class="p">:</span><span class="s2">"Serial_in2_out2[</span><span class="se">\n</span><span class="s2">  Select[0,1,0,1]_in2_out4</span><span class="se">\n</span><span class="s2">  Parallel_in2_out2[</span><span class="se">\n</span><span class="s2">    Serial[</span><span class="se">\n</span><span class="s2">      Embedding_33300_1024</span><span class="se">\n</span><span class="s2">      LSTM_1024</span><span class="se">\n</span><span class="s2">      LSTM_1024</span><span class="se">\n</span><span class="s2">    ]</span><span class="se">\n</span><span class="s2">    Serial[</span><span class="se">\n</span><span class="s2">      ShiftRight(1)</span><span class="se">\n</span><span class="s2">      Embedding_33300_1024</span><span class="se">\n</span><span class="s2">      LSTM_1024</span><span class="se">\n</span><span class="s2">    ]</span><span class="se">\n</span><span class="s2">  ]</span><span class="se">\n</span><span class="s2">  PrepareAttentionInput_in3_out4</span><span class="se">\n</span><span class="s2">  Serial_in4_out2[</span><span class="se">\n</span><span class="s2">    Branch_in4_out3[</span><span class="se">\n</span><span class="s2">      None</span><span class="se">\n</span><span class="s2">      Serial_in4_out2[</span><span class="se">\n</span><span class="s2">        Parallel_in3_out3[</span><span class="se">\n</span><span class="s2">          Dense_1024</span><span class="se">\n</span><span class="s2">          Dense_1024</span><span class="se">\n</span><span class="s2">          Dense_1024</span><span class="se">\n</span><span class="s2">        ]</span><span class="se">\n</span><span class="s2">        PureAttention_in4_out2</span><span class="se">\n</span><span class="s2">        Dense_1024</span><span class="se">\n</span><span class="s2">      ]</span><span class="se">\n</span><span class="s2">    ]</span><span class="se">\n</span><span class="s2">    Add_in2</span><span class="se">\n</span><span class="s2">  ]</span><span class="se">\n</span><span class="s2">  Select[0,2]_in3_out2</span><span class="se">\n</span><span class="s2">  LSTM_1024</span><span class="se">\n</span><span class="s2">  LSTM_1024</span><span class="se">\n</span><span class="s2">  Dense_33300</span><span class="se">\n</span><span class="s2">  LogSoftmax</span><span class="se">\n</span><span class="s2">]"</span><span class="p">,</span>
                    <span class="s2">"error"</span><span class="p">:</span><span class="s2">"The NMTAttn is not defined properly."</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">"name"</span><span class="p">:</span><span class="s2">"layer_len_check"</span><span class="p">,</span>
                    <span class="s2">"expected"</span><span class="p">:</span><span class="mi">9</span><span class="p">,</span>
                    <span class="s2">"error"</span><span class="p">:</span><span class="s2">"We found </span><span class="si">{}</span><span class="s2"> layers in your model. It should be 9.</span><span class="se">\n</span><span class="s2">Check the LSTM stack before the dense layer"</span>
                <span class="p">},</span>
                <span class="p">{</span>
                    <span class="s2">"name"</span><span class="p">:</span><span class="s2">"selection_layer_check"</span><span class="p">,</span>
                    <span class="s2">"expected"</span><span class="p">:[</span><span class="s2">"Select[0,1,0,1]_in2_out4"</span><span class="p">,</span> <span class="s2">"Select[0,2]_in3_out2"</span><span class="p">],</span>
                    <span class="s2">"error"</span><span class="p">:</span><span class="s2">"Look at your selection layers."</span>
                <span class="p">}</span>
            <span class="p">]</span>

    <span class="n">success</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">fails</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">test_case</span> <span class="ow">in</span> <span class="n">test_cases</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"simple_test_check"</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">test_case</span><span class="p">[</span><span class="s2">"expected"</span><span class="p">]</span> <span class="o">==</span> <span class="nb">str</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">())</span>
                <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"layer_len_check"</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s2">"expected"</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">()</span><span class="o">.</span><span class="n">sublayers</span><span class="p">):</span>
                    <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span><span class="n">test_case</span><span class="p">[</span><span class="s2">"error"</span><span class="p">]</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">()</span><span class="o">.</span><span class="n">sublayers</span><span class="p">)))</span> 
                    <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span> <span class="o">==</span> <span class="s2">"selection_layer_check"</span><span class="p">:</span>
                <span class="n">model</span> <span class="o">=</span> <span class="n">NMTAttn</span><span class="p">()</span>
                <span class="n">output</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sublayers</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="nb">str</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">sublayers</span><span class="p">[</span><span class="mi">4</span><span class="p">])]</span>
                <span class="n">check_count</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
                    <span class="k">if</span> <span class="n">test_case</span><span class="p">[</span><span class="s2">"expected"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="n">test_case</span><span class="p">[</span><span class="s2">"error"</span><span class="p">])</span>
                        <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>
                        <span class="k">break</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">check_count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">check_count</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">success</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">test_case</span><span class="p">[</span><span class="s1">'error'</span><span class="p">])</span>
            <span class="n">fails</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">fails</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\033</span><span class="s2">[92m All tests passed"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[92m'</span><span class="p">,</span> <span class="n">success</span><span class="p">,</span><span class="s2">" Tests passed"</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">'</span><span class="se">\033</span><span class="s1">[91m'</span><span class="p">,</span> <span class="n">fails</span><span class="p">,</span> <span class="s2">" Tests failed"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">test_cases</span>
</pre></div>
<div class="highlight">
<pre><span></span><span class="n">test_cases</span> <span class="o">=</span> <span class="n">test_NMTAttn</span><span class="p">(</span><span class="n">NMTAttn</span><span class="p">)</span>
</pre></div>
<pre class="example">
The NMTAttn is not defined properly.
[92m 2  Tests passed
[91m 1  Tests failed
</pre>
<div class="highlight">
<pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">NMTAttn</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
<pre class="example" id="org2f7b849">
Serial_in2_out2[
  Select[0,1,0,1]_in2_out4
  Parallel_in2_out2[
    Serial[
      Embedding_33300_1024
      LSTM_1024
      LSTM_1024
    ]
    Serial[
      Serial[
        ShiftRight(1)
      ]
      Embedding_33300_1024
      LSTM_1024
    ]
  ]
  PrepareAttentionInput_in3_out4
  Serial_in4_out2[
    Branch_in4_out3[
      None
      Serial_in4_out2[
        _in4_out4
        Serial_in4_out2[
          Parallel_in3_out3[
            Dense_1024
            Dense_1024
            Dense_1024
          ]
          PureAttention_in4_out2
          Dense_1024
        ]
        _in2_out2
      ]
    ]
    Add_in2
  ]
  Select[0,2]_in3_out2
  LSTM_1024
  LSTM_1024
  Dense_33300
  LogSoftmax
]
</pre></div>
</div>
</div>
<div class="outline-2" id="outline-container-orga4ea059">
<h2 id="orga4ea059">End</h2>
<div class="outline-text-2" id="text-orga4ea059">
<p>Now that we have the model defined, in the <a href="posts/nlp/neural-machine-translation-training-the-model/">next post</a> we'll train the model. The overview post with links to all the posts in this series is <a href="posts/nlp/neural-machine-translation/">here</a>.</p>
</div>
</div>
</div>
</article>
</div>
<ul class="pager postindexpager clearfix">
<li class="previous"><a href="." rel="prev">Newer posts</a></li>
<li class="next"><a href="index-20.html" rel="next">Older posts</a></li>
</ul>
<script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<script type="text/x-mathjax-config">

        MathJax.Hub.Config({tex2jax: {inlineMath: [['$latex ','$'], ['\\(','\\)']],},

        });
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script>
<script>

    MathJax = {
        tex: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
        }
    }
</script>
<link href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"></script><!--End of body content-->
<footer id="footer"><a href="https://creativecommons.org/licenses/by/4.0/" rel="license"><img alt="Creative Commons License" id="license-image" src="https://licensebuttons.net/l/by/4.0/80x15.png" style="border-width:0"></a>This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>. <a href="mailto:cloisteredmonkey.jmark@slmail.me">Cloistered Monkey</a> - Powered by <a href="https://getnikola.com" rel="nofollow">Nikola</a></footer>
</div>
</div>
<script src="assets/js/all-nocdn.js"></script>
<script>

    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
</script>
</body>
</html>
